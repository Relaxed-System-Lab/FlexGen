2023-10-31 06:15:54,099 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpihipbv6a
2023-10-31 06:15:54,100 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpihipbv6a/_remote_module_non_scriptable.py
2023-10-31 06:15:54,222 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 06:15:54,277 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:15:55,852 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-31 06:15:56,188 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:15:56,188 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:15:56,188 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:15:56,188 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:15:57,026 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:15:57,115 [model.py:130 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 06:15:57,115 [model.py:69 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 06:15:57,159 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:15:57,250 [model.py:79 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 06:15:57,254 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 06:15:57,255 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-31 06:15:57,255 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-31 06:15:57,256 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-31 06:15:57,257 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-31 06:15:57,258 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-31 06:15:57,259 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-31 06:15:57,260 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-31 06:15:57,261 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-31 06:15:57,262 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-31 06:15:57,263 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-31 06:15:57,264 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-31 06:15:57,265 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-31 06:15:57,266 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-31 06:15:57,267 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:15:57,267 [model.py:261 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:15:57,267 [model.py:267 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 06:15:57,270 [model.py:303 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-31 06:15:57,307 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:15:57,476 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 06:15:57,477 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 06:15:57,477 [model.py:393 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 06:15:57,477 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 06:15:57,477 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 06:15:57,478 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 06:15:57,479 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 06:15:57,479 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 06:15:57,479 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 06:15:57,479 [model.py:393 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 06:15:57,482 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:57,483 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 06:15:57,484 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:57,485 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 06:15:57,485 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:57,497 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 06:15:57,500 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:57,507 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 06:15:57,510 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:57,517 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 06:15:57,520 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:57,526 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 06:15:57,528 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:57,535 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 06:15:57,538 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:57,544 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 06:15:57,546 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:57,552 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 06:15:57,555 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:57,561 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 06:15:57,563 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:57,570 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 06:15:57,572 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:57,579 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 06:15:57,581 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:57,587 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 06:15:57,590 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:57,596 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 06:15:57,598 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:57,599 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 06:15:57,600 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:57,608 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 06:15:57,613 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 06:15:57,614 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 06:15:57,615 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 06:15:57,616 [model.py:401 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 06:15:57,628 [model.py:516 in init_all_weights] DEBUG - init all weights...
2023-10-31 06:15:57,666 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 06:15:57,667 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 06:15:57,667 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 06:15:57,667 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 06:15:57,667 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 06:15:57,668 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 06:15:57,668 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 06:15:57,668 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 06:15:57,668 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 06:15:57,669 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 06:15:57,669 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 06:15:57,669 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 06:15:57,670 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 06:15:57,670 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 06:15:57,670 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 06:15:57,670 [wrapper.py:267 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 06:15:57,716 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:15:57,861 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:57,861 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-31 06:15:57,861 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:57,861 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:57,862 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:57,862 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:57,863 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-31 06:15:57,863 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:57,863 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-31 06:15:57,864 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,865 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,866 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:15:57,867 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:57,867 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-31 06:15:57,867 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:57,867 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:57,868 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:57,871 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:57,872 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-31 06:15:57,872 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:57,872 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-31 06:15:57,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:57,875 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:15:57,877 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:57,877 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:57,877 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,878 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:57,878 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:57,881 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:57,885 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:57,885 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,886 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:57,891 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,894 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,909 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:57,909 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:57,909 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:57,909 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,909 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:57,911 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:57,915 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:57,918 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:57,919 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,919 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:57,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:57,936 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:57,937 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:57,937 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,937 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:57,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:57,942 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:57,946 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:57,946 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,947 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:57,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,959 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,962 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:57,962 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:57,963 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:57,963 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,963 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:57,964 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:57,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:57,972 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:57,972 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,973 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:57,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,981 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:57,987 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:57,988 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:15:57,988 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:57,988 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,988 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:15:57,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:57,994 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:57,997 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:57,998 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:57,998 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,013 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,016 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,016 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:15:58,016 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,017 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,017 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:15:58,018 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,022 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:58,026 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,026 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,027 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,041 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,041 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:15:58,041 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,041 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,042 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:15:58,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,051 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,052 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,060 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,067 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,067 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:15:58,068 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,068 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,068 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:15:58,070 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,073 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,077 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,078 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,078 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,095 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,095 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:15:58,095 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,095 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,095 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:15:58,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,100 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,104 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,104 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,105 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,119 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,120 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:15:58,120 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,120 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,120 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:15:58,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,129 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,130 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,135 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,138 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,145 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,145 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:15:58,145 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,145 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,145 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:15:58,147 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,150 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,154 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,154 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,155 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,159 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,170 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,170 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:15:58,170 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,170 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,171 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:15:58,172 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,177 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,177 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,178 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:15:58,192 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:15:58,193 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:15:58,193 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,193 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,193 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:15:58,194 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,195 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,195 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,196 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,196 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:15:58,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:58,198 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:58,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:15:58,199 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:15:58,199 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:15:58,199 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:15:58,200 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,200 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:15:58,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:58,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,201 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:15:58,201 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,202 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:15:58,214 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:15:58,224 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:15:58,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:15:58,248 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-31 06:15:58,249 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:58,249 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:15:58,249 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,249 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:58,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:58,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:58,254 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:15:58,254 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,255 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:15:58,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,257 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,258 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,258 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:58,258 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-31 06:15:58,258 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,258 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:58,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:58,263 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:58,263 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-31 06:15:58,263 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,264 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-31 06:15:58,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,267 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,268 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:58,268 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,269 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,269 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:58,269 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:58,273 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:58,277 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,277 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,279 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,284 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,293 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,293 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:58,293 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,293 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,293 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:58,295 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:58,299 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:58,303 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,303 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,305 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,315 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,320 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,320 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:58,320 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,321 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,321 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:58,322 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:58,326 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:58,330 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,331 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,332 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,337 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,341 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,349 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,353 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,353 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:58,353 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,354 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,354 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:58,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:58,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:58,363 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,363 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,365 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,379 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,380 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:15:58,380 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,380 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,380 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:15:58,382 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:58,386 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:58,390 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,390 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,406 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,406 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:15:58,406 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,406 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,406 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:15:58,408 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,412 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:58,416 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,416 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,417 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,429 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,432 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,432 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:15:58,432 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,433 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,433 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:15:58,434 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,442 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,442 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,444 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,448 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,455 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,458 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,458 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:15:58,458 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,458 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,458 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:15:58,460 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,464 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,468 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,468 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,469 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,474 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,477 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,483 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,483 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:15:58,483 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,484 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,484 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:15:58,485 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,493 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,493 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,495 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,503 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,509 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,509 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:15:58,509 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,509 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,509 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:15:58,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,515 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,519 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,519 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,520 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,535 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,535 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:15:58,535 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,535 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,535 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:15:58,537 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,541 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,545 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,545 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,547 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,551 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,558 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,560 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,560 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:15:58,560 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,561 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,561 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:15:58,562 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,567 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,567 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,568 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,581 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:15:58,584 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:15:58,584 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:15:58,584 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,584 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,584 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:15:58,586 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,586 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,587 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,587 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,588 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:58,588 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,589 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,590 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,590 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,591 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:15:58,591 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,591 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,591 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:15:58,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:58,592 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,592 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,592 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,593 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:58,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:58,616 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:58,624 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:58,631 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:15:58,633 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:58,633 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:15:58,633 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,633 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:58,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:58,639 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:58,639 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:15:58,640 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,641 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:15:58,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,643 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,644 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:58,644 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-31 06:15:58,644 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,644 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:58,645 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:58,649 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:58,650 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-31 06:15:58,650 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,650 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-31 06:15:58,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,652 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,653 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,653 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,655 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:58,655 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,655 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,655 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:58,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:58,659 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:58,663 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,663 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,665 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,670 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,677 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,679 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,679 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:58,679 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,679 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,679 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:58,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:58,685 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:58,689 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,689 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,691 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,705 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,705 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:58,705 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,705 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:58,707 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:58,711 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:58,715 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,715 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,717 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,721 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,725 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,731 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,731 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:58,731 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,731 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:58,733 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:58,737 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:58,741 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,741 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,743 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,755 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,757 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,757 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:15:58,757 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,757 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,758 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:15:58,759 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:58,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:58,767 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,767 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,769 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,774 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,783 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,783 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:15:58,783 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,784 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,784 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:15:58,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:58,793 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,793 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,795 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,803 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,809 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,809 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:15:58,809 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,809 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,809 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:15:58,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:58,818 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,819 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,820 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,825 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,835 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,835 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:15:58,835 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,836 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,836 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:15:58,837 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,841 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:58,845 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,845 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,847 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,859 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,888 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,888 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:15:58,888 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,889 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,889 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:15:58,890 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:58,897 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,898 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,899 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,909 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,913 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,915 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,915 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:15:58,915 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,915 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,915 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:15:58,917 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,921 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:58,924 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,924 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,926 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,938 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,940 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,940 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:15:58,940 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,940 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,941 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:15:58,942 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:58,949 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,949 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,951 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,966 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,967 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:15:58,967 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,967 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,967 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:15:58,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,969 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:58,972 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,973 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:58,975 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:58,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,983 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:15:58,988 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:15:58,988 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:15:58,988 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,988 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:15:58,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:58,991 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,991 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,991 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:58,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:58,994 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:58,994 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:15:58,994 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:58,995 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:58,995 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:15:58,995 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:58,995 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:58,996 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:58,996 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:58,996 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:59,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,018 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,024 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:15:59,025 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:59,025 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:15:59,026 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,026 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:59,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:59,030 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:15:59,030 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,030 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:15:59,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,033 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,033 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:59,033 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-31 06:15:59,033 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,033 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:59,034 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,037 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,038 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-31 06:15:59,038 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,038 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-31 06:15:59,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,040 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,041 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,041 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,043 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:59,043 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,043 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,043 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:59,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,050 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,052 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,065 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,065 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:59,066 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,066 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,066 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:59,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,071 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,074 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,074 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,076 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,090 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,090 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:59,090 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,090 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,090 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:59,092 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:59,095 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,098 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,098 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,100 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,124 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,127 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,129 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,129 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:59,129 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,129 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,129 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:59,131 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:59,135 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:59,138 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,139 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,140 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,154 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,154 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:15:59,154 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,155 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,155 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:15:59,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:59,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:59,163 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,163 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,165 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,170 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,177 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,179 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,180 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:15:59,180 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,181 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,181 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:15:59,182 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:59,186 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:59,189 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,189 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,191 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,204 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,205 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:15:59,205 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,205 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,205 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:15:59,206 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:59,210 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:59,214 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,214 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,216 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,227 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,230 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,230 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:15:59,230 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,230 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,230 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:15:59,232 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:59,236 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:59,240 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,240 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,241 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,249 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,252 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,254 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,254 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:15:59,255 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,255 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,255 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:15:59,256 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:59,260 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:59,263 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,263 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,265 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,279 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,279 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:15:59,279 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,279 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,279 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:15:59,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:59,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:59,288 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,288 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,289 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,294 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,305 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,305 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:15:59,306 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,306 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,306 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:15:59,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:59,311 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:59,314 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,315 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,316 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,320 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,324 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,330 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,330 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:15:59,330 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,330 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,330 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:15:59,332 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:59,332 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:59,336 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,336 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,338 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,342 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,357 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:15:59,359 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:15:59,360 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:15:59,360 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,360 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,360 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:15:59,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:59,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:59,363 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,363 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,364 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:59,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,366 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,367 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,367 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:15:59,367 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,367 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,368 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:15:59,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:59,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:59,369 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,369 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,370 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:59,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,393 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,412 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:15:59,414 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:59,414 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:15:59,415 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,415 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:59,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:59,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:15:59,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,420 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:15:59,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,422 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,422 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:59,422 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-31 06:15:59,423 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,423 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:59,423 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,427 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,427 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-31 06:15:59,427 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,428 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-31 06:15:59,429 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,429 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,430 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,431 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,432 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:59,432 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,433 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,433 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:59,433 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,439 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,439 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,441 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,446 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,456 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,456 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:59,456 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,456 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,456 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:59,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,461 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,464 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,464 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,466 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,482 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,482 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:59,483 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,483 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,483 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:59,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:59,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,491 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,492 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,493 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,505 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,507 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,507 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:59,508 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,508 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,508 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:59,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:59,513 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:59,516 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,516 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,518 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,526 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,531 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,532 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:15:59,532 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,532 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,532 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:15:59,533 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:59,537 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:59,540 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,542 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,551 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,558 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,558 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:15:59,558 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,558 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,558 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:15:59,560 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:59,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:15:59,568 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,568 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,571 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,616 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,619 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,619 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:15:59,620 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,620 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,620 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:15:59,622 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:59,626 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:15:59,630 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,630 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,632 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,648 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,649 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:15:59,649 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,649 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,649 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:15:59,651 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:59,656 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:15:59,660 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,660 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,662 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,667 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,670 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,676 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,677 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:15:59,677 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,677 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,677 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:15:59,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:59,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:15:59,686 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,686 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,688 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,701 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,703 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,703 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:15:59,703 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,704 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,704 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:15:59,705 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:59,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:15:59,713 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,713 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,715 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,729 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:15:59,730 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,730 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,730 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:15:59,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:59,735 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:15:59,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,741 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,756 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,756 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:15:59,757 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,757 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,757 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:15:59,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:59,759 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:15:59,763 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,763 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,765 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:15:59,786 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:15:59,787 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:15:59,787 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,787 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,787 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:15:59,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:59,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:15:59,790 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,790 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,790 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:59,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,792 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,793 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,793 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:15:59,794 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,794 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:15:59,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:59,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:15:59,795 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,795 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,795 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:15:59,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,821 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:15:59,829 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:15:59,831 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:15:59,831 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:15:59,831 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,831 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:15:59,834 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:15:59,836 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:15:59,836 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:15:59,837 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,839 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,839 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,839 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:15:59,840 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-31 06:15:59,840 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:15:59,840 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:15:59,840 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,844 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:15:59,844 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-31 06:15:59,844 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:15:59,845 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-31 06:15:59,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,846 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,847 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:15:59,848 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:15:59,849 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:15:59,849 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,849 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,850 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:15:59,850 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,854 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:15:59,857 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,857 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,859 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,864 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,905 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:15:59,906 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:15:59,906 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,906 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,906 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:15:59,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,912 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:15:59,916 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,916 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,918 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,964 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:15:59,964 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:15:59,964 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,964 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,964 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:15:59,966 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:15:59,970 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:15:59,974 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:15:59,974 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,976 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:15:59,981 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,988 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:15:59,990 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:15:59,990 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:15:59,991 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:15:59,991 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:15:59,991 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:15:59,992 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:15:59,996 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:00,000 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,000 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,002 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,047 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,047 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:00,047 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,048 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,048 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:00,049 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,054 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:00,058 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,058 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,060 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,075 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,076 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:00,076 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,076 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,076 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:00,078 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,082 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,085 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,086 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,087 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,127 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,127 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:00,127 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,128 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,128 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:00,129 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,132 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,136 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,136 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,138 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,152 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,152 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:00,152 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,153 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,153 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:00,154 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,157 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,161 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,163 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,177 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,177 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:00,177 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,177 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,177 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:00,179 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:00,182 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,185 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,186 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,187 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,202 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,202 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:00,202 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,202 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,202 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:00,204 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:00,208 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:00,211 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,211 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,213 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,221 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,227 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,227 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:00,227 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,228 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,228 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:00,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:00,233 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:00,236 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,236 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,238 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,253 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,253 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:00,253 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,253 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,254 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:00,255 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:00,256 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:00,259 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,259 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,261 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:16:00,274 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:16:00,275 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:00,275 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,275 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,275 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:00,276 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:00,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:00,278 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,278 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,278 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:00,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,281 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,281 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:00,281 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,281 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,282 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:00,282 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:00,282 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:00,283 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,283 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,283 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:00,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,297 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,311 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:00,312 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:00,312 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:00,312 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,312 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:00,315 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:00,315 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:00,316 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:00,316 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,317 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:00,317 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,319 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,319 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,319 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:00,319 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-31 06:16:00,320 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,320 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:00,320 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:00,323 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:00,324 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-31 06:16:00,324 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,324 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-31 06:16:00,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,327 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,329 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:00,329 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,329 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,329 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:00,329 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:00,333 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:00,337 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,337 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,339 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,346 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,350 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,358 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,358 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:00,358 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,358 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,358 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:00,360 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:00,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:00,366 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,367 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,368 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,415 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,416 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:00,416 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,416 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,416 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:00,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:00,422 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:00,425 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,426 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,427 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,442 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,442 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:00,443 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,443 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,443 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:00,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:00,449 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:00,452 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,453 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,454 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,480 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,480 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:00,480 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,480 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,481 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:00,482 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,487 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:00,490 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,491 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,492 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,501 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,505 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,507 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,507 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:00,507 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,508 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,508 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:00,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,513 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,517 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,517 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,519 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,524 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,531 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,533 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,533 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:00,534 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,534 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,534 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:00,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,539 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,543 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,543 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,545 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,557 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,559 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,559 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:00,559 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,559 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,559 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:00,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,565 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,568 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,569 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,570 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,581 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,586 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,588 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,588 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:00,588 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,588 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,589 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:00,590 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:00,594 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,598 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,598 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,600 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,609 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,614 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,615 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:00,615 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,615 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,615 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:00,617 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:00,621 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:00,624 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,624 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,626 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,641 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,641 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:00,641 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,641 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,641 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:00,643 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:00,647 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:00,650 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,651 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,652 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,664 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,667 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,667 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:00,667 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,667 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,667 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:00,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:00,670 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:00,673 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,673 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,675 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,680 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,687 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:16:00,690 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:16:00,690 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:00,690 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,690 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,690 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:00,691 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:00,692 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:00,693 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,693 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,693 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:00,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,696 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,696 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:00,697 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,697 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,697 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:00,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:00,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:00,698 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,698 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,699 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:00,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,715 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:00,739 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:00,745 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:00,745 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:00,745 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,746 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:00,749 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:00,749 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:00,750 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:00,750 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,751 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:00,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,752 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,753 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,753 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,754 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:00,754 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-31 06:16:00,754 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:00,754 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:00,754 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:00,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:00,759 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-31 06:16:00,759 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:00,760 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-31 06:16:00,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,762 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:00,763 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:00,764 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:00,764 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,764 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,765 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:00,765 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:00,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:00,772 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,772 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,774 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,789 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,790 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:00,790 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,790 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,790 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:00,792 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:00,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:00,799 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,800 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,801 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,806 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,810 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,817 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,817 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:00,817 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,818 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,818 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:00,819 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:00,823 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:00,827 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,827 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,829 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,841 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,844 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,844 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:00,844 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,844 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,844 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:00,846 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:00,850 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:00,853 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,853 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,855 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,872 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,872 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:00,872 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,872 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,872 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:00,874 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,878 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:00,882 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,882 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,884 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,888 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,898 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,898 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:00,899 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,899 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:00,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,905 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:00,909 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,909 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,911 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,926 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,927 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:00,927 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,927 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,927 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:00,928 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,933 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:00,937 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,937 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,939 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,953 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,954 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:00,954 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,954 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:00,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,960 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:00,964 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,964 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,966 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:00,970 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:00,980 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:00,980 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:00,981 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:00,981 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,981 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:00,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:00,986 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:00,990 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:00,990 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:00,992 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,044 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:01,045 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:01,045 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,045 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,045 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:01,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:01,051 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:01,054 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,055 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,061 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,068 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,070 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:01,071 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:01,071 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,071 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,071 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:01,072 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:01,076 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:01,080 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,080 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,082 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,097 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:01,097 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:01,097 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,097 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,097 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:01,099 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:01,100 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:01,104 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,104 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,106 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,114 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,118 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:16:01,120 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:16:01,120 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:01,121 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,121 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,121 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:01,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:01,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:01,123 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,123 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,124 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:01,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,127 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,127 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:01,127 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,127 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,127 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:01,127 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:01,128 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:01,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,128 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,129 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:01,137 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,157 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:01,159 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:01,159 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:01,159 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,160 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:01,163 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:01,164 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:01,164 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:01,164 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,165 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:01,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,167 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,168 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:01,168 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-31 06:16:01,168 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,168 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:01,168 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:01,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:01,173 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-31 06:16:01,173 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,174 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-31 06:16:01,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,176 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,176 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,178 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:01,178 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,178 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,178 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:01,179 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:01,183 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:01,186 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,187 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,189 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,194 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,198 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,209 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,219 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,220 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:01,220 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,220 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,220 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:01,222 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:01,227 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:01,231 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,231 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,234 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,239 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,251 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,251 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:01,251 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,252 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,252 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:01,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:01,258 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:01,263 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,263 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,265 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,270 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,274 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,282 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,283 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:01,283 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,283 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,283 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:01,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:01,290 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:01,294 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,294 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,296 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,311 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,312 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:01,312 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,312 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,312 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:01,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:01,319 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:01,323 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,323 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,326 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,339 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,341 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,342 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:01,342 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,342 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,342 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:01,344 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:01,349 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:01,353 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,354 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,368 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,376 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,376 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:01,376 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,377 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,377 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:01,379 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:01,383 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:01,388 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,388 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,390 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,395 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,399 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,404 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,407 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,407 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:01,407 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,407 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,407 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:01,409 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:01,414 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:01,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,430 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,437 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,438 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:01,438 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,438 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,438 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:01,440 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:01,444 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:01,449 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,449 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,451 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,491 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,491 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:01,492 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,492 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,492 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:01,495 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:01,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:01,504 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,504 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,506 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,514 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,518 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,526 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,526 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:01,526 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,526 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,526 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:01,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:01,534 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:01,538 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,538 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,541 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,562 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,562 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:01,562 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,563 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,563 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:01,565 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:01,566 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:01,571 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,572 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,574 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,579 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:16:01,590 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:16:01,590 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:01,590 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,590 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,590 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:01,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:01,592 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:01,593 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,593 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,593 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:01,594 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,596 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,596 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:01,596 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,597 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,597 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:01,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:01,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:01,598 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,598 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,598 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:01,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,620 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:01,627 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:01,628 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:01,629 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:01,629 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,629 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:01,632 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:01,633 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:01,634 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:01,634 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,634 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:01,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,637 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,637 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:01,637 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-31 06:16:01,638 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:01,638 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:01,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:01,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:01,643 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-31 06:16:01,643 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:01,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-31 06:16:01,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,645 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:01,646 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:01,648 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:01,648 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,648 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,648 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:01,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:01,652 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:01,656 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,657 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,658 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,663 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,667 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,673 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,673 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:01,673 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,673 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,674 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:01,675 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:01,679 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:01,684 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,684 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,686 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,717 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,739 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,769 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,795 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,795 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:01,795 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,795 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,795 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:01,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:01,802 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:01,806 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,806 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,808 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,814 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,818 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,825 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,825 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:01,825 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,825 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,826 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:01,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:01,832 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:01,836 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,836 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,846 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,854 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,854 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:01,854 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,855 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,855 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:01,856 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:01,861 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:01,865 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,865 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,867 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,884 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,893 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,893 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:01,893 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,893 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,893 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:01,895 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:01,899 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:01,903 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,903 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,905 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,911 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,923 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,923 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:01,923 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,924 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,924 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:01,925 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:01,930 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:01,933 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,934 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,936 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,967 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,978 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:01,978 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:01,978 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:01,978 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,978 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:01,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:01,984 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:01,988 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:01,988 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:01,990 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:01,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:01,999 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,002 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,005 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:02,005 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:02,005 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,005 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,005 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:02,007 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,011 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:02,015 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,015 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,017 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,032 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:02,033 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:02,033 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,033 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,033 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:02,035 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,039 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,045 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,049 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,060 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:02,060 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:02,060 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,061 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,061 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:02,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,066 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,070 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,070 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,072 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,085 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,087 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:02,087 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:02,087 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,088 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,088 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:02,089 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,090 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,094 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,094 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,096 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:16:02,111 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:16:02,111 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:02,112 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,112 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,112 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:02,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,114 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,114 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,115 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,118 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,118 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:02,118 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,118 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,119 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:02,119 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:02,119 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,120 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,120 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,121 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,149 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:02,151 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:02,151 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:02,151 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,151 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:02,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:02,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:02,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:02,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,157 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:02,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,159 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,159 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,160 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:02,160 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-31 06:16:02,160 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,160 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:02,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:02,164 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:02,165 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-31 06:16:02,165 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,166 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-31 06:16:02,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,168 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,170 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:02,170 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,170 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,171 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:02,171 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:02,175 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:02,179 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,179 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,181 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,195 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,195 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:02,196 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,196 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,196 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:02,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:02,202 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:02,205 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,206 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,208 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,223 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,223 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:02,223 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,223 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,223 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:02,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:02,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:02,233 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,233 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,235 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,240 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,244 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,249 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,252 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,252 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:02,252 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,252 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,252 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:02,254 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:02,258 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:02,262 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,262 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,264 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,268 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,278 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,278 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:02,278 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,278 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,279 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:02,280 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:02,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:02,288 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,288 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,290 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,295 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,298 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,304 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,304 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:02,305 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,305 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,305 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:02,306 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:02,310 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:02,314 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,314 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,316 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,321 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,328 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,331 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,331 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:02,331 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,331 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,331 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:02,332 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:02,337 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:02,341 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,341 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,343 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,351 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,357 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,357 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:02,357 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,357 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,358 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:02,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:02,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:02,367 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,367 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,369 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,382 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,386 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,388 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,388 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:02,389 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,389 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,389 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:02,390 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,394 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:02,398 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,398 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,400 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,405 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,408 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,425 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,425 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:02,425 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,425 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,425 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:02,427 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,431 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,434 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,435 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,436 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,448 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,458 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,459 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:02,459 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,459 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,459 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:02,460 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,464 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,468 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,469 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,470 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,479 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,483 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,485 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,485 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:02,485 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,485 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,486 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:02,487 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,492 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,492 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,494 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:16:02,508 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:16:02,508 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:02,509 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,509 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,509 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:02,510 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,511 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,511 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,512 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,514 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,514 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,515 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,515 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:02,515 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,515 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,515 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:02,516 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:02,516 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,516 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,517 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,517 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:02,546 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:02,547 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:02,547 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:02,547 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,547 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:02,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:02,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:02,552 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:02,552 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,553 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:02,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,555 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,556 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:02,556 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-31 06:16:02,556 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,556 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:02,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:02,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:02,561 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-31 06:16:02,561 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,562 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-31 06:16:02,563 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,563 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,565 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,567 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:02,567 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,567 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,567 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:02,567 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:02,571 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:02,575 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,575 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,577 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,585 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,589 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,592 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,592 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:02,592 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,592 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,592 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:02,594 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:02,598 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:02,601 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,602 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,604 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,648 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,649 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:02,649 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,649 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,649 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:02,651 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:02,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:02,659 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,659 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,661 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,666 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,670 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,676 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,676 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:02,676 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,676 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,677 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:02,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:02,683 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:02,686 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,687 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,688 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,701 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,703 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,703 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:02,703 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,704 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,704 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:02,705 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:02,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:02,713 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,713 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,715 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,730 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:02,730 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,730 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,730 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:02,732 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:02,736 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:02,740 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,740 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,742 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,757 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,757 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:02,757 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,757 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,757 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:02,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:02,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:02,767 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,767 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,769 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,774 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,778 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,783 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,784 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:02,784 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,784 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,784 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:02,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:02,790 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:02,794 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,794 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,796 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,808 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,810 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,810 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:02,811 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,811 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,811 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:02,812 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,816 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:02,820 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,820 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,822 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,836 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,837 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:02,837 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,837 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,837 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:02,839 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,843 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:02,846 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,847 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,848 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,853 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,860 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,863 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,863 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:02,863 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,863 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,863 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:02,865 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:02,872 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,873 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,875 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,884 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,909 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,935 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,953 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,953 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:02,954 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,954 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:02,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,957 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:02,961 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,962 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:02,964 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:02,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,973 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:16:02,979 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:16:02,979 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:02,980 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,980 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,980 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:02,981 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,981 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:02,982 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,982 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,983 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:02,985 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:02,986 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:02,986 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:02,986 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:02,986 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:02,986 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:02,987 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:02,987 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:02,987 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:02,988 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:02,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,017 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:03,018 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:03,018 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:03,018 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,018 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:03,022 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,023 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:03,023 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:03,024 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,024 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:03,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,027 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,027 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:03,028 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-31 06:16:03,028 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,028 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:03,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,032 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,033 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-31 06:16:03,033 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,034 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-31 06:16:03,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,036 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,037 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,039 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:03,039 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,039 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,039 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:03,039 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:03,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,047 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,047 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,049 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,055 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,067 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,067 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:03,067 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,067 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,067 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:03,069 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:03,073 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:03,077 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,077 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,079 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,083 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,095 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,095 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:03,095 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,095 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,095 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:03,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:03,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:03,105 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,105 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,107 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,134 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,135 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:03,135 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,135 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,135 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:03,137 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:03,141 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:03,145 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,145 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,147 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,162 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,162 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:03,162 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,162 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,163 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:03,164 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:03,169 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:03,172 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,173 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,175 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,191 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,191 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:03,191 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,192 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,192 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:03,193 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:03,198 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:03,202 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,203 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,205 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,213 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,220 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,220 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:03,220 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,220 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,220 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:03,222 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:03,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:03,229 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,230 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,232 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,237 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,241 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,247 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,248 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:03,248 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,248 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,248 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:03,250 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:03,254 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:03,257 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,258 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,260 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,275 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,275 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:03,276 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,276 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,276 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:03,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:03,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:03,285 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,285 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,287 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,298 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,304 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,305 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:03,305 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,305 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,305 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:03,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:03,311 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:03,315 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,315 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,317 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,322 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,333 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,333 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:03,333 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,333 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,334 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:03,335 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:03,339 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:03,343 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,343 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,345 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,350 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,361 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,361 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:03,361 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,361 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,361 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:03,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:03,364 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:03,368 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,368 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,370 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,384 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:16:03,414 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:16:03,414 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:03,414 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,415 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,415 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:03,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:03,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:03,417 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,417 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,418 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:03,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,419 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,421 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,421 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:03,421 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,421 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,421 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:03,421 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:03,422 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:03,422 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,423 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,423 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:03,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,439 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,446 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,453 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:03,454 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:03,454 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:03,455 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,455 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:03,459 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,459 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:03,460 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:03,460 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,461 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:03,461 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,463 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,463 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,464 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:03,464 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-31 06:16:03,464 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,464 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:03,464 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,468 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,469 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-31 06:16:03,469 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,470 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-31 06:16:03,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,472 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,474 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:03,474 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,474 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,475 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:03,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:03,479 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,483 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,483 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,485 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,502 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,502 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:03,502 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,502 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,502 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:03,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:03,508 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:03,512 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,512 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,514 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,550 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,551 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:03,551 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,551 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,552 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:03,554 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:03,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:03,567 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,567 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,569 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,585 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,585 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:03,585 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,585 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,585 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:03,587 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:03,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:03,595 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,595 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,597 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,609 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,612 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,612 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:03,612 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,612 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,612 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:03,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:03,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:03,622 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,623 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,625 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,639 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,648 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,648 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:03,649 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,649 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,649 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:03,650 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:03,654 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:03,657 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,657 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,659 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,664 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,678 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,680 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,680 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:03,680 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,681 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,681 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:03,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:03,685 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:03,689 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,689 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,691 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,704 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,704 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:03,704 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,705 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:03,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:03,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:03,712 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,713 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,714 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,726 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,728 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,728 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:03,728 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,728 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,729 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:03,730 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:03,733 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:03,736 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,737 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,739 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,754 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,754 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:03,755 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,755 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,755 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:03,756 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:03,760 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:03,763 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,763 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,765 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,770 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,779 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,779 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:03,779 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,779 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,779 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:03,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:03,784 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:03,787 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,787 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,789 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,794 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,797 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,805 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,805 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:03,805 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,805 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,805 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:03,831 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:03,833 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:03,836 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,837 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,839 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:16:03,853 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:16:03,853 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:03,854 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,854 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,854 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:03,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:03,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:03,856 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,856 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,856 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:03,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,859 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,859 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,859 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:03,860 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,860 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,860 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:03,860 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:03,860 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:03,861 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,861 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,861 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:03,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:03,915 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:03,919 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:03,919 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:03,920 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,920 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:03,922 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:03,923 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:03,924 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,924 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:03,925 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,927 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,927 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:03,927 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-31 06:16:03,928 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:03,928 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:03,928 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:03,932 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-31 06:16:03,933 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:03,933 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-31 06:16:03,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,935 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,936 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:03,937 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:03,938 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:03,939 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:03,939 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,939 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:03,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:03,943 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:03,948 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:03,949 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:03,951 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:03,983 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:03,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,002 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,003 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:04,003 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,003 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,003 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:04,005 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,009 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:04,012 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,012 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,014 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,028 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,030 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,030 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:04,030 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,030 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,030 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:04,032 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,036 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,039 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,039 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,041 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,046 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,056 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,056 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:04,056 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,056 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,056 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:04,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,065 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,065 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,067 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,079 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,081 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,082 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:04,082 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,082 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,082 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:04,084 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,091 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,091 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,093 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,098 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,107 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,108 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:04,108 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,108 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,108 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:04,110 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,117 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,117 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,119 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,132 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,142 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,142 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:04,142 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,143 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,143 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:04,144 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,148 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,152 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,152 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,154 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,159 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,169 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,169 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:04,170 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,170 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,170 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:04,171 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,175 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,178 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,179 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,181 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,185 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,195 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,195 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:04,195 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,196 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,196 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:04,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:04,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,204 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,205 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,207 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,211 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,219 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,222 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,222 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:04,222 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,222 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,222 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:04,224 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:04,228 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:04,232 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,232 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,234 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,248 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,248 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:04,248 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,249 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,249 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:04,250 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:04,254 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:04,258 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,258 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,260 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,275 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,275 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:04,275 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,275 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,275 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:04,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:04,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:04,281 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,281 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,283 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,288 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,295 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:16:04,297 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:16:04,297 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:04,298 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,298 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,298 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:04,299 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:04,299 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:04,300 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,300 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,301 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:04,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,303 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,304 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:04,304 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,304 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,304 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:04,304 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:04,305 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:04,305 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,305 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,306 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:04,317 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,324 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,341 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:04,342 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:04,342 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:04,342 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,343 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:04,346 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:04,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:04,347 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:04,347 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,348 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:04,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,349 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,350 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,350 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,351 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:04,351 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-31 06:16:04,351 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,351 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:04,351 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:04,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:04,356 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-31 06:16:04,356 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-31 06:16:04,357 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,359 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,361 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:04,361 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,361 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,361 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:04,361 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:04,365 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:04,368 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,369 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,371 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,408 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,411 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,411 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:04,411 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,411 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,411 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:04,413 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,416 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:04,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,429 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,435 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,435 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:04,435 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,435 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,435 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:04,437 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,440 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,443 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,444 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,445 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,454 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,457 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,459 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,459 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:04,460 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,460 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,460 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:04,461 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,465 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,468 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,468 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,470 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,482 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,484 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,484 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:04,484 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,484 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,484 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:04,485 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,492 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,492 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,494 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,505 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,507 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,507 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:04,508 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,508 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,508 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:04,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,512 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,515 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,515 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,517 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,531 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,531 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:04,532 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,532 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,532 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:04,533 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,540 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,542 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,555 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,556 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:04,556 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,556 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,556 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:04,557 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,564 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,564 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,566 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,580 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,580 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:04,580 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,581 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,581 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:04,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:04,585 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,588 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,588 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,590 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,594 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,598 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,603 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,603 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:04,604 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,604 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,604 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:04,605 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:04,608 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:04,611 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,611 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,613 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,639 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,642 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,642 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:04,642 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,642 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,642 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:04,643 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:04,647 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:04,650 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,650 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,652 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,660 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,663 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,665 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,666 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:04,666 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,666 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,666 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:04,667 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:04,668 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:04,671 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,671 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,678 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:16:04,687 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:16:04,687 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:04,687 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,687 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,687 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:04,689 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:04,689 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:04,690 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,690 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,690 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:04,691 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,693 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,693 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:04,693 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,693 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,693 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:04,694 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:04,694 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:04,695 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,695 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,695 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:04,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,709 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,716 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:04,723 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:04,725 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:04,725 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:04,725 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,725 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:04,729 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:04,729 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:04,730 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:04,730 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,730 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:04,731 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,733 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,734 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:04,734 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-31 06:16:04,734 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:04,734 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:04,734 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:04,738 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:04,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-31 06:16:04,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:04,739 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-31 06:16:04,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:04,742 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:04,744 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:04,744 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,744 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,744 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:04,744 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:04,748 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:04,752 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,752 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,754 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,758 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,762 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,766 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,768 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,768 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:04,768 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,769 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,769 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:04,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,773 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:04,777 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,777 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,779 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,792 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,792 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:04,792 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,793 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,793 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:04,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:04,801 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,801 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,802 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,810 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,814 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,816 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,816 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:04,816 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,817 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,817 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:04,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,821 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:04,825 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,825 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,827 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,831 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,835 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,841 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,841 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:04,841 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,841 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,841 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:04,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,846 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:04,849 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,850 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,851 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,856 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,860 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,863 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,866 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,866 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:04,866 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,866 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,866 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:04,868 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,871 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:04,874 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,874 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,876 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,897 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,918 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,924 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,925 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:04,925 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,926 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,926 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:04,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:04,935 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,936 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,938 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,942 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,952 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,952 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:04,952 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,953 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,953 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:04,954 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,958 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:04,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,962 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,964 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,973 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,979 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:04,979 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:04,979 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:04,979 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:04,981 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:04,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:04,989 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:04,989 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:04,991 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:04,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:04,999 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,005 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:05,006 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:05,006 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,006 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,006 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:05,008 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:05,015 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,016 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,017 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,032 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:05,032 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:05,033 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,033 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,033 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:05,034 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,038 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,042 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,042 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,044 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,059 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:05,060 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:05,060 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,060 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,060 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:05,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,066 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,066 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,068 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,080 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:16:05,083 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:16:05,083 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:05,083 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,083 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,083 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:05,085 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,085 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,086 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,086 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,086 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,089 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,089 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,089 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:05,089 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,089 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,090 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:05,090 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,090 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,091 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,091 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,091 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,114 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,122 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:05,123 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:05,124 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:05,124 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,124 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:05,127 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,128 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:05,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,129 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:05,130 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,131 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,131 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,132 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,132 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:05,132 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-31 06:16:05,132 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,133 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:05,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:05,137 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,137 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-31 06:16:05,138 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,138 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-31 06:16:05,139 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,141 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,143 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:05,143 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,143 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,143 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:05,143 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:05,147 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:05,151 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,151 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,153 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,165 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,168 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,168 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:05,168 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,168 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,168 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:05,170 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:05,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:05,177 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,178 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,180 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,194 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,194 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:05,195 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,195 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,195 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:05,196 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:05,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:05,204 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,204 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,206 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,211 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,219 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,221 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,221 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:05,222 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,222 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,222 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:05,223 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:05,227 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:05,231 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,231 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,233 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,241 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,247 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,248 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:05,248 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,248 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,248 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:05,250 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:05,254 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:05,258 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,258 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,260 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,276 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,276 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:05,276 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,276 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,276 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:05,278 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:05,282 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:05,285 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,285 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,287 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,296 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,302 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,302 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:05,302 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,303 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,303 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:05,304 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:05,308 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:05,312 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,312 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,314 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,358 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,358 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:05,358 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,358 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,358 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:05,360 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:05,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:05,367 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,367 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,369 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,374 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,386 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,410 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,410 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:05,410 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,410 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,411 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:05,412 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:05,417 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:05,421 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,421 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,423 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,454 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,456 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,457 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:05,457 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,457 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,457 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:05,459 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,463 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:05,466 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,466 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,468 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,482 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,483 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:05,483 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,483 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,483 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:05,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,492 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,492 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,494 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,508 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,508 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:05,509 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,509 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,509 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:05,510 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,515 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,515 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,517 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:16:05,532 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:16:05,532 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:05,532 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,532 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,532 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:05,533 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,534 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,534 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,535 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,535 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,537 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,537 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,538 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,538 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:05,538 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,538 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,538 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:05,539 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,539 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,540 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,540 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,569 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:05,571 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:05,571 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:05,571 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,571 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:05,575 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,575 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,576 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:05,576 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,576 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:05,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,579 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,579 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,579 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:05,580 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-31 06:16:05,580 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,580 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:05,580 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:05,584 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,585 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-31 06:16:05,585 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,585 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-31 06:16:05,586 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,588 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,588 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,590 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:05,590 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,590 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,590 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:05,591 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:05,594 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:05,598 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,598 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,600 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,614 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,615 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:05,615 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,615 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,615 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:05,617 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:05,621 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:05,624 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,625 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,627 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,639 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,641 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,641 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:05,642 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,642 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,642 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:05,643 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:05,647 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:05,651 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,651 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,653 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,667 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,667 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:05,668 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,668 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:05,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:05,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:05,677 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,677 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,679 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,704 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,704 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:05,704 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,704 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,704 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:05,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:05,710 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:05,713 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,714 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,715 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,730 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:05,730 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,730 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,730 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:05,732 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:05,736 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:05,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,741 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,756 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,756 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:05,756 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,756 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,757 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:05,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:05,762 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:05,765 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,765 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,767 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,782 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,783 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:05,783 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,783 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,783 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:05,784 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:05,788 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:05,792 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,792 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,794 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,803 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,809 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,809 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:05,809 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,809 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,810 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:05,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:05,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:05,818 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,818 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,820 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,825 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,835 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,835 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:05,835 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,836 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,836 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:05,837 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,841 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:05,845 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,845 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,847 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,859 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,862 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,862 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:05,862 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,862 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,862 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:05,864 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,868 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:05,872 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,872 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,874 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,886 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,892 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,892 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:05,893 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,893 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,893 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:05,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,895 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:05,898 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,898 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,900 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:05,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,908 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:16:05,914 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:16:05,914 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:05,914 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,915 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,915 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:05,916 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,916 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:05,917 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,917 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,918 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,920 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,920 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:05,920 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,920 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,921 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:05,921 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,921 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:05,922 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:05,922 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,922 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:05,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:05,978 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:05,979 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:05,979 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:05,979 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,980 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:05,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:05,984 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:05,984 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,984 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:05,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,987 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,987 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,987 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:05,987 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-31 06:16:05,988 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:05,988 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:05,988 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:05,992 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:05,992 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-31 06:16:05,992 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:05,993 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-31 06:16:05,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:05,996 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:05,997 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:05,997 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:05,998 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:05,998 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:05,998 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,001 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:06,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,005 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,006 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,015 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,018 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,020 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,020 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:06,021 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,021 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,021 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:06,022 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,029 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,029 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,031 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,037 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,041 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,047 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,047 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:06,047 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,047 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,048 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:06,049 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,056 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,056 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,058 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,070 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,072 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,072 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:06,072 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,072 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,072 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:06,074 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:06,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,080 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,080 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,082 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,090 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,094 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,096 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,096 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:06,097 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,097 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,097 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:06,098 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:06,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:06,104 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,105 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,106 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,114 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,118 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,120 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,120 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:06,121 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,121 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,121 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:06,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:06,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:06,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,128 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,130 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,154 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,164 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,164 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:06,164 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,164 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,164 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:06,166 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:06,169 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:06,172 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,172 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,174 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,188 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,188 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:06,188 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,189 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,189 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:06,190 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:06,193 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:06,196 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,196 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,198 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,212 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,213 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:06,213 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,213 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,213 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:06,214 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:06,218 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:06,221 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,221 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,223 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,228 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,236 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,238 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,238 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:06,238 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,239 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,239 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:06,240 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:06,244 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:06,247 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,247 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,249 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,261 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,263 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,263 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:06,263 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,264 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,264 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:06,265 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:06,268 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:06,271 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,271 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,273 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,285 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,287 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,287 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:06,287 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,287 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,287 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:06,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:06,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:06,292 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,292 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,294 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:16:06,307 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:16:06,308 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:06,308 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,308 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,308 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:06,309 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:06,310 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:06,310 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,310 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,311 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:06,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,313 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,314 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:06,314 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,314 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,314 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:06,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:06,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:06,315 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,315 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,315 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:06,324 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,339 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,346 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:06,347 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:06,347 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:06,347 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,347 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:06,350 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:06,350 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:06,351 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:06,351 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,352 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:06,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,354 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,354 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:06,354 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-31 06:16:06,354 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,355 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:06,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:06,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:06,359 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-31 06:16:06,359 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,360 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-31 06:16:06,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,361 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,363 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,364 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:06,364 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,364 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,364 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:06,365 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:06,372 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,372 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,373 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,382 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,391 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,392 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:06,392 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,392 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,392 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:06,394 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,398 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,401 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,401 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,404 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,428 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,437 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,439 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,439 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:06,440 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,440 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,440 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:06,442 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,448 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,448 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,451 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,465 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,468 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,468 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:06,469 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,469 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,469 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:06,471 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:06,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,478 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,478 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,480 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,486 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,495 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,496 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:06,496 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,496 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,496 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:06,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:06,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:06,504 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,504 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,506 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,510 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,514 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,517 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,519 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,520 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:06,520 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,520 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,520 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:06,521 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:06,525 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:06,528 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,528 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,530 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,618 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,618 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:06,619 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,619 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,619 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:06,622 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:06,627 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:06,630 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,631 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,634 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,655 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,660 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,668 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,670 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,670 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:06,671 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,671 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,671 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:06,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:06,677 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:06,680 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,681 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,683 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,699 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,699 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:06,699 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,700 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,700 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:06,701 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:06,705 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:06,708 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,708 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,710 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,715 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,727 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,727 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:06,727 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,727 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,727 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:06,729 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:06,732 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:06,736 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,736 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,738 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,755 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,757 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,758 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:06,758 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,758 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,758 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:06,759 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:06,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:06,766 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,767 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,768 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,788 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,790 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,790 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:06,790 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,790 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,790 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:06,792 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:06,792 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:06,795 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,795 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,797 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,802 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:16:06,811 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:16:06,811 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:06,812 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,812 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,812 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:06,813 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:06,813 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:06,814 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,814 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,815 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:06,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,816 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,817 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,817 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,817 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:06,818 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,818 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,818 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:06,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:06,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:06,819 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,819 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,819 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:06,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,836 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,843 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:06,850 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:06,851 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:06,852 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:06,852 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,852 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:06,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:06,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:06,856 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:06,856 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,856 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:06,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,859 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,859 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:06,859 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-31 06:16:06,859 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:06,859 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:06,860 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:06,863 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:06,864 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-31 06:16:06,864 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:06,864 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-31 06:16:06,865 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:06,867 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:06,869 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:06,869 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,869 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,869 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:06,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,873 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:06,876 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,876 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,878 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,888 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,929 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:06,930 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:06,930 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,930 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,931 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:06,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,936 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:06,940 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,940 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,942 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,956 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:06,956 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:06,956 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,957 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,957 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:06,958 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:06,965 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,965 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,967 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:06,982 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:06,983 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:06,983 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:06,983 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,983 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:06,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:06,988 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:06,992 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:06,992 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:06,994 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:06,998 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,002 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,009 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,009 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:07,009 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,009 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,009 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:07,011 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,015 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:07,018 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,018 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,020 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,028 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,034 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,034 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:07,035 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,035 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,035 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:07,036 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,040 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,045 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,060 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,060 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:07,060 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,060 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,060 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:07,061 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,065 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,069 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,069 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,071 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,079 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,083 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,085 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,085 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:07,085 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,085 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,086 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:07,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,091 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,094 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,094 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,096 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,104 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,110 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,111 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:07,111 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,111 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,111 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:07,112 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:07,116 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,120 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,120 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,121 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,130 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,134 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,151 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,151 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:07,152 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,152 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,152 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:07,154 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:07,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:07,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,162 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,164 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,176 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,178 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,178 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:07,179 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,179 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,179 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:07,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:07,184 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:07,187 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,187 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,189 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,194 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,203 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,204 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:07,204 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,204 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,204 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:07,206 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:07,206 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:07,210 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,210 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,212 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,224 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:16:07,226 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:16:07,227 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:07,227 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,227 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,227 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:07,228 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:07,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:07,229 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,229 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,230 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:07,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,232 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,233 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:07,233 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,233 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,233 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:07,233 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:07,234 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:07,234 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,234 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,235 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:07,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,257 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,264 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:07,265 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:07,266 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:07,266 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,266 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:07,269 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:07,270 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:07,270 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:07,271 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,271 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:07,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,274 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,274 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:07,274 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-31 06:16:07,275 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,275 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:07,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:07,279 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:07,280 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-31 06:16:07,280 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,280 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-31 06:16:07,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,283 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,284 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:07,285 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,285 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,285 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:07,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:07,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:07,292 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,293 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,294 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,307 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,310 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,310 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:07,310 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,310 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,310 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:07,312 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:07,316 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:07,319 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,319 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,321 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,334 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,337 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,337 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:07,337 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,337 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,337 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:07,339 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:07,343 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:07,347 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,347 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,349 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,365 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,365 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:07,365 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,365 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,365 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:07,367 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:07,371 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:07,374 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,375 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,376 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,390 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,394 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,397 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,397 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:07,397 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,397 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,397 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:07,399 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:07,407 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,407 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,410 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,438 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,439 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:07,439 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,439 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,439 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:07,440 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,449 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,449 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,451 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,461 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,465 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,468 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,469 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:07,469 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,469 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,469 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:07,470 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,478 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,479 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,480 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,485 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,496 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,496 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:07,496 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,497 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,497 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:07,498 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,502 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,506 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,506 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,508 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,516 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,528 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,528 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:07,529 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,529 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,529 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:07,530 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:07,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,539 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,539 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,541 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,557 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,558 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:07,558 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,558 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,558 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:07,560 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:07,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:07,568 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,568 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,570 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,579 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,586 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,586 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:07,586 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,586 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,587 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:07,588 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:07,592 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:07,596 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,596 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,598 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,603 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,616 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,616 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:07,616 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,616 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,616 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:07,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:07,619 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:07,623 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,623 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,625 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:16:07,679 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:16:07,679 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:07,679 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,679 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,679 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:07,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:07,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:07,682 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,682 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,683 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:07,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,686 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,686 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:07,686 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,687 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,687 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:07,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:07,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:07,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,688 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,689 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:07,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,711 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:07,720 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:07,721 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:07,721 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:07,721 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,721 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:07,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:07,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:07,726 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:07,726 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:07,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,729 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:07,730 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-31 06:16:07,730 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:07,730 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:07,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:07,735 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:07,736 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-31 06:16:07,736 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:07,736 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-31 06:16:07,737 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,738 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,739 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:07,739 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:07,741 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:07,741 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,741 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,741 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:07,742 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:07,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:07,749 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,750 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,752 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,757 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,765 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,767 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,767 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:07,768 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,768 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,768 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:07,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:07,774 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:07,778 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,778 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,780 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,796 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,796 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:07,796 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,796 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,796 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:07,798 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:07,802 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:07,806 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,806 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,808 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,817 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,821 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,823 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,823 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:07,824 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,824 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,824 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:07,826 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:07,829 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:07,833 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,833 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,835 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,850 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,850 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:07,851 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,851 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,851 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:07,852 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,857 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:07,861 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,861 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,863 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,868 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,876 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,878 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,879 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:07,879 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,879 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,879 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:07,881 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,885 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:07,889 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,889 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,891 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,900 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,930 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,930 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:07,930 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,930 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,930 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:07,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,936 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:07,940 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,940 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,942 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,957 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,957 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:07,958 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,958 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,958 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:07,959 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,963 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:07,967 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,967 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,970 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:07,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:07,987 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:07,987 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:07,987 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:07,987 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,987 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:07,989 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:07,993 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:07,997 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:07,997 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:07,999 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,007 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,013 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:08,014 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:08,014 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,014 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,014 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:08,016 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,019 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:08,023 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,023 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,025 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,040 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:08,040 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:08,040 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,040 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,041 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:08,042 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,046 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,050 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,052 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,057 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,061 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,067 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:08,067 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:08,068 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,068 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,068 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:08,069 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,070 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,073 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,074 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,076 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,080 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:16:08,090 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:16:08,091 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:08,091 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,091 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,091 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:08,092 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,093 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,093 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,094 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,094 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,097 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,097 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:08,097 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,097 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,098 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:08,098 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,098 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,099 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,099 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,099 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,114 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,130 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:08,131 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:08,131 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:08,131 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,132 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:08,135 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,135 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,136 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:08,136 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,137 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:08,137 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,138 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,139 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,139 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,140 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:08,140 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-31 06:16:08,140 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,140 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:08,140 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,144 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,145 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-31 06:16:08,145 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,145 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-31 06:16:08,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,148 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,150 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:08,150 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,150 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,150 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:08,151 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:08,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,159 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,159 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,161 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,170 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,178 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,178 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:08,178 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,178 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,178 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:08,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:08,184 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:08,191 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,191 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,194 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,223 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,223 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:08,223 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,224 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,224 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:08,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:08,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:08,233 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,233 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,235 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,240 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,254 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,254 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:08,254 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,254 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,254 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:08,256 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:08,260 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:08,264 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,264 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,266 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,271 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,274 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,280 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,280 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:08,280 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,281 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,281 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:08,282 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:08,286 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:08,290 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,290 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,292 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,297 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,307 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,307 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:08,307 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,308 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,308 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:08,309 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:08,313 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:08,317 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,317 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,318 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,323 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,333 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,333 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:08,333 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,333 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,333 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:08,335 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:08,339 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:08,342 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,342 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,344 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,349 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,356 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,359 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,359 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:08,359 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,359 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,359 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:08,361 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:08,365 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:08,368 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,369 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,371 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,379 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,389 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,389 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:08,389 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,389 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,389 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:08,391 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:08,395 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:08,398 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,398 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,400 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,405 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,409 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,415 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,415 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:08,415 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,415 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,416 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:08,417 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,421 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:08,425 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,425 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,427 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,439 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,441 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,442 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:08,442 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,442 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,442 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:08,443 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,447 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,451 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,451 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,453 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,458 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,469 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,470 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:08,470 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,470 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,470 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:08,471 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,472 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,476 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,476 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,478 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,483 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,487 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:16:08,493 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:16:08,493 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:08,493 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,493 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,493 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:08,494 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,495 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,496 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,496 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,496 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,499 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,499 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:08,499 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,499 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,499 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:08,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,500 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,501 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,501 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,509 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,517 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,524 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,532 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:08,534 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:08,534 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:08,534 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,534 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:08,537 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,538 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,538 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:08,538 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,539 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:08,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,541 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,542 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,542 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:08,542 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-31 06:16:08,542 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,542 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:08,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,546 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,547 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-31 06:16:08,547 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,548 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-31 06:16:08,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,550 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,552 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:08,552 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,552 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,552 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:08,552 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:08,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,560 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,560 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,562 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,567 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,577 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,577 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:08,577 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,578 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,578 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:08,579 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:08,583 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:08,586 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,587 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,589 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,593 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,603 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,603 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:08,603 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,603 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,604 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:08,605 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:08,609 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:08,612 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,612 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,614 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,619 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,629 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,629 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:08,629 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,630 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,630 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:08,631 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:08,635 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:08,639 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,639 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,641 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,662 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,666 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,667 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:08,667 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,667 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:08,670 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:08,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:08,678 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,678 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,680 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,709 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,711 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,711 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:08,711 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,712 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,712 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:08,713 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:08,717 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:08,721 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,721 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,723 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,731 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,735 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,737 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,737 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:08,738 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,738 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,738 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:08,739 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:08,743 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:08,747 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,747 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,749 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,757 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,763 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,764 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:08,764 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,764 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,764 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:08,766 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:08,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:08,773 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,773 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,775 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,789 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,789 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:08,789 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,790 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,790 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:08,791 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:08,795 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:08,798 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,799 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,800 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,815 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,816 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:08,816 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,816 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,816 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:08,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,821 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:08,825 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,825 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,827 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,832 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,836 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,839 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,842 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,842 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:08,842 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,842 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,842 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:08,843 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,847 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:08,851 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,851 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,853 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,867 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,869 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,869 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:08,870 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,870 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,870 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:08,871 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,872 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:08,875 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,876 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,878 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,895 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:16:08,898 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:16:08,898 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:08,898 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,898 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,898 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:08,899 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:08,901 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,901 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,901 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,904 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,904 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:08,904 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,904 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,904 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:08,905 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,905 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:08,905 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,906 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,906 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:08,914 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,928 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:08,935 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:08,936 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:08,936 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:08,936 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,936 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:08,940 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,940 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:08,941 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:08,941 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,941 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:08,942 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,942 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,944 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,944 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:08,944 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-31 06:16:08,944 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:08,945 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:08,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,949 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:08,949 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-31 06:16:08,949 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:08,950 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-31 06:16:08,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:08,953 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:08,954 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:08,954 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,954 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,955 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:08,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:08,959 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:08,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:08,962 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,964 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:08,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:08,988 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:08,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:08,995 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:08,995 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:08,995 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:08,995 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:08,995 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:08,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,001 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:09,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,004 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,006 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,015 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,023 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,023 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:09,023 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,023 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,023 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:09,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,032 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,032 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,034 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,043 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,047 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,049 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,049 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:09,049 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,050 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,050 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:09,051 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,059 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,059 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,070 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,076 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,076 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:09,076 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,076 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,076 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:09,078 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,082 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,085 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,085 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,087 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,102 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,103 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:09,103 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,103 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,103 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:09,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,112 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,112 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,114 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,123 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,129 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,129 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:09,129 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,129 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,129 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:09,130 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:09,134 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,138 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,138 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,140 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,154 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,155 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:09,155 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,155 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,155 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:09,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:09,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:09,164 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,164 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,166 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,198 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,198 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:09,198 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,198 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,199 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:09,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:09,204 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:09,208 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,208 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,210 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,219 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,225 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,225 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:09,225 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,226 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,226 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:09,227 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:09,231 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:09,235 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,235 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,237 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,253 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,255 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,255 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:09,256 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,256 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,256 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:09,257 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:09,261 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:09,266 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,266 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,268 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,282 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,283 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:09,283 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,283 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,283 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:09,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:09,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:09,288 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,289 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,291 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,295 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:16:09,305 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:16:09,306 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:09,306 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,306 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,306 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:09,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:09,307 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:09,308 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,308 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,309 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:09,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,311 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,312 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:09,312 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,312 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,312 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:09,312 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:09,313 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:09,313 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,313 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,314 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:09,322 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,328 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,342 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:09,343 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:09,343 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:09,344 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,344 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:09,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:09,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:09,348 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:09,348 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,348 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:09,349 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,350 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,351 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,351 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,351 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:09,352 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-31 06:16:09,352 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,352 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:09,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:09,356 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:09,357 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-31 06:16:09,357 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,357 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-31 06:16:09,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,360 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,362 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:09,362 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,362 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,362 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:09,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:09,366 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:09,370 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,370 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,372 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,390 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,390 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:09,390 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,390 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,390 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:09,392 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,396 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:09,400 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,400 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,402 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,417 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,420 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,420 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:09,421 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,421 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,421 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:09,422 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,426 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,430 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,431 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,433 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,438 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,470 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,471 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:09,471 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,471 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,471 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:09,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,476 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,480 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,480 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,482 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,487 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,498 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,499 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:09,499 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,499 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,499 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:09,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,508 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,508 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,510 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,524 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,526 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,526 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:09,527 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,527 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,527 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:09,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,536 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,538 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,552 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,553 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:09,553 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,553 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,553 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:09,554 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:09,558 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,562 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,562 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,564 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,569 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,579 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,579 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:09,579 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,580 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,580 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:09,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:09,585 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:09,588 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,589 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,591 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,599 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,603 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,605 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,605 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:09,606 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,606 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,606 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:09,607 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:09,611 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:09,614 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,615 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,617 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,621 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,640 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,641 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:09,641 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,641 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,641 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:09,643 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:09,646 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:09,650 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,650 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,652 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,662 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,668 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,670 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,670 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:09,671 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,671 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,671 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:09,672 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:09,676 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:09,680 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,680 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,682 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,700 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,701 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:09,701 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,701 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,701 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:09,703 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:09,703 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:09,707 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,707 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,709 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:16:09,724 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:16:09,724 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:09,724 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,724 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,724 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:09,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:09,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:09,727 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,727 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:09,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,730 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,730 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:09,730 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,731 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:09,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:09,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:09,732 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,732 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,732 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:09,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:09,763 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:09,765 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:09,765 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:09,765 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,765 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:09,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:09,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:09,770 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:09,770 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,771 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:09,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,774 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,774 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:09,774 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-31 06:16:09,774 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:09,774 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:09,775 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:09,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:09,779 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-31 06:16:09,779 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:09,779 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-31 06:16:09,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,782 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:09,782 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:09,784 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:09,784 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,784 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,784 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:09,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:09,788 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:09,792 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,792 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,794 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,802 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,809 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,809 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:09,809 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,809 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,809 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:09,811 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:09,818 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,818 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,820 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,836 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,836 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:09,836 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,836 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,836 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:09,838 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,841 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:09,845 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,845 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,847 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,856 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,860 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,862 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,862 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:09,863 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,863 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,863 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:09,865 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:09,872 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,872 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,874 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,895 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,897 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,897 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:09,897 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,898 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,898 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:09,899 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:09,907 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,907 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,909 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,941 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,947 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,948 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:09,948 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,948 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,948 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:09,950 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,953 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:09,957 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,957 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,959 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,965 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,975 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:09,975 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:09,975 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:09,975 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,975 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:09,977 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:09,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:09,984 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:09,984 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:09,986 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:09,991 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:09,999 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,001 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,001 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:10,001 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,002 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,002 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:10,003 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:10,007 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:10,010 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,010 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,012 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,028 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,028 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:10,028 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,029 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,029 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:10,030 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:10,034 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:10,037 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,037 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,039 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,047 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,053 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,053 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:10,054 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,054 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,054 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:10,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:10,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:10,062 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,062 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,064 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,080 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,080 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:10,080 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,080 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,080 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:10,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:10,085 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:10,088 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,088 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,090 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,099 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,103 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,105 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,105 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:10,106 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,106 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,106 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:10,107 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:10,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:10,111 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,111 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,113 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,118 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,139 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:16:10,142 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:16:10,142 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:10,142 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,142 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,142 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:10,144 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:10,144 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:10,145 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,145 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,145 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:10,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,148 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:10,148 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:10,148 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,148 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,148 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:10,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:10,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:10,150 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,150 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,150 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:10,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,178 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:10,179 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:16:10,179 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:16:10,179 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,179 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:16:10,182 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:10,183 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:10,183 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:16:10,183 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,184 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:16:10,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,185 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,186 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:10,186 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:16:10,187 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-31 06:16:10,187 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,187 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:16:10,187 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:10,191 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:16:10,191 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-31 06:16:10,191 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,192 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-31 06:16:10,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,194 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,195 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:10,196 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:16:10,196 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,197 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,197 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:16:10,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:10,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:16:10,204 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,204 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,206 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,214 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,220 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,221 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:16:10,221 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,221 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,221 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:16:10,222 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:10,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:16:10,229 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,229 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,231 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,236 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,240 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,244 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,246 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,246 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:16:10,246 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,246 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,247 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:16:10,248 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:10,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:16:10,255 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,255 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,257 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,265 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,269 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,271 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,272 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:16:10,272 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,272 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,272 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:16:10,273 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:10,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:16:10,280 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,280 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,282 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,290 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,295 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,297 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,297 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:16:10,297 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,297 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,297 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:16:10,299 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:10,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:16:10,305 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,306 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,307 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,315 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,319 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,321 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,321 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:16:10,321 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,322 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,322 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:16:10,323 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:10,326 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:16:10,329 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,330 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,331 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,340 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,345 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,346 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:16:10,346 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,346 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,346 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:16:10,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:10,350 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:16:10,353 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,354 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,355 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,415 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,415 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:16:10,415 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,416 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,416 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:16:10,417 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:10,421 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:16:10,424 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,425 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,427 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,442 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,442 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:16:10,442 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,442 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,443 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:16:10,444 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:10,448 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:16:10,452 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,452 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,454 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,459 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,463 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,471 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,472 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:16:10,472 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,472 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,472 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:16:10,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:10,479 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:16:10,484 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,484 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,486 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,503 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,503 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:16:10,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,504 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,504 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:16:10,505 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:10,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:16:10,513 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,513 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,516 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,535 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,535 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:16:10,535 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,535 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,535 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:16:10,537 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:10,538 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:16:10,542 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,542 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:16:10,544 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:16:10,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,557 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:16:10,560 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:16:10,560 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:16:10,560 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,560 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,560 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:16:10,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:10,562 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:16:10,562 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,562 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,563 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:10,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:16:10,565 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:16:10,566 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:16:10,566 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:16:10,566 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:16:10,566 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:16:10,566 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:16:10,567 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:16:10,567 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:16:10,567 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:16:10,568 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:16:10,576 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,590 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:16:10,598 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:16:10,600 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 06:16:10,600 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:16:10,601 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 06:16:10,601 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:16:10,601 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 06:16:10,601 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:16:10,601 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 06:16:10,601 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:16:10,611 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 06:16:10,612 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 06:16:10,613 [wrapper.py:88 in layer_reset] DEBUG - lm_head from flexgen to old.
