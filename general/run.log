2023-10-09 07:03:27,275 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpca59tug7
2023-10-09 07:03:27,276 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpca59tug7/_remote_module_non_scriptable.py
2023-10-09 07:03:27,755 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-09 07:03:27,878 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 07:03:29,669 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-09 07:03:30,000 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 07:03:30,000 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 07:03:30,000 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 07:03:30,001 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 07:03:30,999 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 07:03:31,125 [model.py:159 in is_on_disk] INFO - [], []
2023-10-09 07:03:31,164 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 07:03:31,289 [model.py:159 in is_on_disk] INFO - [], []
2023-10-09 07:03:31,290 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/bigscience.bloom-560m'
2023-10-09 07:03:31,300 [model.py:138 in get_policy_weight_map] DEBUG - transformer.word_embeddings, [0. 0. 1.], size_todo: 302313472
2023-10-09 07:03:31,301 [model.py:138 in get_policy_weight_map] DEBUG - transformer.word_embeddings_layernorm, [0.00000000e+00 3.98593761e-06 9.99996014e-01], size_todo: 302311424
2023-10-09 07:03:31,302 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.03116083 0.96883917], size_todo: 289715200
2023-10-09 07:03:31,303 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.05953522 0.94046478], size_todo: 277118976
2023-10-09 07:03:31,304 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.08548396 0.91451604], size_todo: 264522752
2023-10-09 07:03:31,304 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.10930533 0.89069467], size_todo: 251926528
2023-10-09 07:03:31,305 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.13125066 0.86874934], size_todo: 239330304
2023-10-09 07:03:31,306 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.15153316 0.84846684], size_todo: 226734080
2023-10-09 07:03:31,307 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.17033494 0.82966506], size_todo: 214137856
2023-10-09 07:03:31,308 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.7, [0.         0.18781242 0.81218758], size_todo: 201541632
2023-10-09 07:03:31,309 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.19277305 0.80722695], size_todo: 188945408
2023-10-09 07:03:31,310 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.20836231 0.79163769], size_todo: 176349184
2023-10-09 07:03:31,310 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.10, [0.         0.21235237 0.78764763], size_todo: 163752960
2023-10-09 07:03:31,311 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.11, [0.        0.2263748 0.7736252], size_todo: 151156736
2023-10-09 07:03:31,312 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.22958653 0.77041347], size_todo: 138560512
2023-10-09 07:03:31,313 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.24229253 0.75770747], size_todo: 125964288
2023-10-09 07:03:31,314 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.24487307 0.75512693], size_todo: 113368064
2023-10-09 07:03:31,314 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.25646083 0.74353917], size_todo: 100771840
2023-10-09 07:03:31,315 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.25852448 0.74147552], size_todo: 88175616
2023-10-09 07:03:31,316 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.26915308 0.73084692], size_todo: 75579392
2023-10-09 07:03:31,317 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27078978 0.72921022], size_todo: 62983168
2023-10-09 07:03:31,318 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.28058853 0.71941147], size_todo: 50386944
2023-10-09 07:03:31,319 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.20, [0.        0.2818699 0.7181301], size_todo: 37790720
2023-10-09 07:03:31,319 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.21, [0.         0.29094504 0.70905496], size_todo: 25194496
2023-10-09 07:03:31,320 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.22, [0.        0.2919287 0.7080713], size_todo: 12598272
2023-10-09 07:03:31,321 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.23, [0.         0.30036843 0.69963157], size_todo: 2048
2023-10-09 07:03:31,322 [model.py:138 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-09 07:03:31,322 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-09 07:03:31,322 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-09 07:03:31,325 [model.py:148 in get_policy_weight_map] INFO - CausalLM bigscience/bloom-560m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.31 GiB (30.04%), Disk Mem 0.73 Gib (69.96%)
2023-10-09 07:03:31,326 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-09 07:03:31,369 [forward.py:46 in to_test_forward] DEBUG - transformer.word_embeddings to test forward
2023-10-09 07:03:31,369 [forward.py:46 in to_test_forward] DEBUG - transformer.word_embeddings_layernorm to test forward
2023-10-09 07:03:31,369 [forward.py:46 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-09 07:03:31,370 [forward.py:46 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-09 07:03:31,371 [forward.py:46 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.20 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.21 to test forward
2023-10-09 07:03:31,372 [forward.py:46 in to_test_forward] DEBUG - transformer.h.22 to test forward
2023-10-09 07:03:31,373 [forward.py:46 in to_test_forward] DEBUG - transformer.h.23 to test forward
2023-10-09 07:03:31,373 [forward.py:46 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-09 07:03:31,373 [forward.py:46 in to_test_forward] DEBUG - lm_head to test forward
2023-10-09 07:03:31,416 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 07:03:32,236 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:32,237 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:32,237 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:32,238 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:32,239 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:32,254 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:32,256 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:32,263 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:32,264 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:32,271 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:32,273 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:32,280 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:32,281 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:32,288 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:32,290 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:32,296 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:32,298 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:32,305 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:32,307 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:32,315 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:32,317 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:32,324 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:32,326 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:32,337 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:32,339 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:32,346 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:32,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:32,355 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:32,356 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:32,364 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:32,365 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:32,373 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:32,374 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:32,381 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:32,383 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:32,391 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:32,392 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:32,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:32,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:32,407 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:32,408 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:32,415 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:32,417 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:32,423 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:32,424 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:32,431 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:32,433 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:32,439 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:32,441 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:32,447 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:32,449 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:32,456 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:32,458 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:32,458 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:32,459 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:32,506 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:32,529 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-09 07:03:32,529 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:32,636 [forward.py:26 in reset_forward] DEBUG - transformer.word_embeddings from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.word_embeddings_layernorm from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.h.0 from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.h.1 from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.h.2 from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.h.3 from test to old.
2023-10-09 07:03:32,637 [forward.py:26 in reset_forward] DEBUG - transformer.h.4 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.5 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.6 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.7 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.8 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.9 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.10 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.11 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.12 from test to old.
2023-10-09 07:03:32,638 [forward.py:26 in reset_forward] DEBUG - transformer.h.13 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.14 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.15 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.16 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.17 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.18 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.19 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.20 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.21 from test to old.
2023-10-09 07:03:32,639 [forward.py:26 in reset_forward] DEBUG - transformer.h.22 from test to old.
2023-10-09 07:03:32,640 [forward.py:26 in reset_forward] DEBUG - transformer.h.23 from test to old.
2023-10-09 07:03:32,640 [forward.py:26 in reset_forward] DEBUG - transformer.ln_f from test to old.
2023-10-09 07:03:32,640 [forward.py:26 in reset_forward] DEBUG - lm_head from test to old.
2023-10-09 07:03:32,640 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.word_embeddings to flexgen forward
2023-10-09 07:03:32,640 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.word_embeddings_layernorm to flexgen forward
2023-10-09 07:03:32,640 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-09 07:03:32,640 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-09 07:03:32,641 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-09 07:03:32,642 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.20 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.21 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.22 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.h.23 to flexgen forward
2023-10-09 07:03:32,643 [forward.py:117 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-09 07:03:32,644 [forward.py:117 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-09 07:03:32,687 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 07:03:33,370 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:33,371 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:33,372 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9]),)
2023-10-09 07:03:33,372 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:33,372 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9]),)
2023-10-09 07:03:33,372 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:33,372 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:33,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:33,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:33,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:33,373 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 1024])
2023-10-09 07:03:33,374 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 1024])
2023-10-09 07:03:33,374 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:33,374 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:33,375 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:33,378 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,379 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:33,379 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,379 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:33,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:33,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:33,380 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:33,380 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:33,380 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 1024])
2023-10-09 07:03:33,380 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 1024])
2023-10-09 07:03:33,380 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:33,381 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:33,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:33,388 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,388 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,388 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,389 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,389 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:33,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:33,408 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:33,418 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:33,424 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,425 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,425 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:33,426 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:33,429 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:33,432 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,432 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,432 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,433 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,433 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:33,439 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:33,446 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:33,453 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:33,462 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,463 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,463 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:33,465 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:33,469 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:33,474 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,475 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,475 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,475 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,476 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:33,485 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:33,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:33,499 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:33,506 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,506 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,507 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:33,508 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:33,511 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:33,514 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,514 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,514 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,515 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,515 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:33,522 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:33,529 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:33,535 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:33,542 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,542 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,542 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:33,544 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:33,546 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:33,550 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,550 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,550 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,550 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,550 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:33,557 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:33,563 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:33,569 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:33,575 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,575 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,575 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:33,577 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:33,579 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:33,583 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,583 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,583 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,583 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,583 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:33,590 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:33,595 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:33,602 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:33,607 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,608 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,608 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:33,610 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:33,612 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:33,616 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,616 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,616 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,616 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,616 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:33,632 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:33,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:33,644 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:33,650 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,651 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,651 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:33,652 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:33,655 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:33,659 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,659 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,659 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,660 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,660 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:33,665 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:33,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:33,675 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:33,682 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,683 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,683 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:33,685 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:33,688 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:33,692 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,692 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,692 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,692 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:33,698 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:33,704 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:33,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:33,713 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,714 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,714 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:33,717 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:33,720 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:33,723 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,723 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,723 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,723 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:33,730 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:33,736 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:33,742 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:33,748 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,749 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,749 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:33,751 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:33,754 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:33,757 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,758 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,758 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,758 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,758 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:33,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:33,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:33,775 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:33,780 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,781 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,781 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:33,783 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:33,786 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:33,790 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,790 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,790 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,790 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,791 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:33,798 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:33,804 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:33,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:33,818 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,819 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,819 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:33,820 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:33,823 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:33,827 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,827 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,828 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,828 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,828 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:33,836 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:33,844 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:33,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:33,858 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,858 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,859 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:33,860 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:33,863 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:33,867 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,867 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,867 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,868 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,868 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:33,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:33,886 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:33,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:33,900 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,901 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,901 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:33,902 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:33,905 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:33,909 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,909 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,910 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,910 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,910 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:33,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:33,924 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:33,932 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:33,938 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,939 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,939 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:33,941 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:33,944 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:33,947 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,947 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,948 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,948 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,948 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:33,956 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:33,962 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:33,968 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:33,975 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:33,976 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:33,976 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:33,978 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:33,981 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:33,984 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:33,985 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:33,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:33,985 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:33,985 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:33,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:34,000 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:34,007 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:34,014 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,014 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,014 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:34,016 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:34,019 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:34,022 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,022 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,023 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,023 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:34,031 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:34,037 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:34,044 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:34,051 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,052 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,052 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:34,053 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:34,056 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:34,059 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,060 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,060 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,060 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,060 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:34,068 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:34,075 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:34,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:34,088 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,089 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,089 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:34,091 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:34,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:34,097 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,097 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,097 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,098 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,098 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:34,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:34,112 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:34,118 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:34,125 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,126 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,126 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:34,127 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:34,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:34,133 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,133 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,134 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,134 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,134 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:34,175 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:34,182 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:34,188 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:34,195 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,197 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,198 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:34,199 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:34,203 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:34,206 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,207 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,207 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,207 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:34,215 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:34,220 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:34,231 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:34,236 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,237 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,238 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:34,239 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:34,242 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:34,246 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,246 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,247 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,247 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,247 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:34,253 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:34,258 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:34,264 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:34,268 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,269 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,270 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:34,272 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:34,275 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:34,276 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,276 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 9])}
2023-10-09 07:03:34,277 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,277 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 9, 9]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 9])}
2023-10-09 07:03:34,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:34,283 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:34,288 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:34,294 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:34,298 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 1024]), (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])))
2023-10-09 07:03:34,300 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 1024]), (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])))
2023-10-09 07:03:34,300 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:34,302 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:34,303 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:34,303 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,304 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:34,304 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,305 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:34,305 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:34,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:34,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:34,307 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:34,307 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 1024])
2023-10-09 07:03:34,307 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 1024])
2023-10-09 07:03:34,308 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:34,309 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:34,309 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:34,310 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 1024]),)
2023-10-09 07:03:34,310 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:34,311 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 1024]),)
2023-10-09 07:03:34,311 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:34,311 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:34,368 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:34,415 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:34,466 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:34,517 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 250880])
2023-10-09 07:03:34,535 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 250880])
2023-10-09 07:03:34,536 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:34,562 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:34,563 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:34,564 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:34,564 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:34,564 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:34,565 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:34,565 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:34,565 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:34,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:34,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:34,566 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:34,566 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:34,566 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:34,567 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:34,567 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:34,571 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,571 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:34,571 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,571 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:34,572 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:34,572 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:34,572 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:34,572 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:34,572 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:34,573 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:34,573 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:34,573 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:34,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:34,579 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,580 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,580 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,580 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,580 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:34,584 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:34,588 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:34,591 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:34,594 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,594 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,594 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:34,596 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:34,598 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:34,601 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,602 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,602 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,602 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,602 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:34,607 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:34,610 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:34,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:34,616 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,617 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,617 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:34,618 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:34,621 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:34,624 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,624 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,624 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,624 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:34,629 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:34,632 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:34,635 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:34,639 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,640 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,640 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:34,641 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:34,644 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:34,647 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,648 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,648 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,648 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:34,652 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:34,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:34,659 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:34,662 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,662 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,663 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:34,664 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:34,666 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:34,670 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,670 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,670 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,670 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:34,675 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:34,679 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:34,682 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:34,685 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,686 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,686 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:34,687 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:34,690 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:34,693 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,693 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,693 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,694 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,694 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:34,698 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:34,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:34,705 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:34,709 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,710 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,710 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:34,711 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:34,714 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:34,718 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,718 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,718 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,718 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,718 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:34,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:34,726 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:34,730 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:34,734 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,734 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,735 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:34,736 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:34,739 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:34,741 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,742 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,742 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,742 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,742 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:34,748 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:34,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:34,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:34,757 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,758 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,758 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:34,760 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:34,763 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:34,766 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,766 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,766 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,767 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,767 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:34,771 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:34,774 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:34,777 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:34,780 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,780 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,781 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:34,783 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:34,785 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:34,788 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,788 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,789 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,789 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,789 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:34,793 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:34,797 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:34,800 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:34,804 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,805 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,805 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:34,807 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:34,809 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:34,813 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,813 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,813 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,813 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,813 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:34,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:34,822 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:34,826 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:34,828 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,829 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,829 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:34,831 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:34,833 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:34,836 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,837 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,837 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,837 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,837 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:34,841 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:34,845 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:34,849 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:34,852 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,853 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,853 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:34,855 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:34,857 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:34,860 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,861 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,861 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,861 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,861 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:34,866 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:34,869 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:34,873 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:34,876 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,876 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,877 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:34,878 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:34,881 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:34,884 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,884 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,884 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,884 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:34,890 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:34,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:34,897 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:34,901 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,901 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,901 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:34,902 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:34,905 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:34,908 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,909 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,909 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,909 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,909 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:34,914 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:34,919 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:34,923 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:34,927 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,928 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,928 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:34,930 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:34,933 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:34,936 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,936 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,936 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,936 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:34,942 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:34,946 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:34,950 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:34,954 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,954 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,954 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:34,956 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:34,958 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:34,962 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,962 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,962 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,962 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,962 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:34,967 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:34,971 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:34,974 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:34,978 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:34,978 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:34,978 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:34,980 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:34,983 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:34,986 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:34,986 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:34,986 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:34,986 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:34,987 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:34,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:34,995 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:34,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:35,003 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,003 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,003 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:35,005 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:35,007 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:35,011 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,011 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,011 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,011 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,011 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:35,016 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:35,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:35,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:35,026 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,027 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,027 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:35,029 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:35,031 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:35,034 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,035 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,035 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,035 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,035 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:35,039 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:35,043 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:35,046 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:35,049 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,050 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,050 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:35,051 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:35,054 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:35,057 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,058 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,058 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,058 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,058 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:35,062 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:35,066 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:35,070 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:35,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,073 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:35,075 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:35,078 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:35,081 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,081 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,081 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,081 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,082 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:35,086 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:35,090 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:35,094 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:35,098 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,098 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,098 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:35,100 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:35,102 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:35,105 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,106 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,106 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,106 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,106 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:35,110 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:35,115 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:35,119 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:35,122 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,123 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,123 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:35,125 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:35,127 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:35,128 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,128 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 9]), torch.Size([128, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-09 07:03:35,129 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,129 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 9]), torch.Size([32, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-09 07:03:35,129 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:35,134 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:35,137 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:35,141 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:35,145 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-09 07:03:35,146 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-09 07:03:35,146 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:35,147 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:35,148 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:35,148 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,148 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:35,149 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,149 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:35,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:35,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:35,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:35,150 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:35,150 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:35,150 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:35,150 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:35,151 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:35,151 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:35,151 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,152 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:35,152 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,152 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:35,152 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:35,214 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:35,264 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:35,314 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:35,362 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:35,364 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:35,364 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:35,394 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:35,395 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:35,397 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:35,397 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:35,397 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:35,397 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:35,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:35,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:35,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:35,399 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:35,399 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:35,399 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:35,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:35,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:35,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:35,404 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,404 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:35,404 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,404 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:35,405 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:35,405 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:35,405 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:35,405 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:35,406 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:35,406 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:35,406 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:35,407 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:35,410 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:35,414 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,414 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,414 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,415 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,415 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:35,419 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:35,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:35,429 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:35,434 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,435 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,436 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:35,438 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:35,442 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:35,448 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,448 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,449 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,449 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,449 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:35,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:35,460 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:35,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:35,471 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,472 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,472 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:35,474 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:35,478 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:35,485 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,485 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,485 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,486 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,486 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:35,493 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:35,498 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:35,503 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:35,509 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,511 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,511 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:35,513 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:35,518 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:35,523 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,524 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,524 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,524 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:35,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:35,535 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:35,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:35,546 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,546 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,547 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:35,549 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:35,553 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:35,557 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,558 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,558 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,558 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,558 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:35,563 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:35,568 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:35,573 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:35,577 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,578 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,579 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:35,581 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:35,585 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:35,591 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,591 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,592 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,592 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:35,598 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:35,603 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:35,608 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:35,613 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,614 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,614 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:35,616 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:35,621 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:35,626 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,627 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,627 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,627 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,627 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:35,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:35,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:35,643 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:35,647 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,648 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,648 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:35,650 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:35,652 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:35,655 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,656 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,656 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,656 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:35,661 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:35,665 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:35,669 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:35,673 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,674 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,674 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:35,676 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:35,678 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:35,682 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,682 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,682 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,683 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,683 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:35,687 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:35,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:35,696 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:35,700 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,700 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,700 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:35,702 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:35,705 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:35,708 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,708 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,708 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,709 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:35,714 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:35,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:35,722 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:35,725 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,726 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,726 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:35,727 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:35,730 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:35,734 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,734 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,734 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,734 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,734 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:35,740 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:35,744 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:35,748 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:35,751 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,752 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,752 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:35,754 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:35,757 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:35,761 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,761 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,761 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,761 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,761 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:35,766 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:35,770 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:35,773 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:35,777 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,778 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,778 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:35,779 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:35,783 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:35,786 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,787 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,787 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,787 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,787 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:35,792 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:35,796 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:35,800 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:35,803 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,804 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,804 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:35,806 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:35,809 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:35,812 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,813 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,813 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,813 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,814 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:35,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:35,822 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:35,826 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:35,831 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,831 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,832 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:35,833 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:35,836 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:35,840 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,841 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,841 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,841 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,841 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:35,846 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:35,850 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:35,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:35,859 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,859 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,860 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:35,862 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:35,865 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:35,868 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,869 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,869 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,869 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:35,875 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:35,879 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:35,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:35,887 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,888 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,888 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:35,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:35,893 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:35,897 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,897 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,898 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,898 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,898 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:35,903 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:35,907 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:35,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:35,915 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,915 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,916 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:35,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:35,921 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:35,925 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,925 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,925 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,925 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,926 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:35,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:35,935 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:35,939 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:35,943 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,943 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,944 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:35,945 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:35,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:35,952 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,953 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,953 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,953 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,954 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:35,959 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:35,963 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:35,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:35,974 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:35,975 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:35,975 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:35,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:35,980 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:35,984 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:35,984 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:35,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:35,985 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:35,985 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:35,990 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:35,994 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:35,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:36,002 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:36,003 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:36,003 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:36,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:36,008 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:36,011 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,012 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:36,012 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,012 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:36,013 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:36,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:36,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:36,025 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:36,028 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:36,029 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:36,029 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:36,031 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:36,034 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:36,037 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,038 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:36,038 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,038 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:36,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:36,043 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:36,047 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:36,051 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:36,055 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:36,056 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:36,057 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:36,058 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:36,061 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:36,064 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,065 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:36,065 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,065 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:36,065 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:36,072 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:36,076 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:36,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:36,084 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:36,084 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:36,085 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:36,087 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:36,090 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:36,090 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,090 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-09 07:03:36,091 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,091 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-09 07:03:36,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:36,096 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:36,101 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:36,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:36,109 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-09 07:03:36,110 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-09 07:03:36,110 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:36,111 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:36,112 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:36,113 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,113 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:36,113 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,113 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:36,113 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:36,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:36,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:36,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:36,114 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:36,115 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:36,115 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:36,115 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:36,116 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:36,116 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,116 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:36,117 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,117 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:36,117 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:36,173 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:36,222 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:36,272 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:36,320 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:36,321 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:36,322 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:36,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:36,349 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:36,350 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:36,350 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:36,350 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:36,351 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:36,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:36,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:36,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:36,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:36,352 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:36,352 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:36,353 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:36,353 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:36,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:36,358 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,358 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:36,359 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,359 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:36,359 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:36,359 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:36,359 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:36,360 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:36,360 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:36,360 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:36,360 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:36,361 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:36,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:36,368 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,368 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,369 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:36,374 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:36,378 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:36,382 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:36,386 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,387 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,387 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:36,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:36,391 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:36,395 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,395 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,396 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,396 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,396 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:36,401 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:36,406 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:36,411 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:36,415 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,415 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,416 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:36,417 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:36,420 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:36,424 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,424 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,424 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,425 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,425 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:36,430 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:36,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:36,440 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:36,444 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,445 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,445 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:36,446 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:36,449 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:36,453 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,454 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,454 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,454 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:36,460 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:36,469 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:36,474 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:36,479 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,480 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,480 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:36,482 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:36,485 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:36,488 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,489 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,489 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,489 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,489 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:36,495 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:36,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:36,506 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:36,511 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,512 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,512 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:36,514 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:36,517 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:36,521 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,521 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,521 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,522 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,522 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:36,554 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:36,588 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:36,621 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:36,635 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,636 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,637 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:36,638 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:36,642 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:36,647 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,648 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,649 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,649 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:36,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:36,659 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:36,663 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:36,667 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,667 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,668 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:36,669 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:36,672 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:36,675 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,675 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,676 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,676 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,676 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:36,681 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:36,685 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:36,690 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:36,693 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,694 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,694 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:36,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:36,700 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:36,704 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,704 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,704 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,704 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,705 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:36,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:36,713 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:36,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:36,722 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,723 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,724 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:36,726 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:36,729 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:36,733 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,733 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,733 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,733 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,734 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:36,738 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:36,742 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:36,746 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:36,749 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,750 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,750 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:36,751 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:36,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:36,759 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,759 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,759 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,759 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,760 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:36,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:36,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:36,774 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:36,796 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,797 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,797 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:36,799 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:36,802 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:36,805 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,806 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,806 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,806 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,806 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:36,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:36,813 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:36,816 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:36,819 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,820 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,820 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:36,821 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:36,824 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:36,828 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,828 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,829 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,829 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,829 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:36,833 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:36,837 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:36,840 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:36,842 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,843 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,843 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:36,845 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:36,848 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:36,851 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,851 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,851 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,851 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:36,856 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:36,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:36,861 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:36,864 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,864 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,864 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:36,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:36,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:36,872 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,872 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,872 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,872 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,872 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:36,877 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:36,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:36,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:36,886 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,887 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,887 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:36,889 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:36,891 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:36,894 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,895 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,895 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,895 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,895 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:36,900 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:36,903 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:36,907 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:36,910 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,910 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,910 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:36,911 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:36,914 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:36,917 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,918 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,918 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:36,923 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:36,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:36,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:36,933 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,934 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,934 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:36,936 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:36,938 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:36,941 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,941 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,942 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,942 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,942 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:36,947 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:36,951 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:36,954 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:36,957 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,957 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,957 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:36,959 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:36,961 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:36,965 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,965 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,965 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,965 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,965 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:36,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:36,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:36,976 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:36,980 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:36,980 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:36,980 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:36,982 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:36,985 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:36,988 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:36,988 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:36,988 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:36,988 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:36,988 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:36,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:36,998 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:37,000 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:37,003 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:37,003 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:37,003 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:37,005 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:37,007 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:37,011 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,011 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:37,011 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,011 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:37,011 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:37,015 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:37,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:37,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:37,025 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:37,026 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:37,026 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:37,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:37,030 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:37,033 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,033 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:37,034 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,034 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:37,034 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:37,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:37,041 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:37,044 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:37,048 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:37,048 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:37,048 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:37,050 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:37,052 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:37,056 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:37,056 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,056 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:37,057 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:37,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:37,064 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:37,066 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:37,069 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:37,070 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:37,070 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:37,072 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:37,074 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:37,075 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,075 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-09 07:03:37,075 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,075 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-09 07:03:37,076 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:37,079 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:37,083 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:37,086 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:37,088 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-09 07:03:37,089 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-09 07:03:37,089 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:37,090 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:37,091 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:37,092 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,092 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,092 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,092 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,092 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:37,092 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:37,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:37,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:37,093 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:37,093 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:37,093 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:37,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:37,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:37,095 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,095 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,095 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,095 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,095 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:37,140 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:37,181 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:37,220 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:37,261 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:37,262 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:37,263 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:37,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:37,287 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:37,288 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:37,288 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,288 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:37,288 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:37,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:37,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:37,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:37,290 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:37,290 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:37,290 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:37,290 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:37,291 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:37,294 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,295 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,295 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,295 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,295 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:37,295 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:37,296 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:37,297 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:37,297 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:37,297 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:37,297 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:37,298 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:37,301 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:37,305 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,305 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,306 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,306 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:37,312 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:37,316 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:37,321 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:37,325 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,326 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,326 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:37,328 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:37,331 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:37,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,334 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,334 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,335 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,335 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:37,340 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:37,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:37,350 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:37,354 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,355 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,355 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:37,357 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:37,360 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:37,363 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,363 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,364 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,364 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,364 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:37,370 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:37,374 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:37,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:37,383 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,384 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,384 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:37,386 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:37,389 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:37,392 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,393 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,393 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,393 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:37,399 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:37,403 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:37,408 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:37,413 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,414 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,414 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:37,416 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:37,419 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:37,422 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,422 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,423 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,423 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,423 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:37,430 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:37,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:37,441 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:37,447 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,448 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,448 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:37,450 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:37,452 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:37,456 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,456 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,456 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,457 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,457 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:37,463 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:37,468 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:37,473 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:37,477 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,478 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,478 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:37,480 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:37,482 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:37,486 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,486 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,486 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,487 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,487 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:37,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:37,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:37,502 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:37,507 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,507 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,508 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:37,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:37,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:37,515 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,515 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,516 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,516 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,516 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:37,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:37,526 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:37,532 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:37,536 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,537 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,537 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:37,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:37,541 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:37,545 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,545 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,545 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,546 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,546 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:37,551 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:37,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:37,561 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:37,566 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,566 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,567 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:37,569 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:37,571 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:37,575 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,575 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,575 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,575 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,576 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:37,581 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:37,586 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:37,590 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:37,593 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,593 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,593 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:37,595 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:37,597 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:37,601 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,601 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,601 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,601 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,601 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:37,606 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:37,609 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:37,612 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:37,615 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,616 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,616 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:37,617 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:37,620 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:37,623 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,623 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,624 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,624 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:37,628 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:37,631 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:37,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:37,637 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,638 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,638 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:37,639 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:37,642 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:37,645 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,645 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,646 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,646 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,646 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:37,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:37,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:37,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:37,659 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,660 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,660 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:37,662 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:37,665 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:37,668 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,668 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,668 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,668 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,669 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:37,673 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:37,676 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:37,679 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:37,682 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,683 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,683 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:37,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:37,687 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:37,690 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,690 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,691 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,691 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,691 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:37,695 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:37,699 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:37,703 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:37,705 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,706 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,706 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:37,708 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:37,711 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:37,714 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,714 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,714 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,714 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,715 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:37,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:37,722 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:37,725 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:37,728 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,728 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,729 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:37,730 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:37,733 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:37,736 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,736 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,736 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,737 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,737 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:37,741 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:37,744 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:37,747 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:37,750 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,751 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,751 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:37,752 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:37,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:37,758 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,758 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,759 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,759 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,759 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:37,763 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:37,767 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:37,770 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:37,773 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,773 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,773 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:37,775 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:37,778 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:37,781 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,781 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,781 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,782 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,782 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:37,786 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:37,789 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:37,792 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:37,795 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,796 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,796 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:37,798 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:37,801 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:37,804 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,804 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,805 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,805 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,805 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:37,809 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:37,812 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:37,815 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:37,818 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,819 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,819 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:37,820 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:37,823 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:37,826 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,827 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,827 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,827 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:37,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:37,835 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:37,837 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:37,841 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,841 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,841 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:37,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:37,846 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:37,849 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,849 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,849 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,849 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,850 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:37,854 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:37,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:37,861 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:37,864 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,864 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,864 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:37,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:37,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:37,872 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,872 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,872 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,872 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,872 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:37,877 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:37,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:37,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:37,886 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,887 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,887 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:37,889 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:37,892 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:37,892 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-09 07:03:37,893 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,893 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-09 07:03:37,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:37,897 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:37,900 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:37,903 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:37,906 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-09 07:03:37,907 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-09 07:03:37,907 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:37,908 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:37,909 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:37,910 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,910 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,910 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,910 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,910 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:37,910 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:37,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:37,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:37,911 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:37,911 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:37,911 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:37,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:37,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:37,913 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:37,913 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:37,913 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:37,913 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:37,913 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:37,959 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:37,998 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:38,036 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:38,075 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:38,076 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:38,076 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:38,100 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:38,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:38,102 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:38,102 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,103 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:38,103 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,103 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:38,103 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:38,104 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:38,104 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:38,105 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:38,105 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:38,105 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:38,105 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:38,106 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:38,110 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,110 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,110 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,110 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,110 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:38,110 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:38,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:38,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:38,111 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:38,111 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:38,111 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:38,112 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:38,115 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:38,118 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,118 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,118 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,119 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,119 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:38,123 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:38,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:38,130 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:38,134 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,134 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,134 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:38,136 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:38,138 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:38,142 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,142 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,142 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,142 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,142 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:38,146 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:38,150 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:38,153 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:38,156 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,157 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,157 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:38,158 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:38,161 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:38,165 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,165 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,165 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,165 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,165 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:38,170 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:38,173 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:38,177 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:38,180 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,181 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,181 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:38,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:38,185 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:38,188 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,189 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,189 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,189 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,189 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:38,194 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:38,197 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:38,214 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:38,222 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,223 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,224 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:38,225 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:38,228 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:38,231 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,231 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,231 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,232 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,232 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:38,236 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:38,240 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:38,244 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:38,247 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,248 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,248 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:38,250 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:38,253 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:38,257 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,257 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,258 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,258 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:38,262 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:38,266 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:38,270 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:38,274 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,274 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,274 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:38,276 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:38,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:38,282 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,282 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,283 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,283 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,283 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:38,288 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:38,291 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:38,295 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:38,298 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,299 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,299 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:38,300 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:38,303 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:38,306 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,306 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,306 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,307 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,307 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:38,311 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:38,315 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:38,318 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:38,322 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,322 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,322 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:38,323 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:38,326 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:38,332 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,333 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,333 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,333 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,333 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:38,341 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:38,346 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:38,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:38,357 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,358 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,358 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:38,361 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:38,365 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:38,368 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,369 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,369 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:38,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:38,376 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:38,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:38,383 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,383 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,383 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:38,385 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:38,387 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:38,391 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,391 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,391 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,391 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,391 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:38,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:38,400 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:38,404 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:38,407 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,407 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,407 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:38,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:38,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:38,415 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,415 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,415 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,415 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:38,420 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:38,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:38,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:38,432 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,432 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,433 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:38,435 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:38,439 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:38,444 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,444 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,444 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,445 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,445 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:38,449 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:38,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:38,458 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:38,462 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,463 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,463 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:38,465 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:38,468 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:38,471 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,471 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,471 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,471 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,471 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:38,476 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:38,480 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:38,484 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:38,487 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,488 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,488 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:38,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:38,493 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:38,496 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,497 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,497 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,497 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:38,502 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:38,505 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:38,509 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:38,513 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,513 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,514 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:38,515 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:38,518 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:38,521 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,521 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,521 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,521 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,522 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:38,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:38,528 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:38,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:38,533 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,534 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,534 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:38,535 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:38,538 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:38,541 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,542 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,542 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,542 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:38,546 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:38,550 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:38,554 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:38,557 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,558 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,558 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:38,559 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:38,562 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:38,565 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,565 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,566 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,566 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:38,570 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:38,574 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:38,577 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:38,582 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,583 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,583 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:38,584 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:38,587 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:38,590 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,591 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,591 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,591 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,591 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:38,595 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:38,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:38,603 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:38,606 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,607 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,607 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:38,608 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:38,611 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:38,614 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,614 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,615 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,615 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,615 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:38,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:38,627 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:38,631 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:38,634 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,635 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,635 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:38,636 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:38,639 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:38,642 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,642 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,643 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,643 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,643 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:38,647 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:38,651 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:38,655 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:38,658 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,659 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,659 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:38,661 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:38,663 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:38,667 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,667 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,667 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,667 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,667 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:38,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:38,675 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:38,678 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:38,681 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,682 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,682 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:38,683 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:38,686 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:38,689 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,689 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,690 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,690 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,690 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:38,694 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:38,698 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:38,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:38,705 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,706 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,706 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:38,708 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:38,711 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:38,711 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,711 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-09 07:03:38,712 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,712 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-09 07:03:38,712 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:38,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:38,720 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:38,724 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:38,727 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-09 07:03:38,728 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-09 07:03:38,728 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:38,729 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:38,730 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:38,730 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,730 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,731 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,731 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:38,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:38,732 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:38,732 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:38,732 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:38,732 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:38,732 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:38,733 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:38,733 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:38,734 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,734 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,734 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,734 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,734 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:38,783 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:38,823 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:38,863 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:38,902 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:38,903 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:38,903 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:38,927 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:38,928 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:38,929 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:38,929 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,929 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:38,929 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,929 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:38,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:38,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:38,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:38,930 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:38,930 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:38,930 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:38,931 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:38,931 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:38,935 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,935 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:38,935 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,935 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:38,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:38,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:38,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:38,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:38,936 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:38,937 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:38,937 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:38,937 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:38,940 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:38,943 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,943 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:38,944 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,944 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:38,944 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:38,948 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:38,952 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:38,956 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:38,959 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:38,960 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:38,960 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:38,961 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:38,964 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:38,968 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,968 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:38,968 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,968 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:38,968 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:38,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:38,976 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:38,980 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:38,983 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:38,984 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:38,984 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:38,985 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:38,988 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:38,991 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:38,991 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:38,991 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:38,992 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:38,992 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:38,997 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:39,000 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:39,005 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:39,008 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,009 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,009 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:39,011 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:39,013 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:39,017 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,017 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,017 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,017 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:39,022 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:39,026 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:39,029 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:39,032 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,033 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,033 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:39,035 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:39,037 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:39,041 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,041 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,041 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,041 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,041 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:39,046 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:39,049 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:39,053 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:39,057 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,058 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,058 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:39,059 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:39,062 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:39,066 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,066 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,066 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,066 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,066 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:39,071 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:39,075 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:39,079 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:39,082 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,083 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,083 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:39,084 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:39,087 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:39,090 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,090 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,091 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,091 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:39,096 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:39,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:39,104 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:39,108 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,108 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,109 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:39,110 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:39,112 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:39,115 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,116 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,116 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,116 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,116 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:39,121 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:39,124 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:39,128 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:39,131 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,132 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,132 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:39,133 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:39,136 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:39,139 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,139 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,140 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,140 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,140 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:39,145 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:39,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:39,152 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:39,176 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,176 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:39,179 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:39,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:39,185 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,185 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,186 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,186 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,186 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:39,191 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:39,195 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:39,199 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:39,203 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,203 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,204 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:39,205 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:39,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:39,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,212 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,213 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,213 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:39,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:39,221 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:39,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:39,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,229 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,229 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:39,231 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:39,234 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:39,238 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,238 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,238 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,238 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:39,243 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:39,247 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:39,251 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:39,254 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,255 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,255 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:39,257 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:39,260 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:39,263 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,264 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,264 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,264 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,264 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:39,269 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:39,273 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:39,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:39,280 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,281 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,281 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:39,284 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:39,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:39,290 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,290 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,290 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,290 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,290 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:39,294 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:39,298 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:39,300 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:39,303 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,304 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,304 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:39,305 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:39,308 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:39,311 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,311 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,311 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,312 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,312 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:39,319 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:39,322 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:39,326 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:39,329 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,330 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,330 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:39,332 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:39,335 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:39,338 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,338 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,338 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,338 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,338 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:39,342 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:39,346 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:39,349 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:39,352 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,352 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,352 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:39,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:39,356 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:39,360 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,360 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,360 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,360 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,360 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:39,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:39,372 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:39,376 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:39,379 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,380 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,380 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:39,382 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:39,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:39,387 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,387 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,388 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,388 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,388 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:39,392 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:39,396 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:39,400 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:39,404 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,405 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,405 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:39,406 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:39,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:39,412 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,412 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,413 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,413 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,413 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:39,417 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:39,421 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:39,425 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:39,428 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,429 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,429 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:39,431 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:39,433 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:39,436 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,436 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,437 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,437 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,437 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:39,441 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:39,445 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:39,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:39,453 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,454 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,454 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:39,455 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:39,458 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:39,461 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,461 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,462 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,462 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:39,469 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:39,472 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:39,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:39,478 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,479 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,479 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:39,480 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:39,483 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:39,486 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,487 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,487 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,487 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,487 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:39,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:39,495 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:39,498 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:39,501 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,502 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,502 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:39,503 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:39,506 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:39,510 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,510 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,510 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,510 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,511 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:39,514 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:39,518 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:39,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:39,524 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,524 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,524 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:39,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:39,529 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:39,529 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,529 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-09 07:03:39,530 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,530 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-09 07:03:39,530 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:39,534 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:39,537 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:39,540 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:39,543 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-09 07:03:39,543 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-09 07:03:39,543 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:39,544 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:39,545 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:39,546 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,546 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:39,546 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,546 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:39,546 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:39,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:39,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:39,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:39,549 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:39,549 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:39,549 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:39,549 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:39,550 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:39,550 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,550 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:39,551 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,551 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:39,551 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:39,595 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:39,634 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:39,673 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:39,712 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:39,714 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:39,714 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:39,737 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:39,738 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:39,739 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:39,739 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:39,740 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:39,740 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:39,741 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:39,741 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:39,742 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:39,743 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:39,744 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:39,745 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:39,745 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:39,745 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:39,746 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:39,750 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,750 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:39,751 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,751 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:39,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:39,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:39,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:39,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:39,752 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:39,752 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:39,752 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:39,753 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:39,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:39,759 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,759 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,759 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,759 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,759 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:39,763 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:39,767 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:39,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:39,772 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,773 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,773 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:39,774 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:39,777 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:39,780 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,780 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,781 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,781 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,781 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:39,785 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:39,789 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:39,792 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:39,795 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,795 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,795 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:39,797 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:39,799 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:39,803 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,803 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,803 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,803 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:39,808 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:39,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:39,813 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:39,816 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,817 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,817 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:39,818 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:39,820 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:39,824 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,824 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,824 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,824 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,825 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:39,829 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:39,832 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:39,835 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:39,838 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,839 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,839 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:39,840 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:39,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:39,846 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,847 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,847 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,847 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,847 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:39,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:39,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:39,857 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:39,860 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,861 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,861 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:39,862 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:39,865 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:39,868 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,868 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,868 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,869 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,869 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:39,873 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:39,878 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:39,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:39,884 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,885 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,885 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:39,887 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:39,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:39,893 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,893 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,894 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,894 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,894 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:39,899 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:39,902 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:39,906 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:39,910 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,910 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,910 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:39,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:39,915 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:39,918 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,918 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,918 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:39,923 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:39,926 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:39,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:39,932 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,933 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,933 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:39,934 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:39,937 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:39,940 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,940 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,941 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,941 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,941 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:39,945 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:39,950 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:39,953 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:39,957 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,958 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,958 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:39,959 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:39,962 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:39,965 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,965 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,966 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,966 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,966 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:39,972 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:39,975 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:39,978 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:39,982 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:39,982 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:39,982 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:39,984 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:39,987 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:39,990 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:39,991 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:39,991 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:39,991 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:39,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:39,996 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:39,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:40,003 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:40,006 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,006 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,007 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:40,008 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:40,011 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:40,014 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,015 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,015 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,015 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,015 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:40,020 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:40,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:40,026 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:40,029 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,030 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,030 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:40,031 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:40,034 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:40,037 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,037 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,037 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,038 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:40,042 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:40,046 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:40,049 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:40,052 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,053 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,053 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:40,055 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:40,057 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:40,060 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,061 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,061 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,061 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:40,065 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:40,069 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:40,071 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:40,075 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,076 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,076 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:40,077 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:40,080 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:40,083 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,083 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,084 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,084 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:40,088 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:40,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:40,095 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:40,099 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,099 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,099 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:40,102 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:40,105 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:40,108 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,108 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,109 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,109 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,109 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:40,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:40,117 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:40,121 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:40,124 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,124 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,125 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:40,126 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:40,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:40,133 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,133 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,134 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,134 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,134 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:40,139 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:40,144 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:40,148 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:40,153 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,154 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,154 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:40,156 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:40,159 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:40,162 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,162 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,162 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,162 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:40,167 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:40,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:40,175 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:40,179 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,179 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,180 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:40,181 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:40,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:40,187 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,188 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,188 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,188 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:40,195 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:40,200 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:40,204 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:40,208 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,209 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,209 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:40,212 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:40,215 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:40,218 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,218 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,219 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,219 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,219 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:40,224 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:40,228 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:40,233 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:40,236 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,237 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,237 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:40,239 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:40,241 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:40,245 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,245 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,245 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,245 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,245 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:40,251 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:40,255 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:40,259 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:40,263 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,264 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,264 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:40,266 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:40,269 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:40,272 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,272 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,272 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,272 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,272 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:40,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:40,281 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:40,285 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:40,289 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,290 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,290 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:40,291 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:40,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:40,297 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,298 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,298 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,298 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,298 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:40,303 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:40,308 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:40,312 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:40,316 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,317 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,317 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:40,318 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:40,321 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:40,322 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,322 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-09 07:03:40,322 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,322 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-09 07:03:40,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:40,328 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:40,332 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:40,337 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:40,340 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-09 07:03:40,341 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-09 07:03:40,341 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:40,342 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:40,343 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:40,344 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,344 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:40,344 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,344 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:40,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:40,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:40,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:40,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:40,345 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:40,345 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:40,345 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:40,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:40,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:40,347 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,347 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:40,347 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,347 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:40,347 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:40,410 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:40,461 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:40,503 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:40,548 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:40,550 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:40,550 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:40,591 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:40,592 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:40,593 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:40,593 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:40,593 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:40,594 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:40,594 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:40,594 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:40,594 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:40,595 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:40,595 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:40,595 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:40,595 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:40,596 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:40,597 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:40,602 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,603 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:40,603 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,603 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:40,603 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:40,604 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:40,604 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:40,605 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:40,605 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:40,605 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:40,605 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:40,606 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:40,609 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:40,612 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,612 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,612 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,613 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:40,617 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:40,619 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:40,622 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:40,625 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,625 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,626 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:40,627 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:40,630 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:40,633 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,633 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,633 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,634 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:40,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:40,642 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:40,645 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:40,648 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,648 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,648 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:40,650 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:40,652 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:40,656 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,656 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,656 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,656 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:40,660 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:40,664 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:40,667 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:40,671 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,671 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,671 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:40,673 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:40,676 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:40,679 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,679 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,679 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,680 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,680 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:40,684 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:40,687 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:40,689 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:40,693 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,693 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,693 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:40,695 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:40,697 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:40,701 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,701 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,701 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,701 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:40,706 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:40,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:40,712 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:40,715 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,715 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,715 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:40,717 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:40,720 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:40,723 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,723 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,724 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,724 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,724 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:40,728 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:40,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:40,734 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:40,738 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,738 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,738 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:40,740 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:40,742 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:40,746 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,746 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,746 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,746 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,747 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:40,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:40,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:40,757 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:40,761 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,762 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,762 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:40,763 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:40,766 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:40,769 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,769 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,769 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,769 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,770 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:40,773 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:40,777 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:40,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:40,782 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,783 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,783 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:40,784 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:40,787 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:40,790 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,791 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,791 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,791 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,791 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:40,795 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:40,798 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:40,801 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:40,804 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,804 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,805 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:40,806 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:40,809 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:40,812 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,812 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,813 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,813 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,813 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:40,817 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:40,820 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:40,824 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:40,827 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,827 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,828 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:40,829 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:40,832 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:40,835 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,836 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,836 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,836 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,836 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:40,840 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:40,843 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:40,846 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:40,849 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,849 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,849 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:40,851 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:40,854 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:40,857 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,857 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,857 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,857 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:40,862 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:40,865 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:40,867 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:40,870 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,871 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,871 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:40,872 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:40,875 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:40,878 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,879 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,879 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,879 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,879 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:40,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:40,886 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:40,888 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:40,891 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,892 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,892 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:40,894 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:40,896 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:40,900 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,900 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,900 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,900 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,900 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:40,904 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:40,908 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:40,910 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:40,913 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,914 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,914 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:40,915 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:40,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:40,921 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,922 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,922 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,922 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,922 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:40,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:40,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:40,933 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:40,935 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,936 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,936 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:40,938 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:40,941 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:40,944 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,944 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,944 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,944 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,944 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:40,948 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:40,951 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:40,954 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:40,958 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,959 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,960 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:40,962 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:40,966 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:40,972 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:40,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:40,972 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:40,973 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:40,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:40,982 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:40,988 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:40,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:40,998 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:40,999 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:40,999 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:41,001 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:41,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:41,007 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,007 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,007 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,007 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,008 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:41,014 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:41,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:41,020 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:41,023 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,024 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,024 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:41,026 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:41,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:41,032 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,032 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,032 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,032 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,032 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:41,036 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:41,040 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:41,045 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:41,074 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,075 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,075 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:41,077 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:41,080 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:41,083 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,084 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,084 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,084 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,084 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:41,088 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:41,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:41,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:41,096 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,097 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,097 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:41,098 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:41,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:41,104 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,105 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,105 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,105 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:41,109 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:41,113 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:41,116 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:41,119 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,119 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,119 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:41,121 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:41,124 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:41,127 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,127 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,127 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,128 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,128 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:41,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:41,135 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:41,138 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:41,140 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,141 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,141 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:41,142 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:41,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:41,148 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,149 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,149 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,149 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:41,154 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:41,157 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:41,160 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:41,163 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,163 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,163 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:41,165 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:41,168 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:41,168 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,169 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-09 07:03:41,169 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,169 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-09 07:03:41,169 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:41,173 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:41,176 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:41,179 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:41,181 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-09 07:03:41,182 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-09 07:03:41,182 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:41,183 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:41,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:41,185 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,185 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:41,185 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,185 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:41,185 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:41,185 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:41,186 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:41,186 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:41,186 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:41,186 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:41,186 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:41,187 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:41,187 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:41,188 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,188 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:41,188 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:41,188 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:41,231 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:41,267 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:41,306 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:41,347 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:41,349 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:41,349 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:41,374 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:41,375 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:41,376 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:41,376 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:41,376 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:41,377 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:41,377 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:41,377 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:41,377 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:41,378 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:41,378 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:41,378 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:41,378 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:41,378 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:41,379 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:41,383 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,383 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:41,383 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,384 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:41,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:41,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:41,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:41,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:41,385 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:41,385 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:41,385 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:41,386 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:41,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:41,392 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,392 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,392 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,393 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:41,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:41,400 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:41,403 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:41,406 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,407 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,407 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:41,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:41,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:41,415 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,416 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,416 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,416 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:41,420 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:41,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:41,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:41,430 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,431 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,431 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:41,432 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:41,435 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:41,439 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,439 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,439 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,439 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,440 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:41,444 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:41,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:41,451 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:41,454 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,455 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,455 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:41,456 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:41,459 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:41,463 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,463 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,463 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,463 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,463 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:41,467 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:41,471 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:41,474 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:41,477 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,477 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,478 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:41,479 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:41,482 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:41,485 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,485 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,486 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,486 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,486 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:41,493 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:41,496 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:41,499 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:41,502 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,503 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,503 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:41,504 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:41,507 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:41,510 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,511 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,511 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,511 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,511 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:41,516 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:41,520 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:41,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:41,530 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,531 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,531 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:41,533 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:41,535 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:41,539 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,539 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,539 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,540 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,540 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:41,547 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:41,551 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:41,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:41,560 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,561 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,561 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:41,563 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:41,566 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:41,569 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,570 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,570 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,570 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,570 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:41,576 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:41,580 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:41,584 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:41,588 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,589 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,590 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:41,591 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:41,594 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:41,598 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,598 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,598 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,599 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:41,604 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:41,609 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:41,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:41,618 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,618 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,619 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:41,621 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:41,623 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:41,627 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,627 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,628 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,628 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,628 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:41,633 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:41,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:41,642 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:41,646 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,646 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,647 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:41,648 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:41,651 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:41,655 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,655 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,655 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,656 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:41,661 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:41,665 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:41,668 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:41,671 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,672 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,672 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:41,674 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:41,677 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:41,680 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,680 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,681 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,681 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,681 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:41,685 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:41,688 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:41,691 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:41,694 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,694 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,694 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:41,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:41,698 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:41,702 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,702 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,702 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,703 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,703 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:41,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:41,711 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:41,714 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:41,717 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,718 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,718 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:41,719 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:41,722 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:41,725 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,726 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,726 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,726 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,726 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:41,742 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:41,746 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:41,749 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:41,751 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,752 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,753 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:41,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:41,758 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:41,762 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,762 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,762 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,763 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,763 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:41,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:41,773 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:41,777 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:41,781 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,782 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,782 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:41,784 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:41,787 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:41,790 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,791 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,791 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,791 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,791 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:41,797 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:41,800 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:41,804 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:41,808 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,809 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,809 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:41,810 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:41,813 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:41,817 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,817 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,818 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,818 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:41,823 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:41,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:41,830 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:41,834 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,835 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,835 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:41,837 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:41,841 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:41,844 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,844 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,845 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,845 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,845 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:41,850 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:41,854 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:41,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:41,861 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,862 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,862 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:41,863 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:41,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:41,870 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,871 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,871 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,871 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,872 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:41,876 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:41,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:41,884 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:41,887 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,888 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,889 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:41,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:41,893 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:41,896 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,897 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,897 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,897 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,898 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:41,902 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:41,905 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:41,909 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:41,912 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,913 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,913 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:41,914 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:41,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:41,921 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,922 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,922 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,922 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,922 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:41,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:41,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:41,934 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:41,937 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,938 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,938 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:41,940 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:41,943 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:41,947 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,947 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,947 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,948 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,948 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:41,952 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:41,955 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:41,959 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:41,962 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,963 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,964 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:41,965 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:41,969 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:41,972 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,973 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,973 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,973 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:41,978 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:41,981 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:41,984 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:41,988 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:41,989 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:41,989 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:41,992 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:41,997 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:41,998 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:41,998 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-09 07:03:41,999 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:41,999 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-09 07:03:41,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:42,005 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:42,008 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:42,012 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:42,015 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-09 07:03:42,016 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-09 07:03:42,017 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:42,018 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:42,019 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:42,019 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,020 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,020 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,020 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,020 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:42,020 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:42,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:42,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:42,022 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:42,022 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:42,022 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:42,023 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:42,023 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:42,024 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,024 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,024 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,024 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,024 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:42,069 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:42,110 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:42,150 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:42,189 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:42,191 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:42,191 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:42,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:42,215 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:42,216 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:42,216 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,217 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:42,217 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,217 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:42,217 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:42,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:42,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:42,218 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:42,219 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:42,219 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:42,219 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:42,220 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:42,224 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,224 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,224 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,224 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,224 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:42,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:42,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:42,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:42,226 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:42,226 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:42,226 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:42,227 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:42,230 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:42,233 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,234 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,234 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,234 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,234 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:42,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:42,242 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:42,245 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:42,248 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,249 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,249 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:42,250 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:42,254 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:42,259 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,259 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,260 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,260 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,260 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:42,266 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:42,270 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:42,275 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:42,279 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,280 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,280 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:42,282 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:42,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:42,292 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,292 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,292 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,293 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,293 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:42,298 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:42,302 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:42,305 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:42,309 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,310 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,310 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:42,312 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:42,317 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:42,322 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,322 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,322 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,322 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:42,327 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:42,330 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:42,333 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:42,336 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,337 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,337 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:42,339 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:42,342 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:42,348 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,348 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,348 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,349 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,349 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:42,355 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:42,359 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:42,364 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:42,369 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,370 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,370 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:42,372 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:42,377 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:42,382 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,383 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,383 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,383 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,383 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:42,388 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:42,391 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:42,394 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:42,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,398 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:42,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:42,403 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:42,407 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,407 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,407 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,407 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,407 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:42,411 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:42,415 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:42,418 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:42,421 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,422 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,422 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:42,423 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:42,426 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:42,429 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,429 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,429 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,430 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,430 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:42,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:42,438 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:42,442 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:42,445 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,446 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,446 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:42,447 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:42,450 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:42,454 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,454 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,454 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,454 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:42,459 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:42,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:42,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:42,470 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,470 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,470 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:42,472 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:42,475 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:42,478 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,478 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,478 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,479 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,479 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:42,484 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:42,487 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:42,491 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:42,495 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,496 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,496 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:42,497 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:42,500 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:42,504 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,504 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,504 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,504 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,504 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:42,509 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:42,513 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:42,517 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:42,520 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,521 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,521 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:42,523 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:42,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:42,529 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,529 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,530 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,530 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,530 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:42,535 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:42,539 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:42,544 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:42,547 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,548 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,548 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:42,549 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:42,552 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:42,555 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,555 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,556 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,556 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:42,562 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:42,565 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:42,569 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:42,573 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,573 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,574 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:42,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:42,579 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:42,582 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,582 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,583 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,583 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,583 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:42,588 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:42,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:42,598 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:42,603 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,604 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,604 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:42,606 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:42,609 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:42,612 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,613 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,613 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,613 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:42,618 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:42,622 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:42,626 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:42,630 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,631 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,631 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:42,633 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:42,636 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:42,639 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,640 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,640 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,640 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,640 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:42,645 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:42,649 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:42,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:42,658 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,658 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,659 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:42,660 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:42,663 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:42,667 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,668 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,668 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,668 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,668 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:42,673 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:42,678 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:42,682 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:42,686 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,687 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,687 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:42,689 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:42,692 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:42,696 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,696 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,696 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,697 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,697 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:42,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:42,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:42,712 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:42,716 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,717 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,717 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:42,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:42,721 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:42,725 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,725 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,726 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,726 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,726 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:42,732 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:42,736 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:42,740 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:42,744 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,745 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,745 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:42,747 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:42,750 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:42,753 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,754 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,754 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,754 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:42,760 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:42,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:42,768 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:42,772 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,773 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,773 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:42,775 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:42,779 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:42,785 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,785 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,786 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,786 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,786 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:42,792 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:42,796 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:42,800 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:42,804 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,805 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,805 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:42,808 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:42,811 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:42,814 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,814 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,815 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,815 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,815 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:42,819 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:42,823 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:42,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:42,830 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,831 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,832 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:42,833 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:42,837 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:42,841 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,841 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,842 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,842 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,842 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:42,852 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:42,856 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:42,860 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:42,863 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,864 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,864 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:42,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:42,869 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:42,869 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,869 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-09 07:03:42,870 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,870 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-09 07:03:42,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:42,875 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:42,878 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:42,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:42,884 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-09 07:03:42,885 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-09 07:03:42,885 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:42,886 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:42,887 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:42,887 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,887 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,888 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,888 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,888 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:42,888 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:42,888 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:42,888 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:42,889 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:42,889 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:42,889 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:42,889 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:42,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:42,890 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:42,890 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:42,891 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:42,891 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:42,891 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:42,947 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:42,990 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:43,030 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:43,071 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:43,072 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:43,073 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:43,095 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:43,096 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:43,097 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:43,098 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,098 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:43,098 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,099 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:43,099 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:43,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:43,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:43,101 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:43,101 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:43,101 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:43,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:43,102 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:43,106 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,106 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,106 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,106 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,106 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:43,106 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:43,107 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:43,107 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:43,107 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:43,107 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:43,107 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:43,108 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:43,111 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:43,114 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,114 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,114 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,114 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:43,119 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:43,121 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:43,124 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:43,127 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,127 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,127 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:43,128 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:43,131 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:43,134 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,134 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,135 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,135 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,135 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:43,139 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:43,141 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:43,144 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:43,146 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,147 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,147 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:43,148 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:43,151 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:43,154 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,154 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,154 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,155 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,155 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:43,158 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:43,162 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:43,164 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:43,167 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,168 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,168 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:43,169 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:43,172 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:43,175 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,175 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,176 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,176 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,176 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:43,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:43,183 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:43,185 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:43,188 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,189 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,189 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:43,190 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:43,193 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:43,196 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,196 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,196 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,197 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,197 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:43,200 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:43,204 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:43,206 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:43,208 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,209 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,209 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:43,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:43,213 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:43,217 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,217 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,217 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,218 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:43,222 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:43,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:43,228 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:43,231 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,231 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,232 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:43,233 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:43,236 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:43,239 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,239 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,239 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,240 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,240 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:43,244 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:43,247 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:43,249 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:43,252 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,253 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,253 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:43,254 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:43,257 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:43,260 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,260 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,260 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,260 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,261 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:43,264 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:43,267 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:43,270 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:43,273 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,273 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,274 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:43,275 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:43,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:43,281 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,281 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,281 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,282 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,282 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:43,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:43,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:43,291 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:43,294 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,295 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,295 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:43,296 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:43,299 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:43,302 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,302 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,302 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,303 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,303 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:43,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:43,309 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:43,312 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:43,315 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,315 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,316 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:43,317 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:43,320 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:43,323 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,324 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,324 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,324 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,324 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:43,331 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:43,334 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:43,337 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:43,339 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,340 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,340 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:43,342 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:43,344 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:43,347 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,347 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,348 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,348 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,348 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:43,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:43,355 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:43,357 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:43,360 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,360 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,361 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:43,362 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:43,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:43,368 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,368 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,368 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,368 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:43,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:43,376 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:43,378 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:43,381 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,382 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,382 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:43,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:43,386 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:43,389 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,389 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,390 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,390 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,390 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:43,394 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:43,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:43,399 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:43,402 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,403 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,403 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:43,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:43,407 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:43,410 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,410 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,410 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,410 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,410 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:43,414 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:43,417 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:43,419 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:43,422 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,422 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,422 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:43,424 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:43,426 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:43,429 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,430 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,430 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,430 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,430 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:43,434 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:43,437 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:43,439 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:43,442 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,442 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,442 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:43,443 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:43,446 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:43,449 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,450 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,450 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,450 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,450 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:43,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:43,458 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:43,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:43,464 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,465 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,465 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:43,467 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:43,470 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:43,473 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,473 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,473 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,473 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,474 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:43,478 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:43,481 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:43,484 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:43,487 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,488 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,489 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:43,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:43,492 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:43,496 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,496 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,496 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,497 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:43,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:43,504 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:43,506 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:43,509 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,510 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,510 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:43,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:43,515 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:43,517 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,518 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,518 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,518 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,518 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:43,522 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:43,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:43,528 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:43,530 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,531 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,531 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:43,532 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:43,535 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:43,539 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,539 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,539 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,539 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,540 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:43,543 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:43,547 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:43,550 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:43,552 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,553 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,553 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:43,555 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:43,558 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:43,561 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,561 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,561 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,561 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,562 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:43,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:43,569 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:43,572 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:43,575 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,576 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,576 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:43,577 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:43,580 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:43,584 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,584 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,584 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,584 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,585 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:43,589 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:43,593 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:43,596 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:43,599 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,600 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,600 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:43,602 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:43,604 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:43,605 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,605 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-09 07:03:43,605 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,606 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-09 07:03:43,606 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:43,610 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:43,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:43,616 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:43,620 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-09 07:03:43,620 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-09 07:03:43,620 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:43,622 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:43,622 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:43,623 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,623 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,623 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,623 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,623 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:43,623 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:43,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:43,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:43,624 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:43,624 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:43,625 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:43,625 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:43,626 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:43,626 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,626 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,626 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,626 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,626 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:43,674 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:43,714 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:43,755 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:43,796 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:43,798 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:43,798 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:43,821 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:43,822 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:43,823 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:43,824 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,824 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:43,824 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,825 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:43,825 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:43,826 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:43,826 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:43,826 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:43,826 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:43,826 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:43,827 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:43,827 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:43,831 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,831 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:43,831 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,831 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:43,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:43,832 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:43,832 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:43,832 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:43,832 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:43,833 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:43,833 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:43,834 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:43,836 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:43,840 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,840 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,840 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,840 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,840 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:43,844 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:43,848 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:43,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:43,854 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,854 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,855 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:43,856 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:43,859 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:43,862 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,862 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,862 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,863 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,863 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:43,867 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:43,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:43,873 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:43,876 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,877 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,877 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:43,878 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:43,881 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:43,884 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,884 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,884 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,885 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:43,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:43,896 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:43,899 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:43,903 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,903 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,903 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:43,904 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:43,907 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:43,910 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,911 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,911 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,911 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:43,915 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:43,919 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:43,923 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:43,926 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,927 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,927 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:43,928 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:43,932 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:43,936 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,936 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,936 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,937 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,937 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:43,942 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:43,946 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:43,950 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:43,953 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,954 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,954 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:43,955 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:43,958 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:43,962 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,962 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,962 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,963 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,963 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:43,967 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:43,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:43,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:43,976 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:43,976 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:43,976 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:43,978 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:43,980 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:43,984 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:43,984 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:43,984 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:43,984 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:43,985 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:43,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:43,992 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:43,995 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:43,999 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,000 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,000 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:44,001 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:44,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:44,007 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,007 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,007 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,007 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,008 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:44,012 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:44,016 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:44,020 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:44,023 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,024 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,024 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:44,025 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:44,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:44,031 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,031 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,031 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,032 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,032 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:44,036 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:44,039 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:44,042 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:44,045 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,046 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,046 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:44,047 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:44,050 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:44,053 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,053 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,053 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,054 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,054 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:44,058 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:44,062 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:44,064 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:44,067 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,068 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,068 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:44,069 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:44,072 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:44,075 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,076 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,076 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,076 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,076 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:44,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:44,084 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:44,087 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:44,090 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,090 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,091 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:44,093 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:44,096 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:44,099 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,099 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,099 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,100 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:44,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:44,108 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:44,112 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:44,116 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,116 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,117 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:44,118 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:44,121 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:44,124 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,125 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,125 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,125 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:44,129 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:44,133 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:44,152 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:44,175 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,176 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:44,178 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:44,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:44,185 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,186 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,186 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,187 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,187 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:44,192 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:44,195 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:44,198 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:44,201 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,202 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,202 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:44,204 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:44,207 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:44,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,213 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,213 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,213 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,214 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:44,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:44,221 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:44,224 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:44,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,228 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,229 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:44,232 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:44,235 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:44,238 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,238 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,238 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,238 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:44,243 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:44,258 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:44,262 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:44,266 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,266 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,267 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:44,268 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:44,271 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:44,274 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,275 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,275 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,275 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:44,280 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:44,283 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:44,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:44,290 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,290 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,290 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:44,292 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:44,295 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:44,298 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,298 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,299 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,299 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,299 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:44,303 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:44,307 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:44,310 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:44,313 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,314 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,314 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:44,315 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:44,318 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:44,321 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,322 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,322 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,322 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,322 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:44,327 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:44,330 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:44,333 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:44,336 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,339 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,339 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:44,341 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:44,343 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:44,346 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,347 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,347 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,347 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,347 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:44,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:44,354 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:44,357 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:44,360 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,361 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,361 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:44,362 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:44,365 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:44,368 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,369 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,369 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:44,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:44,377 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:44,380 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:44,383 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,383 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,384 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:44,386 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:44,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:44,392 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,392 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,392 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,393 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:44,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:44,400 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:44,404 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:44,407 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,408 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,408 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:44,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:44,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:44,415 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,416 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,416 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,416 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:44,420 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:44,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:44,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:44,430 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,431 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,431 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:44,433 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:44,436 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:44,436 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,436 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-09 07:03:44,437 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,437 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-09 07:03:44,437 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:44,442 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:44,446 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:44,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:44,451 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-09 07:03:44,452 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-09 07:03:44,452 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:44,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:44,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:44,455 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,455 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:44,455 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,456 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:44,456 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:44,456 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:44,456 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:44,457 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:44,457 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:44,457 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:44,458 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:44,458 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:44,459 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:44,459 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,459 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:44,459 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,460 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:44,460 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:44,502 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:44,541 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:44,580 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:44,620 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:44,622 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:44,623 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:44,645 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:44,646 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:44,646 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:44,647 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:44,647 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:44,647 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:44,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:44,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:44,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:44,649 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:44,649 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:44,649 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:44,649 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:44,650 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:44,651 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:44,654 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,654 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:44,655 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,655 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:44,655 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:44,655 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:44,655 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:44,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:44,656 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:44,656 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:44,656 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:44,657 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:44,659 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:44,663 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,663 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,663 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,663 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,663 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:44,667 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:44,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:44,674 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:44,676 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,682 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,682 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:44,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:44,687 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:44,693 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,694 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,694 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,694 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:44,700 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:44,703 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:44,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:44,711 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,712 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,712 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:44,714 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:44,717 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:44,720 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,720 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,721 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,721 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,721 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:44,725 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:44,729 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:44,733 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:44,736 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,736 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,736 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:44,738 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:44,741 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:44,744 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,745 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,745 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,745 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,745 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:44,749 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:44,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:44,757 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:44,760 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,761 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,761 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:44,762 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:44,765 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:44,769 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,769 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,769 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,769 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:44,773 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:44,778 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:44,782 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:44,787 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,787 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,787 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:44,789 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:44,792 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:44,796 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,796 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,796 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,796 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,796 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:44,801 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:44,805 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:44,809 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:44,812 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,812 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,813 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:44,814 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:44,817 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:44,820 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,820 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,821 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,821 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,821 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:44,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:44,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:44,835 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:44,838 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,838 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,838 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:44,840 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:44,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:44,846 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,846 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,847 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,847 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,847 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:44,852 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:44,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:44,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:44,861 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,862 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,862 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:44,863 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:44,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:44,870 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,870 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,870 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,870 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,871 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:44,875 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:44,879 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:44,884 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:44,887 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,888 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,888 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:44,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:44,892 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:44,895 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,896 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,896 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,896 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,896 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:44,905 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:44,908 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:44,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:44,914 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,915 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,915 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:44,916 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:44,919 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:44,922 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,923 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,923 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,923 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,923 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:44,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:44,932 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:44,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:44,939 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,940 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,940 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:44,942 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:44,945 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:44,948 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,948 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,949 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,949 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,949 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:44,955 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:44,961 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:44,966 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:44,971 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:44,972 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:44,973 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:44,975 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:44,979 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:44,984 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:44,985 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:44,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:44,985 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:44,986 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:44,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:44,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:45,005 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:45,010 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,011 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,011 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:45,014 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:45,017 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:45,020 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,020 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,021 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,021 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:45,027 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:45,032 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:45,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:45,043 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,045 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,045 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:45,047 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:45,050 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:45,054 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,054 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,054 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,055 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,055 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:45,060 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:45,065 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:45,071 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:45,076 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,078 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,078 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:45,080 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:45,083 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:45,086 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,087 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,087 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,087 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,087 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:45,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:45,098 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:45,103 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:45,108 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,109 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,109 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:45,111 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:45,114 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:45,118 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,118 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,119 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,119 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,119 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:45,125 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:45,130 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:45,135 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:45,140 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,141 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,142 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:45,144 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:45,147 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:45,150 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,151 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,151 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,151 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:45,157 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:45,162 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:45,167 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:45,172 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,173 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,173 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:45,175 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:45,178 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:45,182 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,183 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,183 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,183 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,183 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:45,190 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:45,195 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:45,200 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:45,205 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,206 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,206 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:45,209 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:45,212 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:45,215 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,215 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,216 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,216 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,216 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:45,222 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:45,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:45,229 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:45,232 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,233 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,233 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:45,234 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:45,237 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:45,241 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,241 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,241 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,242 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,242 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:45,246 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:45,249 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:45,252 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:45,255 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,256 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,256 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:45,258 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:45,261 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:45,264 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,264 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,265 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,265 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,265 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:45,269 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:45,273 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:45,276 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:45,279 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,280 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,280 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:45,281 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:45,284 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:45,288 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,288 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,289 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,289 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:45,293 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:45,297 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:45,301 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:45,304 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,304 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,305 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:45,307 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:45,310 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:45,310 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,311 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-09 07:03:45,311 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,311 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-09 07:03:45,311 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:45,316 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:45,319 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:45,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:45,326 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-09 07:03:45,327 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-09 07:03:45,327 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:45,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:45,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:45,330 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,330 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:45,331 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,331 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:45,331 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:45,331 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:45,332 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:45,332 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:45,332 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:45,332 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:45,333 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:45,333 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:45,334 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:45,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,334 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:45,335 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,335 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:45,335 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:45,380 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:45,418 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:45,457 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:45,496 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:45,497 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:45,498 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:45,522 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:45,523 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:45,523 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:45,524 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:45,524 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:45,524 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:45,524 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:45,524 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:45,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:45,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:45,525 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:45,525 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:45,525 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:45,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:45,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:45,530 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,530 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:45,530 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,531 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:45,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:45,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:45,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:45,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:45,532 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:45,532 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:45,532 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:45,533 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:45,535 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:45,539 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,539 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,539 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,539 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,540 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:45,544 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:45,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:45,552 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:45,556 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,557 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,557 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:45,559 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:45,561 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:45,565 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,565 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,565 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,565 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,565 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:45,570 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:45,574 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:45,578 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:45,582 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,583 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,583 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:45,584 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:45,587 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:45,590 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,591 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,591 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,591 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,591 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:45,596 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:45,600 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:45,604 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:45,608 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,609 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,610 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:45,612 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:45,615 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:45,619 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,619 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,619 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,619 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,619 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:45,624 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:45,627 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:45,631 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:45,634 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,635 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,635 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:45,636 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:45,639 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:45,643 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,643 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,643 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,643 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,643 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:45,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:45,651 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:45,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:45,657 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,658 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,658 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:45,659 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:45,662 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:45,665 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,666 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,666 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,666 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:45,672 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:45,675 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:45,679 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:45,682 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,683 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,683 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:45,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:45,687 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:45,691 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,691 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,691 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,691 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:45,697 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:45,701 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:45,706 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:45,710 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,710 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,710 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:45,712 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:45,715 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:45,718 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,718 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,718 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,719 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:45,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:45,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:45,735 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:45,740 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,740 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,741 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:45,742 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:45,745 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:45,748 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,749 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,749 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,749 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,749 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:45,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:45,758 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:45,762 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:45,766 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,767 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,767 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:45,769 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:45,772 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:45,775 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,776 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,776 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,776 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:45,781 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:45,786 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:45,790 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:45,794 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,795 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,795 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:45,796 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:45,799 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:45,803 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,803 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,803 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,804 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:45,809 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:45,814 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:45,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:45,821 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,822 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,822 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:45,824 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:45,827 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:45,830 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,830 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,831 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:45,835 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:45,840 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:45,844 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:45,847 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,848 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,848 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:45,850 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:45,852 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:45,856 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,856 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,856 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,857 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,857 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:45,862 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:45,867 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:45,872 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:45,877 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,878 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,879 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:45,881 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:45,886 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:45,891 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,891 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,891 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,892 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,892 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:45,896 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:45,900 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:45,904 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:45,907 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,907 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,907 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:45,909 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:45,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:45,918 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,919 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,919 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,919 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:45,925 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:45,929 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:45,934 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:45,939 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,940 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,940 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:45,943 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:45,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:45,952 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,952 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,952 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,953 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:45,958 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:45,962 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:45,966 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:45,971 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:45,972 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:45,972 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:45,973 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:45,976 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:45,981 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:45,981 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:45,981 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:45,981 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:45,981 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:45,986 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:45,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:45,996 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:46,000 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,001 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,002 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:46,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:46,008 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:46,011 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,011 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,012 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,012 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,012 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:46,054 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:46,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:46,064 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:46,069 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,070 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,070 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:46,072 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:46,075 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:46,079 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,079 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,080 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,080 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:46,085 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:46,090 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:46,094 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:46,098 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,099 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,099 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:46,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:46,105 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:46,108 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,109 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,109 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,110 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,110 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:46,115 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:46,120 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:46,124 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:46,127 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,128 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,128 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:46,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:46,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:46,138 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,138 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,138 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,139 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,139 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:46,144 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:46,148 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:46,152 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:46,156 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,156 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,157 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:46,159 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:46,162 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:46,165 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,166 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,166 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,166 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,166 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:46,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:46,175 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:46,179 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:46,183 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,184 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,184 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:46,186 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:46,189 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:46,192 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,193 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,193 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,193 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,193 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:46,199 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:46,203 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:46,207 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:46,211 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,212 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,212 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:46,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:46,217 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:46,218 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,218 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-09 07:03:46,218 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,218 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-09 07:03:46,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:46,224 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:46,228 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:46,231 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:46,234 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-09 07:03:46,235 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-09 07:03:46,235 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:46,236 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:46,237 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:46,238 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,238 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:46,238 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,238 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:46,238 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:46,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:46,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:46,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:46,239 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:46,240 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:46,240 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:46,240 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:46,241 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:46,241 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,241 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:46,242 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,242 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:46,242 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:46,295 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:46,339 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:46,380 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:46,422 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:46,427 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:46,428 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:46,452 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:46,453 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:46,454 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:46,454 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:46,454 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:46,454 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:46,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:46,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:46,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:46,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:46,455 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:46,455 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:46,456 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:46,456 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:46,456 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:46,460 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,461 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:46,461 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,461 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:46,461 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:46,461 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:46,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:46,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:46,462 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:46,462 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:46,462 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:46,463 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:46,466 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:46,469 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,470 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,470 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,470 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,470 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:46,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:46,478 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:46,482 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:46,485 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,485 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,485 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:46,487 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:46,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:46,493 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,494 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,494 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,494 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,494 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:46,499 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:46,503 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:46,507 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:46,510 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,510 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,510 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:46,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:46,515 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:46,518 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,518 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,519 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,519 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,519 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:46,523 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:46,527 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:46,530 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:46,534 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,534 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,534 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:46,536 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:46,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:46,542 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,542 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,543 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,543 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,543 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:46,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:46,552 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:46,555 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:46,559 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,559 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,559 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:46,561 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:46,564 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:46,567 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,568 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,568 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,568 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,568 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:46,573 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:46,578 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:46,583 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:46,587 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,588 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,588 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:46,590 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:46,593 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:46,597 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,597 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,597 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,597 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,598 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:46,602 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:46,606 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:46,609 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:46,612 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,613 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,613 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:46,614 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:46,617 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:46,621 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,621 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,621 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,622 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,622 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:46,626 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:46,630 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:46,633 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:46,636 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,637 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,637 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:46,638 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:46,641 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:46,644 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,645 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,645 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,645 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,645 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:46,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:46,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:46,657 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:46,660 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,661 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,661 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:46,662 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:46,665 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:46,669 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,669 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,669 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,669 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,669 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:46,674 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:46,677 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:46,680 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:46,684 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,685 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,685 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:46,687 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:46,690 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:46,693 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,693 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,693 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,693 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,694 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:46,698 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:46,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:46,706 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:46,710 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,710 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,711 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:46,712 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:46,715 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:46,718 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,719 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,719 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,719 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:46,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:46,727 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:46,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:46,734 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,735 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,735 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:46,737 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:46,740 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:46,743 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,743 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,743 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,744 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:46,748 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:46,752 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:46,756 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:46,760 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,760 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,760 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:46,762 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:46,765 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:46,768 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,769 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,769 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,769 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,769 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:46,774 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:46,778 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:46,782 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:46,787 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,788 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,788 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:46,790 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:46,793 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:46,796 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,796 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,797 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,797 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,797 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:46,804 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:46,808 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:46,812 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:46,816 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,816 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,817 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:46,818 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:46,821 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:46,824 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,825 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,825 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,825 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,825 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:46,829 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:46,833 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:46,837 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:46,841 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,841 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,842 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:46,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:46,846 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:46,850 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,850 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,850 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,850 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:46,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:46,859 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:46,862 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:46,865 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,866 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,866 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:46,867 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:46,870 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:46,874 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,874 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,874 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,875 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,875 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:46,879 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:46,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:46,886 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:46,889 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,890 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,890 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:46,892 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:46,895 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:46,899 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,899 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,899 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,899 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,899 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:46,905 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:46,909 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:46,913 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:46,916 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,917 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,917 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:46,919 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:46,922 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:46,926 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,926 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,927 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,927 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:46,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:46,935 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:46,939 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:46,943 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,943 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,943 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:46,945 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:46,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:46,951 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,952 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,952 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,952 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:46,957 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:46,961 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:46,965 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:46,969 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,969 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,970 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:46,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:46,974 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:46,977 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:46,978 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:46,978 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:46,978 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:46,978 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:46,983 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:46,987 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:46,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:46,994 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:46,994 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:46,995 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:46,996 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:46,999 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:47,003 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,003 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:47,003 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,003 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:47,004 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:47,008 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:47,012 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:47,016 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:47,021 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:47,022 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:47,022 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:47,024 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:47,026 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:47,030 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,030 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:47,031 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,031 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:47,031 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:47,036 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:47,040 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:47,045 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:47,048 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:47,048 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:47,048 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:47,050 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:47,053 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:47,054 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,054 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-09 07:03:47,054 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,054 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-09 07:03:47,055 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:47,060 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:47,063 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:47,067 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:47,070 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-09 07:03:47,071 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-09 07:03:47,071 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:47,072 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:47,073 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:47,073 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,074 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,074 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,074 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,074 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:47,074 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:47,075 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:47,075 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:47,075 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:47,075 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:47,076 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:47,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:47,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:47,077 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,077 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,077 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,077 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,078 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:47,124 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:47,165 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:47,205 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:47,245 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:47,248 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:47,248 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:47,274 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:47,275 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:47,275 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:47,276 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,276 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:47,276 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,276 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:47,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:47,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:47,277 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:47,278 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:47,278 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:47,278 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:47,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:47,279 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:47,284 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,284 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,284 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,284 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,284 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:47,284 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:47,285 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:47,285 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:47,285 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:47,285 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:47,286 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:47,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:47,289 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:47,293 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,293 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,293 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,293 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,294 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:47,299 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:47,302 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:47,305 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:47,309 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,310 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,310 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:47,312 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:47,314 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:47,318 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,318 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,318 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,318 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,318 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:47,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:47,327 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:47,330 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:47,333 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,334 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,334 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:47,336 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:47,338 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:47,342 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,343 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,343 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,343 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:47,347 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:47,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:47,354 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:47,357 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,358 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,358 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:47,359 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:47,362 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:47,366 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,366 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,366 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,367 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,367 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:47,372 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:47,376 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:47,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:47,383 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,383 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,384 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:47,385 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:47,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:47,391 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,391 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,392 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,392 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,392 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:47,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:47,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:47,405 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:47,410 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,410 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,410 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:47,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:47,415 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:47,418 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,419 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,419 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,419 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,419 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:47,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:47,428 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:47,432 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:47,436 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,436 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,437 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:47,438 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:47,441 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:47,444 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,444 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,444 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,445 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,445 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:47,449 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:47,453 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:47,457 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:47,460 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,460 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,460 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:47,462 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:47,464 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:47,467 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,467 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,468 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,468 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,468 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:47,472 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:47,476 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:47,479 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:47,482 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,482 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,483 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:47,484 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:47,488 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:47,492 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,492 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,492 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,492 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:47,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:47,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:47,504 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:47,507 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,508 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,508 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:47,510 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:47,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:47,516 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,516 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,516 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,516 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,516 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:47,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:47,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:47,530 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:47,534 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,537 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,537 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:47,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:47,543 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:47,548 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,548 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,549 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,549 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,549 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:47,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:47,561 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:47,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:47,570 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,571 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,571 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:47,573 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:47,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:47,579 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,579 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,579 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,579 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,579 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:47,583 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:47,587 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:47,590 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:47,593 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,594 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,594 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:47,596 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:47,599 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:47,602 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,603 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,603 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,603 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,603 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:47,608 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:47,611 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:47,615 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:47,618 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,619 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,619 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:47,620 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:47,623 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:47,626 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,626 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,626 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,627 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,627 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:47,631 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:47,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:47,637 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:47,641 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,642 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,642 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:47,644 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:47,646 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:47,650 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,650 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,650 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,650 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:47,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:47,658 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:47,661 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:47,664 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,665 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,665 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:47,667 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:47,669 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:47,672 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,672 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,673 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,673 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,673 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:47,677 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:47,680 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:47,683 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:47,687 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,688 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,688 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:47,689 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:47,692 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:47,695 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,695 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,696 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,696 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,696 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:47,700 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:47,704 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:47,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:47,710 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,711 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,711 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:47,713 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:47,716 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:47,719 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,719 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,719 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,719 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:47,724 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:47,728 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:47,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:47,734 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,735 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,735 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:47,736 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:47,739 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:47,742 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,743 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,743 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,743 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:47,747 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:47,751 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:47,755 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:47,758 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,759 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,759 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:47,761 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:47,764 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:47,767 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,767 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,768 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,768 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,768 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:47,772 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:47,776 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:47,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:47,782 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,783 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,783 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:47,784 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:47,787 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:47,790 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,790 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,791 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,791 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,791 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:47,797 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:47,801 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:47,805 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:47,808 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,809 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,809 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:47,810 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:47,813 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:47,816 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,816 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,816 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,817 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,817 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:47,821 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:47,824 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:47,828 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:47,831 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,831 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,831 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:47,833 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:47,835 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:47,839 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,839 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,839 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,839 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,839 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:47,844 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:47,847 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:47,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:47,854 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,855 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,856 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:47,857 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:47,860 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:47,861 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,861 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-09 07:03:47,861 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,861 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-09 07:03:47,861 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:47,866 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:47,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:47,873 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:47,876 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-09 07:03:47,877 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-09 07:03:47,877 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:47,879 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:47,879 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:47,880 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,880 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,880 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,881 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:47,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:47,881 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:47,882 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:47,882 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:47,882 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:47,882 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:47,883 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:47,883 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:47,884 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:47,884 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:47,884 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:47,884 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:47,885 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:47,929 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:47,968 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:48,006 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:48,043 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:48,045 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:48,045 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:48,068 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:48,069 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:48,070 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:48,070 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,071 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:48,071 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,071 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:48,072 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:48,072 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:48,072 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:48,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:48,073 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:48,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:48,074 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:48,075 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:48,079 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,080 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,080 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,080 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:48,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:48,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:48,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:48,082 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:48,082 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:48,082 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:48,083 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:48,085 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:48,088 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,089 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,089 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,089 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,089 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:48,094 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:48,098 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:48,101 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:48,104 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,105 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,105 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:48,107 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:48,110 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:48,113 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,114 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,114 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,114 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:48,118 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:48,122 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:48,126 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:48,129 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,130 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,130 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:48,131 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:48,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:48,137 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,138 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,138 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,138 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,138 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:48,142 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:48,147 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:48,151 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:48,154 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,155 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,155 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:48,157 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:48,159 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:48,163 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,163 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,163 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,163 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,163 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:48,168 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:48,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:48,175 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:48,179 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,179 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,180 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:48,181 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:48,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:48,187 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,187 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,187 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,188 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:48,192 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:48,196 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:48,199 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:48,202 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,203 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,203 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:48,204 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:48,207 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:48,211 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,211 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,211 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,211 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,211 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:48,215 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:48,219 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:48,222 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:48,226 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,227 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,227 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:48,228 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:48,231 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:48,234 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,235 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,235 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,235 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,235 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:48,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:48,243 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:48,246 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:48,249 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,250 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,250 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:48,251 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:48,254 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:48,257 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,257 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,257 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,257 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:48,261 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:48,265 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:48,268 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:48,272 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,272 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,272 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:48,274 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:48,277 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:48,281 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,281 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,281 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,281 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,281 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:48,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:48,288 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:48,291 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:48,295 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,296 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,296 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:48,298 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:48,300 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:48,304 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,304 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,304 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,304 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,304 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:48,308 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:48,312 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:48,316 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:48,319 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,320 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,320 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:48,321 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:48,324 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:48,327 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,327 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,328 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,328 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,328 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:48,332 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:48,336 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:48,340 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:48,343 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,343 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,344 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:48,345 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:48,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:48,351 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,351 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,352 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,352 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:48,357 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:48,361 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:48,365 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:48,368 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,368 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,369 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:48,370 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:48,373 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:48,376 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,377 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,377 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,377 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,377 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:48,382 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:48,385 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:48,389 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:48,392 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,393 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,393 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:48,395 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:48,398 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:48,402 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,402 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,402 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,403 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,403 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:48,407 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:48,410 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:48,413 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:48,417 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,418 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,419 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:48,421 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:48,423 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:48,427 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,428 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,428 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,428 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,428 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:48,433 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:48,437 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:48,441 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:48,444 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,445 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,445 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:48,448 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:48,451 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:48,454 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,454 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,454 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,454 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:48,458 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:48,462 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:48,465 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:48,468 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,469 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,469 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:48,471 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:48,473 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:48,477 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,477 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,477 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,478 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,478 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:48,482 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:48,486 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:48,490 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:48,494 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,495 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,495 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:48,498 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:48,501 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:48,504 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,505 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,505 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,505 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,505 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:48,511 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:48,515 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:48,519 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:48,522 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,523 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,524 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:48,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:48,529 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:48,532 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,533 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,533 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,533 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,533 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:48,537 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:48,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:48,544 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:48,548 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,549 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,549 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:48,551 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:48,554 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:48,557 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,557 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,557 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,557 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,557 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:48,562 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:48,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:48,570 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:48,573 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,574 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,574 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:48,575 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:48,578 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:48,582 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,582 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,582 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,582 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,582 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:48,586 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:48,590 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:48,593 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:48,596 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,597 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,597 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:48,599 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:48,602 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:48,605 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,605 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,606 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,606 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,606 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:48,610 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:48,613 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:48,616 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:48,620 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,621 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,621 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:48,622 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:48,625 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:48,628 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,628 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,629 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,629 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,629 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:48,633 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:48,637 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:48,640 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:48,644 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,645 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,645 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:48,647 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:48,650 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:48,650 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,651 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-09 07:03:48,651 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,651 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-09 07:03:48,651 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:48,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:48,659 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:48,663 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:48,667 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-09 07:03:48,668 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-09 07:03:48,668 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:48,670 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:48,671 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:48,671 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,671 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,671 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,671 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:48,672 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:48,672 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:48,672 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:48,672 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:48,673 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:48,673 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:48,673 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:48,674 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:48,674 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,674 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,674 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,674 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,674 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:48,723 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:48,763 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:48,805 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:48,847 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:48,849 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:48,849 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:48,874 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:48,876 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:48,876 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:48,877 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,877 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:48,877 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,877 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:48,878 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:48,878 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:48,878 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:48,878 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:48,879 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:48,879 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:48,879 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:48,880 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:48,884 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,884 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:48,884 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,884 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:48,884 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:48,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:48,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:48,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:48,885 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:48,886 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:48,886 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:48,887 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:48,889 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:48,892 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,893 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:48,893 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,893 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:48,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:48,898 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:48,902 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:48,906 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:48,909 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:48,910 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:48,910 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:48,911 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:48,914 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:48,917 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:48,918 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,918 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:48,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:48,922 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:48,926 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:48,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:48,933 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:48,933 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:48,934 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:48,935 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:48,938 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:48,941 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,942 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:48,942 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,942 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:48,942 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:48,947 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:48,950 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:48,953 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:48,956 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:48,957 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:48,957 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:48,958 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:48,961 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:48,964 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,965 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:48,965 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,965 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:48,965 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:48,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:48,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:48,977 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:48,980 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:48,981 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:48,981 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:48,982 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:48,985 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:48,988 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:48,989 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:48,989 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:48,989 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:48,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:48,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:48,998 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:49,003 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:49,006 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,007 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,007 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:49,008 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:49,011 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:49,014 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,015 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,015 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,015 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,015 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:49,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:49,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:49,027 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:49,030 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,030 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,031 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:49,032 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:49,035 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:49,039 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,039 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,039 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,039 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,039 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:49,045 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:49,048 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:49,051 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:49,055 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,056 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,056 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:49,058 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:49,061 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:49,064 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,064 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,064 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,064 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,064 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:49,068 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:49,073 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:49,076 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:49,080 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,081 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,081 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:49,083 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:49,086 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:49,089 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,089 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,090 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,090 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,090 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:49,095 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:49,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:49,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:49,110 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,110 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,111 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:49,113 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:49,117 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:49,121 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,121 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,121 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,121 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,121 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:49,126 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:49,130 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:49,133 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:49,138 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,139 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,139 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:49,140 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:49,144 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:49,148 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,148 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,149 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,149 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,149 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:49,155 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:49,159 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:49,164 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:49,176 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,177 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:49,180 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:49,185 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:49,190 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,190 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,191 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,191 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,192 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:49,197 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:49,202 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:49,207 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:49,211 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,212 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,213 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:49,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:49,217 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:49,221 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,221 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,221 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,222 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,222 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:49,226 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:49,230 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:49,233 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:49,237 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,238 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,238 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:49,240 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:49,243 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:49,246 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,246 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,247 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,247 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,247 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:49,252 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:49,255 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:49,259 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:49,262 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,263 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,263 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:49,265 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:49,268 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:49,271 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,271 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,272 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,272 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,272 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:49,276 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:49,280 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:49,284 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:49,288 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,289 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,289 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:49,291 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:49,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:49,297 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,297 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,297 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,298 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,298 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:49,302 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:49,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:49,310 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:49,313 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,314 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,314 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:49,316 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:49,319 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:49,322 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,322 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,323 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,323 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:49,327 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:49,331 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:49,334 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:49,337 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,338 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,338 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:49,340 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:49,343 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:49,346 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,346 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,346 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,347 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,347 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:49,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:49,355 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:49,358 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:49,361 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,362 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,362 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:49,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:49,367 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:49,370 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,370 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,371 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,371 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,371 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:49,378 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:49,381 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:49,386 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:49,389 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,390 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,390 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:49,392 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:49,394 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:49,397 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,398 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,398 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,398 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:49,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:49,406 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:49,410 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:49,413 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,414 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,414 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:49,416 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:49,419 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:49,422 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,423 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,423 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,423 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,423 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:49,428 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:49,431 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:49,434 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:49,437 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,438 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,438 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:49,440 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:49,443 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:49,447 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,447 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,447 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,448 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:49,452 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:49,457 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:49,460 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:49,463 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,464 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,464 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:49,466 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:49,469 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:49,473 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,474 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,474 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,475 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:49,480 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:49,484 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:49,488 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:49,492 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,493 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,493 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:49,495 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:49,498 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:49,499 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,500 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-09 07:03:49,500 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,500 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-09 07:03:49,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:49,505 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:49,508 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:49,512 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:49,516 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-09 07:03:49,516 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-09 07:03:49,517 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:49,518 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:49,519 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:49,519 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,519 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:49,520 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,520 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:49,520 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:49,520 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:49,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:49,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:49,521 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:49,522 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:49,522 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:49,523 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:49,523 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:49,524 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,524 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:49,524 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,524 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:49,524 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:49,567 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:49,606 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:49,645 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:49,691 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:49,693 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:49,694 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:49,716 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:49,717 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:49,718 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:49,718 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:49,719 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:49,719 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:49,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:49,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:49,720 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:49,720 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:49,720 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:49,721 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:49,721 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:49,721 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:49,722 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:49,725 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,725 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:49,726 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,726 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:49,726 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:49,728 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:49,729 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:49,730 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:49,730 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:49,730 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:49,730 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:49,731 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:49,734 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:49,738 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,738 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,739 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,739 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,739 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:49,752 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:49,756 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:49,759 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:49,763 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,764 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,764 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:49,766 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:49,769 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:49,772 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,773 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,773 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,773 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,773 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:49,778 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:49,781 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:49,784 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:49,788 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,789 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,789 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:49,790 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:49,793 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:49,797 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,797 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,797 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,797 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,797 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:49,801 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:49,805 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:49,808 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:49,811 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,812 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,812 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:49,814 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:49,816 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:49,819 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,820 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,820 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,820 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,820 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:49,824 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:49,828 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:49,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:49,834 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,835 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,835 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:49,836 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:49,839 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:49,842 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,842 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,842 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,843 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,843 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:49,848 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:49,852 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:49,856 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:49,859 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,860 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,860 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:49,861 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:49,864 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:49,867 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,867 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,868 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,868 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,868 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:49,873 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:49,876 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:49,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:49,884 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,893 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,893 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:49,894 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:49,898 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:49,902 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,902 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,902 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,902 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,902 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:49,908 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:49,913 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:49,919 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:49,925 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,926 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,926 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:49,928 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:49,932 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:49,938 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,939 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,939 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,940 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:49,947 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:49,954 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:49,959 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:49,964 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,965 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,965 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:49,966 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:49,969 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:49,972 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:49,973 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:49,973 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:49,973 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:49,973 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:49,979 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:49,983 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:49,988 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:49,993 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:49,994 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:49,994 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:49,996 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:49,999 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:50,002 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,002 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,003 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,003 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:50,009 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:50,014 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:50,018 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:50,023 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,024 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,024 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:50,026 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:50,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:50,032 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,032 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,032 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,033 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,033 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:50,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:50,043 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:50,048 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:50,052 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,053 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,053 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:50,055 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:50,058 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:50,061 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,061 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,062 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,062 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,062 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:50,069 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:50,074 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:50,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:50,085 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,087 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,087 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:50,089 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:50,093 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:50,097 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,097 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,098 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,098 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,098 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:50,104 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:50,109 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:50,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:50,117 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,118 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,118 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:50,120 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:50,123 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:50,126 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,126 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,127 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,127 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:50,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:50,137 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:50,141 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:50,145 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,146 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,146 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:50,147 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:50,150 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:50,153 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,154 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,154 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,154 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,155 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:50,159 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:50,163 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:50,167 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:50,171 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,172 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,172 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:50,174 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:50,177 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:50,180 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,180 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,180 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,181 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,181 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:50,186 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:50,191 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:50,196 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:50,200 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,201 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,201 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:50,203 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:50,206 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:50,210 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,210 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,211 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,211 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,211 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:50,215 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:50,220 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:50,224 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:50,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,228 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,229 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:50,230 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:50,233 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:50,236 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,236 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,237 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,237 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,237 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:50,241 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:50,245 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:50,249 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:50,252 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,253 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,253 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:50,254 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:50,257 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:50,260 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,260 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,260 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,261 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,261 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:50,265 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:50,269 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:50,273 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:50,276 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,277 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,277 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:50,279 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:50,282 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:50,285 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,285 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,285 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,285 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:50,290 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:50,294 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:50,298 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:50,302 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,303 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,303 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:50,304 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:50,307 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:50,310 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,311 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,311 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,311 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,311 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:50,315 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:50,319 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:50,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:50,326 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,327 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,327 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:50,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:50,332 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:50,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,335 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,335 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,335 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,335 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:50,339 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:50,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:50,347 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:50,350 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,351 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,351 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:50,352 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:50,355 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:50,359 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,359 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,359 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,359 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,359 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:50,363 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:50,367 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:50,370 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:50,374 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,374 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,375 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:50,376 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:50,379 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:50,380 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,380 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-09 07:03:50,380 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,380 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-09 07:03:50,380 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:50,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:50,389 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:50,392 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:50,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-09 07:03:50,398 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-09 07:03:50,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:50,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:50,401 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:50,401 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,401 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:50,401 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,401 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:50,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:50,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:50,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:50,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:50,402 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:50,403 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:50,403 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:50,403 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:50,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:50,404 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,404 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:50,405 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,405 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:50,405 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:50,449 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:50,487 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:50,526 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:50,564 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:50,566 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:50,567 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:50,589 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:50,590 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:50,591 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:50,591 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:50,591 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:50,592 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:50,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:50,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:50,593 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:50,593 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:50,593 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:50,593 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:50,593 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:50,594 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:50,594 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:50,598 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,598 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:50,598 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,598 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:50,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:50,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:50,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:50,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:50,599 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:50,600 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:50,600 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:50,600 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:50,603 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:50,607 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,607 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,607 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,607 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,608 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:50,612 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:50,616 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:50,619 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:50,622 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,624 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,624 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:50,625 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:50,628 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:50,631 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,631 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,632 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,632 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,632 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:50,637 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:50,640 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:50,644 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:50,648 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,648 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,648 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:50,650 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:50,652 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:50,656 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,656 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,656 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,656 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:50,660 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:50,664 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:50,667 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:50,671 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,672 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,672 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:50,674 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:50,676 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:50,680 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,680 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,680 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,680 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,680 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:50,684 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:50,688 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:50,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:50,696 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,697 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,697 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:50,698 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:50,701 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:50,704 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,704 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,704 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,705 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,705 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:50,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:50,714 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:50,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:50,721 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,722 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,723 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:50,724 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:50,727 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:50,730 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,730 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,730 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,731 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:50,735 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:50,739 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:50,743 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:50,747 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,748 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,748 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:50,749 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:50,752 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:50,755 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,755 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,755 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,755 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,756 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:50,760 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:50,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:50,767 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:50,771 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,772 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,772 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:50,773 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:50,776 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:50,779 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,780 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,780 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,780 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,780 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:50,784 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:50,789 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:50,793 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:50,796 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,797 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,797 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:50,798 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:50,801 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:50,804 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,805 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,805 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,805 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,805 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:50,810 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:50,814 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:50,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:50,821 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,822 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,822 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:50,824 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:50,826 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:50,829 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,830 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,830 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,830 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:50,834 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:50,838 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:50,842 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:50,845 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,846 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,846 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:50,847 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:50,851 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:50,854 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,854 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,854 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,855 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:50,859 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:50,863 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:50,867 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:50,871 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,872 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,872 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:50,874 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:50,877 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:50,880 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,880 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,880 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,880 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:50,885 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:50,889 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:50,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:50,897 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,898 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,898 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:50,899 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:50,902 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:50,905 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,905 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,905 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,906 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,906 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:50,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:50,914 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:50,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:50,921 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,922 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,922 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:50,924 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:50,927 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:50,930 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,930 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,930 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,930 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,930 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:50,935 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:50,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:50,944 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:50,947 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,949 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,949 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:50,950 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:50,953 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:50,956 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,956 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,956 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,956 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,957 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:50,961 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:50,964 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:50,968 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:50,980 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:50,981 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:50,981 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:50,983 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:50,986 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:50,989 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:50,989 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:50,989 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:50,989 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:50,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:50,994 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:50,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:51,003 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:51,006 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,007 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,008 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:51,009 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:51,012 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:51,015 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,016 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,016 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,016 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:51,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:51,025 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:51,029 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:51,032 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,033 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,034 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:51,036 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:51,039 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:51,042 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,042 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,043 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,043 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,043 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:51,048 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:51,052 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:51,056 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:51,061 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,062 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,063 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:51,064 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:51,068 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:51,073 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,073 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,074 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,074 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,074 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:51,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:51,084 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:51,088 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:51,092 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,093 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,093 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:51,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:51,097 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:51,100 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,100 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,101 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,101 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,101 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:51,105 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:51,109 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:51,113 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:51,118 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,118 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,118 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:51,120 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:51,122 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:51,126 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,126 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,126 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,126 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:51,133 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:51,137 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:51,141 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:51,145 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,145 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,146 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:51,147 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:51,150 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:51,153 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,153 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,153 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,153 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,154 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:51,158 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:51,162 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:51,166 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:51,171 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,172 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,172 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:51,173 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:51,176 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:51,179 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,180 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,180 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,180 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:51,185 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:51,189 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:51,193 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:51,197 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,198 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,198 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:51,200 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:51,203 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:51,204 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,204 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-09 07:03:51,204 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,205 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-09 07:03:51,205 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:51,209 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:51,213 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:51,217 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:51,221 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-09 07:03:51,222 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-09 07:03:51,222 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:51,224 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:51,224 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:51,225 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,225 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:51,225 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,226 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:51,226 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:51,226 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:51,226 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:51,227 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:51,227 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:51,227 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:51,227 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:51,228 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:51,228 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:51,229 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,229 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:51,229 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,230 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:51,230 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:51,274 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:51,315 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:51,356 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:51,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:51,399 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:51,400 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:51,423 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:51,424 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:51,425 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:51,426 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:51,426 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:51,426 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:51,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:51,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:51,428 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:51,429 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:51,429 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:51,429 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:51,430 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:51,430 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:51,430 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:51,434 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,435 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:51,435 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,435 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:51,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:51,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:51,436 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:51,436 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:51,436 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:51,436 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:51,436 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:51,437 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:51,440 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:51,443 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,443 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,443 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,444 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,444 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:51,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:51,452 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:51,456 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:51,460 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,460 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,461 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:51,462 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:51,465 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:51,468 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,468 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,468 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,469 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,469 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:51,473 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:51,479 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:51,483 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:51,486 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,487 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,488 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:51,489 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:51,492 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:51,495 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,495 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,495 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,496 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,496 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:51,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:51,504 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:51,508 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:51,513 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,514 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,515 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:51,516 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:51,519 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:51,523 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,524 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,524 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,524 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,524 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:51,549 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:51,586 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:51,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:51,597 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,598 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,599 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:51,601 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:51,605 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:51,609 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,609 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,610 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,610 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,611 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:51,625 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:51,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:51,658 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:51,663 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,664 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,665 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:51,667 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:51,670 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:51,675 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,675 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,676 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,677 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,677 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:51,682 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:51,687 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:51,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:51,697 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,713 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,714 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:51,715 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:51,719 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:51,722 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,722 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,723 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,723 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:51,729 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:51,734 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:51,739 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:51,743 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,744 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,744 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:51,746 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:51,749 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:51,752 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,752 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,753 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,753 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,753 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:51,758 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:51,762 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:51,766 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:51,770 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,771 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,771 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:51,772 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:51,775 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:51,779 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,779 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,779 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,780 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,780 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:51,784 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:51,789 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:51,793 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:51,798 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,799 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,799 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:51,801 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:51,804 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:51,807 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,807 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,808 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,808 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,808 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:51,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:51,823 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:51,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:51,831 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,832 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,833 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:51,834 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:51,837 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:51,840 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,841 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,841 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,841 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,841 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:51,846 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:51,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:51,856 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:51,860 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,861 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,861 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:51,863 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:51,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:51,869 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,869 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,869 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,869 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:51,874 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:51,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:51,884 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:51,890 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,890 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,891 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:51,892 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:51,895 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:51,899 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,899 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,899 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,899 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,899 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:51,904 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:51,908 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:51,912 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:51,916 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,917 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,918 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:51,919 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:51,922 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:51,925 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,926 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,926 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,926 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,926 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:51,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:51,935 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:51,939 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:51,943 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,944 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,944 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:51,945 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:51,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:51,952 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,952 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,953 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,953 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:51,957 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:51,962 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:51,966 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:51,971 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,971 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,971 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:51,973 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:51,976 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:51,979 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:51,979 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:51,979 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:51,980 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:51,980 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:51,985 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:51,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:51,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:51,998 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:51,999 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:51,999 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:52,000 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:52,003 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:52,007 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,007 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,007 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,007 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,007 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:52,013 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:52,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:52,021 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:52,025 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,025 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,026 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:52,027 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:52,030 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:52,033 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,034 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,034 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,034 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,034 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:52,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:52,042 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:52,046 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:52,050 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,050 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,051 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:52,052 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:52,055 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:52,058 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,058 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,059 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,059 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,059 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:52,063 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:52,067 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:52,071 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:52,075 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,077 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,077 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:52,078 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:52,081 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:52,084 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,084 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,085 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,085 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,085 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:52,090 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:52,094 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:52,099 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:52,103 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,105 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,105 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:52,106 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:52,109 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:52,113 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,113 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,113 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,113 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:52,118 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:52,122 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:52,126 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:52,129 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,130 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,130 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:52,132 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:52,135 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:52,138 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,138 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,138 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,139 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,139 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:52,143 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:52,147 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:52,151 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:52,155 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,155 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,155 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:52,157 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:52,159 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:52,163 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,163 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,163 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,164 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,164 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:52,168 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:52,172 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:52,176 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:52,180 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,181 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,181 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:52,183 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:52,186 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:52,186 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,186 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-09 07:03:52,187 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,187 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-09 07:03:52,187 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:52,192 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:52,196 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:52,200 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:52,204 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-09 07:03:52,204 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-09 07:03:52,205 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:52,206 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:52,207 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:52,207 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,207 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:52,207 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,208 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:52,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:52,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:52,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:52,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:52,209 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:52,209 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:52,209 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:52,210 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:52,210 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:52,210 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,211 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:52,211 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,211 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:52,211 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:52,281 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:52,338 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:52,393 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:52,436 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:52,438 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:52,439 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:52,462 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:52,463 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:52,464 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:52,464 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:52,464 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:52,464 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:52,465 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:52,465 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:52,465 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:52,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:52,466 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:52,466 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:52,466 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:52,467 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:52,468 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:52,471 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,472 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:52,472 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,472 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:52,472 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:52,472 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:52,473 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:52,473 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:52,473 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:52,473 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:52,473 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:52,474 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:52,477 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:52,480 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,481 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,481 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,481 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,481 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:52,490 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:52,496 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:52,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:52,506 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,507 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,507 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:52,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:52,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:52,515 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,515 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,516 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,516 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,516 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:52,523 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:52,527 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:52,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:52,534 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,535 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,535 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:52,536 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:52,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:52,543 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,543 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,543 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,543 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,543 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:52,548 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:52,552 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:52,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:52,560 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,560 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,561 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:52,562 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:52,565 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:52,568 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,568 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,568 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,568 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,569 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:52,574 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:52,580 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:52,585 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:52,589 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,590 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,590 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:52,591 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:52,594 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:52,597 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,597 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,597 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,598 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,598 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:52,602 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:52,606 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:52,610 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:52,614 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,615 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,615 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:52,616 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:52,619 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:52,623 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,623 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,623 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,623 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,623 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:52,628 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:52,632 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:52,635 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:52,639 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,640 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,640 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:52,641 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:52,644 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:52,647 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,648 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,648 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,648 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:52,652 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:52,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:52,660 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:52,664 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,665 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,665 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:52,666 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:52,669 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:52,672 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,672 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,672 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,672 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,673 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:52,678 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:52,682 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:52,686 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:52,691 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,691 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,691 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:52,693 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:52,695 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:52,699 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,699 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,699 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,699 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,699 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:52,704 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:52,708 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:52,712 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:52,715 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,716 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,717 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:52,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:52,721 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:52,724 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,724 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,724 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,725 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,725 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:52,729 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:52,733 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:52,737 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:52,741 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,741 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,742 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:52,743 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:52,746 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:52,749 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,749 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,750 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,750 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,750 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:52,754 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:52,758 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:52,762 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:52,766 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,766 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,767 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:52,768 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:52,771 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:52,774 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,774 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,774 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,775 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,775 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:52,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:52,783 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:52,786 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:52,790 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,791 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,791 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:52,792 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:52,795 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:52,798 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,799 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,799 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,799 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,799 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:52,803 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:52,807 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:52,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:52,814 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,815 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,815 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:52,817 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:52,819 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:52,823 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,823 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,823 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,823 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,823 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:52,827 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:52,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:52,834 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:52,838 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,839 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,839 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:52,840 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:52,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:52,846 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,847 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,847 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,847 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,847 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:52,851 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:52,855 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:52,858 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:52,862 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,863 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,863 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:52,865 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:52,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:52,871 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,871 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,871 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,871 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,871 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:52,876 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:52,879 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:52,883 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:52,886 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,887 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,887 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:52,889 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:52,892 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:52,896 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,896 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,897 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,897 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,897 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:52,901 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:52,905 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:52,909 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:52,913 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,917 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,918 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:52,920 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:52,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:52,926 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,927 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,927 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,927 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,927 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:52,932 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:52,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:52,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:52,944 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,945 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,945 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:52,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:52,951 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:52,955 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,955 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,956 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,956 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,956 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:52,960 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:52,965 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:52,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:52,974 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:52,975 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:52,975 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:52,978 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:52,981 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:52,985 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:52,985 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:52,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:52,986 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:52,986 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:52,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:52,996 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:53,001 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:53,006 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:53,007 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:53,007 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:53,008 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:53,011 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:53,016 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,016 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:53,017 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,017 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:53,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:53,023 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:53,027 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:53,032 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:53,037 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:53,038 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:53,038 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:53,040 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:53,043 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:53,047 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,047 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:53,047 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,047 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:53,047 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:53,056 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:53,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:53,067 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:53,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:53,074 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:53,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:53,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:53,078 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:53,082 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,082 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:53,083 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,083 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:53,083 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:53,088 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:53,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:53,097 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:53,102 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:53,103 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:53,104 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:53,106 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:53,109 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:53,110 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,110 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-09 07:03:53,110 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,110 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-09 07:03:53,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:53,115 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:53,119 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:53,123 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:53,127 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-09 07:03:53,128 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-09 07:03:53,128 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:53,129 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:53,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:53,131 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,131 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:53,131 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,131 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:53,131 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:53,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:53,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:53,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:53,133 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:53,133 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:53,133 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:53,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:53,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:53,135 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,135 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:53,135 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,135 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:53,136 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:53,190 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:53,234 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:53,275 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:53,315 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:53,317 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:53,318 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:53,340 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:53,342 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:53,343 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:53,343 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:53,343 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:53,344 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:53,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:53,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:53,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:53,345 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:53,346 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:53,346 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:53,346 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:53,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:53,347 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:53,351 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,351 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:53,351 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,351 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:53,351 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:53,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:53,353 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:53,353 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:53,354 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:53,354 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:53,354 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:53,355 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:53,357 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:53,361 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,361 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,361 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,362 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,362 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:53,366 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:53,397 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:53,401 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:53,406 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,407 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,407 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:53,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:53,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:53,416 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,416 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,417 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,417 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,417 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:53,422 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:53,426 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:53,429 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:53,433 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,434 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,434 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:53,436 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:53,439 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:53,442 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,443 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,443 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,443 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,443 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:53,447 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:53,452 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:53,455 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:53,459 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,461 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,461 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:53,462 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:53,465 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:53,469 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,469 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,469 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,470 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,470 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:53,474 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:53,478 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:53,481 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:53,485 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,486 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,486 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:53,487 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:53,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:53,494 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,494 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,494 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,494 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,494 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:53,498 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:53,503 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:53,506 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:53,510 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,511 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,511 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:53,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:53,515 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:53,519 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,519 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,519 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,519 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,519 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:53,523 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:53,527 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:53,531 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:53,535 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,536 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,536 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:53,538 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:53,541 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:53,544 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,544 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,545 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,545 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,545 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:53,551 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:53,555 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:53,560 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:53,564 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,564 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,565 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:53,566 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:53,569 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:53,572 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,572 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,572 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,572 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,573 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:53,577 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:53,582 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:53,586 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:53,590 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,591 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,591 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:53,592 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:53,595 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:53,599 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,599 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,599 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,599 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,600 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:53,604 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:53,608 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:53,612 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:53,616 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,617 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,617 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:53,619 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:53,622 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:53,625 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,625 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,625 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,625 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,626 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:53,631 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:53,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:53,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:53,642 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,642 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,643 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:53,644 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:53,647 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:53,650 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,651 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,651 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,651 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,651 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:53,656 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:53,660 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:53,664 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:53,668 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,668 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,668 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:53,670 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:53,673 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:53,676 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,677 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,677 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,677 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,677 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:53,682 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:53,686 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:53,691 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:53,694 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,695 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,695 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:53,697 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:53,700 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:53,703 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,703 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,704 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,704 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,704 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:53,709 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:53,713 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:53,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:53,721 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,722 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,722 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:53,724 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:53,727 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:53,731 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,731 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,731 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,731 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,731 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:53,745 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:53,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:53,782 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:53,793 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,799 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,800 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:53,802 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:53,805 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:53,809 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,810 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,810 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,811 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:53,818 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:53,831 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:53,847 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:53,852 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,853 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,854 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:53,856 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:53,859 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:53,862 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,863 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,863 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,864 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,864 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:53,870 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:53,875 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:53,880 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:53,884 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,886 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,886 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:53,888 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:53,891 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:53,895 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,895 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,896 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,896 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,897 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:53,902 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:53,907 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:53,912 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:53,916 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,917 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,918 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:53,920 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:53,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:53,927 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,927 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,928 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,928 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,929 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:53,934 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:53,939 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:53,945 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:53,950 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,951 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,952 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:53,953 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:53,956 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:53,960 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,960 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,960 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,961 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,961 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:53,966 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:53,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:53,975 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:53,979 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:53,980 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:53,980 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:53,982 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:53,985 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:53,988 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:53,989 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:53,989 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:53,989 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:53,990 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:53,994 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:53,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:54,004 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:54,009 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:54,010 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:54,010 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:54,012 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:54,015 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:54,018 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,018 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:54,018 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,019 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:54,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:54,024 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:54,028 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:54,032 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:54,036 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:54,037 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:54,037 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:54,039 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:54,042 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:54,045 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,046 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:54,046 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,046 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:54,047 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:54,052 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:54,056 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:54,060 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:54,064 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:54,067 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:54,067 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:54,068 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:54,071 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:54,075 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,075 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:54,076 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,076 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:54,076 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:54,081 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:54,086 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:54,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:54,095 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:54,096 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:54,096 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:54,098 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:54,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:54,102 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,103 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-09 07:03:54,103 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,103 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-09 07:03:54,103 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:54,108 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:54,112 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:54,117 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:54,121 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-09 07:03:54,122 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-09 07:03:54,122 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:54,124 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:54,125 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:54,125 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,125 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,126 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,126 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,126 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:54,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:54,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:54,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:54,128 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:54,128 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:54,128 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:54,129 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:54,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:54,130 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,130 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,131 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,131 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,131 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:54,186 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:54,229 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:54,271 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:54,313 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:54,315 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:54,315 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:54,340 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:54,341 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:54,342 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:54,342 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,342 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:54,342 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,342 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:54,342 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:54,343 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:54,343 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:54,343 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:54,343 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:54,343 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:54,344 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:54,344 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:54,349 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,349 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,349 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,349 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,349 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:54,349 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:54,350 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:54,350 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:54,350 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:54,350 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:54,350 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:54,351 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:54,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:54,357 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,357 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,358 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,358 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,358 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:54,364 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:54,368 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:54,373 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:54,376 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,377 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,377 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:54,379 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:54,382 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:54,385 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,385 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,385 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,386 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,386 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:54,390 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:54,394 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:54,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:54,402 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,402 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,403 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:54,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:54,406 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:54,410 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,410 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,410 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,411 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,411 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:54,415 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:54,420 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:54,423 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:54,426 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,427 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,428 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:54,429 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:54,431 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:54,435 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,435 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,435 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,435 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,436 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:54,440 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:54,444 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:54,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:54,452 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,453 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,453 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:54,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:54,457 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:54,460 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,460 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,461 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,461 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,461 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:54,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:54,471 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:54,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:54,478 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,479 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,479 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:54,480 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:54,483 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:54,487 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,487 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,487 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,487 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,488 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:54,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:54,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:54,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:54,505 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,506 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,506 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:54,507 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:54,510 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:54,513 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,514 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,514 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,514 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,514 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:54,519 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:54,524 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:54,527 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:54,531 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,532 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,533 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:54,534 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:54,536 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:54,540 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,540 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,540 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,541 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:54,545 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:54,549 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:54,552 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:54,556 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,556 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,557 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:54,558 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:54,561 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:54,564 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,564 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,564 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,565 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,565 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:54,569 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:54,573 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:54,576 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:54,579 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,580 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,580 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:54,582 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:54,585 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:54,588 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,588 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,588 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,588 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,588 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:54,593 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:54,596 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:54,600 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:54,604 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,605 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,605 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:54,607 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:54,610 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:54,613 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,614 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,614 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,614 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,614 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:54,619 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:54,622 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:54,626 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:54,629 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,630 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,630 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:54,632 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:54,634 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:54,638 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,638 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,638 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,638 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,638 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:54,646 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:54,651 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:54,658 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:54,663 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,665 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,666 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:54,668 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:54,673 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:54,679 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,680 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,680 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,681 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,681 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:54,687 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:54,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:54,696 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:54,701 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,701 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,702 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:54,703 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:54,706 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:54,709 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,709 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,710 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,710 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,710 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:54,715 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:54,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:54,723 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:54,726 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,727 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,727 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:54,728 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:54,731 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:54,734 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,735 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,735 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,735 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,735 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:54,739 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:54,743 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:54,747 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:54,751 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,752 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,752 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:54,753 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:54,756 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:54,759 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,759 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,760 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,760 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,760 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:54,764 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:54,768 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:54,771 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:54,774 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,775 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,775 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:54,776 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:54,779 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:54,783 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,783 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,783 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,783 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,783 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:54,788 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:54,792 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:54,795 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:54,799 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,800 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,800 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:54,802 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:54,805 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:54,808 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,808 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,808 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,808 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:54,813 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:54,817 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:54,820 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:54,824 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,825 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,825 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:54,827 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:54,829 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:54,833 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,833 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,833 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,833 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,834 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:54,838 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:54,842 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:54,845 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:54,849 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,849 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,849 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:54,851 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:54,854 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:54,857 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,857 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,857 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,857 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,857 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:54,862 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:54,866 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:54,869 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:54,872 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,873 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,874 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:54,875 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:54,877 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:54,881 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,881 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,881 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,882 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,882 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:54,886 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:54,890 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:54,893 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:54,897 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,898 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,898 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:54,900 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:54,902 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:54,906 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,906 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,906 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,906 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,906 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:54,911 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:54,914 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:54,918 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:54,921 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,922 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,922 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:54,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:54,926 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:54,930 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,930 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,930 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,930 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:54,935 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:54,939 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:54,942 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:54,945 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,946 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,946 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:54,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:54,951 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:54,951 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,951 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-09 07:03:54,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,952 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-09 07:03:54,952 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:54,956 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:54,960 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:54,963 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:54,966 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-09 07:03:54,966 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-09 07:03:54,967 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:54,968 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:54,968 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:54,969 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,969 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,969 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,969 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:54,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:54,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:54,970 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:54,971 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:54,971 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:54,971 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:54,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:54,972 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:54,972 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:54,972 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:54,973 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:54,973 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:54,973 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:55,018 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:55,058 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:55,099 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:55,140 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:55,141 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:55,142 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:55,167 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:55,168 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:55,169 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:55,170 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:55,170 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:55,170 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:55,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:55,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:55,172 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:55,173 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:55,173 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:55,173 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:55,174 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:55,174 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:55,174 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:55,179 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,179 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:55,179 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,179 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:55,179 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:55,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:55,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:55,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:55,180 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:55,181 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:55,181 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:55,181 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:55,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:55,188 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,188 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,188 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,188 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:55,193 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:55,196 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:55,200 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:55,203 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,204 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,204 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:55,205 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:55,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:55,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,212 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,212 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,212 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:55,216 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:55,220 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:55,223 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:55,227 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,228 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,228 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:55,230 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:55,232 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:55,236 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,236 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,236 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,236 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,236 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:55,241 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:55,244 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:55,247 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:55,250 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,251 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,252 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:55,253 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:55,256 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:55,259 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,259 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,259 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,260 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,260 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:55,264 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:55,268 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:55,271 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:55,274 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,275 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,275 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:55,277 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:55,279 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:55,283 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,283 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,283 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,284 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,284 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:55,288 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:55,292 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:55,296 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:55,299 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,300 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,301 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:55,302 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:55,305 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:55,308 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,308 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,309 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,309 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,309 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:55,313 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:55,317 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:55,321 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:55,324 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,325 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,325 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:55,327 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:55,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:55,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,334 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,334 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,335 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,335 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:55,339 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:55,343 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:55,346 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:55,349 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,350 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,350 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:55,352 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:55,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:55,358 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,358 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,358 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,358 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,358 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:55,363 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:55,367 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:55,370 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:55,372 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,373 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,373 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:55,375 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:55,378 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:55,381 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,382 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,382 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,382 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,382 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:55,387 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:55,390 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:55,394 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:55,399 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,400 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,400 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:55,402 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:55,405 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:55,408 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,408 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,408 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,409 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,409 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:55,414 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:55,418 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:55,421 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:55,423 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,425 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,425 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:55,426 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:55,429 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:55,433 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,433 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,433 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,433 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,433 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:55,437 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:55,442 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:55,446 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:55,449 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,450 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,450 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:55,452 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:55,455 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:55,458 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,458 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,458 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,458 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,458 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:55,463 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:55,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:55,469 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:55,473 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,474 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,474 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:55,475 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:55,478 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:55,482 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,482 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,482 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,482 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,483 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:55,488 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:55,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:55,495 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:55,498 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,499 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,499 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:55,501 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:55,504 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:55,507 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,507 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,507 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,508 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,508 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:55,512 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:55,516 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:55,520 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:55,523 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,524 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,524 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:55,526 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:55,528 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:55,532 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,532 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,532 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,532 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,533 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:55,537 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:55,542 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:55,545 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:55,548 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,549 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,550 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:55,551 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:55,554 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:55,557 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,557 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,558 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,558 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,558 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:55,563 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:55,566 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:55,570 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:55,574 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,575 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,575 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:55,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:55,579 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:55,582 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,583 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,583 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,583 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,583 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:55,588 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:55,592 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:55,596 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:55,600 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,601 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,601 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:55,603 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:55,605 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:55,608 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,609 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,609 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,609 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,609 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:55,614 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:55,619 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:55,623 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:55,626 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,627 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,627 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:55,628 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:55,631 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:55,635 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,635 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,635 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,635 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,635 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:55,640 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:55,645 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:55,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:55,653 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,654 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,654 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:55,656 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:55,659 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:55,662 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,662 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,662 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,663 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,663 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:55,667 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:55,671 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:55,674 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:55,678 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,679 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,679 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:55,680 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:55,683 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:55,686 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,686 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,687 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,687 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,687 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:55,691 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:55,695 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:55,699 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:55,702 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,703 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,703 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:55,705 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:55,707 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:55,711 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,711 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,711 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,711 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,711 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:55,715 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:55,719 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:55,722 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:55,725 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,726 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,726 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:55,728 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:55,730 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:55,734 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,734 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,734 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,734 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,734 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:55,740 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:55,744 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:55,747 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:55,750 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,751 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,751 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:55,753 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:55,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:55,756 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,756 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-09 07:03:55,756 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,757 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-09 07:03:55,757 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:55,763 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:55,767 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:55,771 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:55,775 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-09 07:03:55,776 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-09 07:03:55,776 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:55,777 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:55,778 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:55,778 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,778 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:55,779 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,779 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:55,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:55,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:55,779 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:55,780 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:55,780 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:55,780 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:55,780 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:55,781 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:55,781 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:55,782 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:55,782 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:55,782 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:55,782 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:55,782 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:55,841 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:55,889 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:55,933 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:55,978 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:55,981 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:55,981 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:56,007 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:56,009 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:56,010 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:56,010 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,010 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:56,010 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,010 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:56,010 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:56,011 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:56,011 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:56,011 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:56,012 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:56,012 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:56,012 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:56,013 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:56,017 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,017 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,018 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,018 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,018 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:56,018 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:56,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:56,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:56,019 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:56,019 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:56,019 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:56,020 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:56,023 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:56,028 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,028 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,028 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,028 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,028 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:56,035 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:56,039 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:56,043 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:56,046 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,047 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,047 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:56,049 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:56,052 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:56,056 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,056 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,056 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,056 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:56,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:56,066 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:56,070 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:56,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,074 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:56,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:56,079 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:56,083 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,083 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,083 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,084 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:56,089 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:56,093 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:56,097 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:56,101 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,102 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,102 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:56,103 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:56,107 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:56,110 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,111 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,111 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,111 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:56,116 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:56,120 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:56,124 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:56,128 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,129 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,129 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:56,131 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:56,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:56,138 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,138 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,138 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,138 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,139 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:56,144 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:56,148 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:56,152 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:56,156 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,157 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,157 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:56,158 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:56,162 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:56,165 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,166 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,166 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,166 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,166 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:56,171 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:56,175 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:56,180 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:56,183 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,184 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,185 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:56,186 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:56,189 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:56,193 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,194 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,194 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,194 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,194 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:56,199 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:56,204 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:56,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:56,211 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,212 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,212 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:56,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:56,217 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:56,220 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,220 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,220 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,220 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,221 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:56,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:56,230 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:56,233 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:56,237 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,238 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,238 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:56,240 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:56,243 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:56,247 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,247 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,247 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,247 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,248 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:56,253 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:56,257 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:56,261 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:56,264 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,265 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,266 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:56,267 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:56,271 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:56,274 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,274 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,275 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,275 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:56,280 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:56,285 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:56,289 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:56,294 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,294 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,295 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:56,296 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:56,299 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:56,303 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,303 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,303 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,303 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,304 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:56,309 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:56,315 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:56,319 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:56,324 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,325 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,326 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:56,327 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:56,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:56,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,334 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,334 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,334 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,335 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:56,340 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:56,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:56,348 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:56,352 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,354 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,354 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:56,355 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:56,359 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:56,363 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,363 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,364 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,364 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,364 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:56,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:56,374 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:56,378 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:56,383 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,384 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,384 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:56,386 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:56,389 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:56,392 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,393 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,393 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,393 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:56,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:56,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:56,406 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:56,411 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,412 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,412 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:56,414 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:56,417 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:56,420 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,421 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,421 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,421 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,421 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:56,427 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:56,431 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:56,435 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:56,439 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,441 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,442 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:56,444 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:56,447 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:56,452 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,453 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,453 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,453 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,453 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:56,460 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:56,464 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:56,471 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:56,475 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,476 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,476 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:56,478 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:56,482 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:56,487 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,487 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,488 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,488 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,488 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:56,494 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:56,499 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:56,504 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:56,509 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,512 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,512 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:56,514 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:56,518 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:56,522 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,523 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,523 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,523 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,524 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:56,529 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:56,534 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:56,538 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:56,542 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,543 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,544 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:56,545 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:56,549 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:56,554 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,555 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,555 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,556 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,556 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:56,562 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:56,567 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:56,571 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:56,575 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,576 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,576 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:56,578 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:56,580 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:56,584 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,584 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,584 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,584 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,584 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:56,589 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:56,594 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:56,599 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:56,602 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,603 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,604 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:56,605 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:56,608 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:56,611 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,611 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,612 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,612 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,612 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:56,617 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:56,622 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:56,625 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:56,629 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,630 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,631 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:56,632 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:56,636 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:56,639 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,639 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,640 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,640 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,640 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:56,645 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:56,650 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:56,654 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:56,658 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,659 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,660 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:56,661 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:56,664 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:56,667 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,667 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,668 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,668 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,668 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:56,673 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:56,677 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:56,681 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:56,685 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,686 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,686 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:56,688 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:56,691 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:56,691 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,692 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-09 07:03:56,692 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,692 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-09 07:03:56,692 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:56,697 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:56,702 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:56,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:56,711 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-09 07:03:56,712 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-09 07:03:56,712 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:56,713 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:56,714 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:56,715 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,715 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,715 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,715 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,716 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:56,716 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:56,716 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:56,717 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:56,717 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:56,717 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:56,717 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:56,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:56,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:56,719 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,719 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,719 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,720 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,720 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:56,779 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:56,823 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:56,866 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:56,910 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:56,911 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:56,912 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:56,936 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:56,937 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:56,938 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:56,939 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,939 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:56,939 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:56,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:56,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:56,941 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:56,941 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:56,942 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:56,942 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:56,943 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:56,944 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:56,948 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,948 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:56,948 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,949 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:56,949 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:56,951 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:56,951 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:56,952 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:56,952 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:56,952 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:56,952 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:56,953 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:56,956 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:56,959 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,960 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:56,960 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,960 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:56,960 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:56,964 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:56,968 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:56,972 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:56,975 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:56,976 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:56,976 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:56,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:56,980 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:56,984 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:56,984 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:56,984 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:56,984 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:56,984 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:56,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:56,994 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:56,999 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:57,003 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,004 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,004 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:57,006 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:57,009 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:57,012 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,012 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,012 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,013 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,013 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:57,017 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:57,022 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:57,026 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:57,031 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,031 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,032 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:57,033 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:57,036 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:57,039 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,040 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,040 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,040 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,040 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:57,045 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:57,050 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:57,054 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:57,058 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,059 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,059 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:57,060 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:57,063 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:57,067 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,067 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,067 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,067 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,068 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:57,073 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:57,078 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:57,083 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:57,087 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,090 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,090 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:57,092 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:57,095 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:57,099 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,099 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,100 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,100 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:57,106 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:57,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:57,116 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:57,121 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,122 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,123 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:57,124 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:57,127 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:57,131 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,131 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,132 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,132 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:57,138 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:57,143 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:57,148 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:57,154 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,155 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,155 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:57,156 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:57,160 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:57,163 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,163 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,163 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,164 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,164 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:57,169 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:57,173 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:57,177 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:57,181 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,182 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,183 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:57,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:57,187 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:57,191 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,191 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,192 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,192 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,192 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:57,197 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:57,202 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:57,207 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:57,211 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,212 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,212 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:57,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:57,216 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:57,219 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,220 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,220 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,220 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,220 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:57,225 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:57,230 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:57,235 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:57,239 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,240 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,240 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:57,241 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:57,244 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:57,247 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,247 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,248 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,248 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,248 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:57,254 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:57,259 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:57,264 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:57,269 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,270 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,271 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:57,273 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:57,276 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:57,279 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,280 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,280 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,280 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,280 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:57,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:57,291 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:57,296 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:57,302 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,304 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,305 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:57,307 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:57,312 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:57,320 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,321 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,322 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,322 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,322 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:57,338 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:57,346 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:57,352 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:57,357 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,359 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,359 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:57,362 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:57,366 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:57,371 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,371 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,372 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,372 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,372 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:57,379 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:57,384 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:57,390 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:57,397 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,398 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:57,401 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:57,406 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:57,411 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,411 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,412 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,412 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,412 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:57,419 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:57,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:57,430 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:57,436 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,438 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,439 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:57,442 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:57,447 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:57,452 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,452 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,453 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,453 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:57,460 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:57,465 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:57,470 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:57,474 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,475 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,475 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:57,477 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:57,479 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:57,483 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,483 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,483 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,483 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,483 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:57,489 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:57,493 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:57,498 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:57,503 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,504 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,504 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:57,506 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:57,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:57,512 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,512 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,512 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,512 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,512 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:57,518 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:57,523 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:57,528 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:57,532 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,533 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,533 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:57,534 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:57,537 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:57,540 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,541 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,541 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:57,553 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:57,558 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:57,563 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:57,567 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,568 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,568 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:57,570 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:57,572 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:57,576 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,576 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,576 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,577 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,577 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:57,582 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:57,587 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:57,591 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:57,595 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,596 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,596 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:57,598 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:57,601 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:57,604 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,604 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,605 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,605 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,605 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:57,610 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:57,615 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:57,620 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:57,624 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,625 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,626 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:57,627 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:57,630 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:57,633 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,633 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,634 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,634 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,634 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:57,639 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:57,643 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:57,648 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:57,652 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,652 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,653 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:57,654 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:57,657 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:57,660 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,660 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,661 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,661 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,661 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:57,666 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:57,670 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:57,675 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:57,679 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,679 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,680 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:57,681 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:57,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:57,685 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,685 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-09 07:03:57,685 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,685 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-09 07:03:57,685 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:57,691 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:57,695 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:57,699 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:57,703 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-09 07:03:57,704 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-09 07:03:57,704 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:57,706 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:57,706 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:57,707 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,707 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:57,707 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,707 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:57,707 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:57,708 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:57,708 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:57,708 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:57,708 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:57,708 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:57,709 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:57,709 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:57,710 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:57,710 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,710 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:57,710 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,711 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:57,711 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:57,776 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:57,821 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:57,862 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:57,905 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:57,907 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:57,908 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:57,936 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:57,937 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:57,938 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:57,939 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:57,939 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:57,940 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:57,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:57,940 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:57,941 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:57,941 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:57,942 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:57,942 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:57,942 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:57,942 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:57,943 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:57,946 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,946 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:57,947 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,947 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:57,947 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:57,948 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:57,949 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:57,950 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:57,950 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:57,950 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:57,950 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:57,951 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:57,954 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:57,957 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,957 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:57,957 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,958 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:57,958 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:57,963 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:57,967 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:57,971 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:57,975 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:57,975 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:57,976 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:57,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:57,980 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:57,983 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:57,983 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:57,984 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:57,984 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:57,984 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:57,989 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:57,993 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:57,997 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:58,001 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,002 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,002 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:58,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:58,006 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:58,010 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,010 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,010 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,010 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,010 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:58,015 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:58,018 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:58,022 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:58,025 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,026 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,026 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:58,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:58,030 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:58,034 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,034 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,034 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,034 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,034 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:58,038 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:58,042 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:58,045 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:58,047 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,049 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,049 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:58,050 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:58,053 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:58,056 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,057 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,057 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,057 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:58,061 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:58,065 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:58,068 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:58,071 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,072 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,072 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:58,074 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:58,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:58,080 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,080 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,080 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,080 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,080 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:58,085 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:58,088 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:58,091 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:58,094 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,095 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,095 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:58,097 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:58,099 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:58,103 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,103 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,103 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,103 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,103 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:58,107 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:58,111 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:58,114 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:58,117 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,118 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,118 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:58,120 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:58,123 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:58,126 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,126 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,127 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,127 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,127 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:58,134 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:58,138 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:58,143 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:58,147 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,148 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,148 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:58,149 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:58,152 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:58,156 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,156 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,157 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,157 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,157 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:58,162 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:58,167 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:58,172 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:58,176 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,177 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:58,179 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:58,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:58,185 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,185 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,185 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,186 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,186 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:58,191 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:58,195 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:58,199 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:58,203 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,204 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,204 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:58,206 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:58,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:58,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,212 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,212 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,213 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:58,218 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:58,223 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:58,228 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:58,232 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,233 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,233 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:58,235 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:58,238 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:58,242 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,242 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,242 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,242 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,243 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:58,248 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:58,253 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:58,258 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:58,263 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,264 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,264 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:58,266 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:58,268 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:58,272 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,273 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,273 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,273 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,273 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:58,278 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:58,282 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:58,287 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:58,291 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,291 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,292 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:58,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:58,297 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:58,300 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,300 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,300 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,301 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,301 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:58,306 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:58,310 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:58,314 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:58,318 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,320 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,320 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:58,321 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:58,324 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:58,327 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,328 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,328 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,328 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,328 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:58,333 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:58,337 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:58,342 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:58,346 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,346 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,347 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:58,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:58,351 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:58,354 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,354 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,354 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,354 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,354 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:58,360 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:58,364 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:58,369 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:58,373 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,373 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,374 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:58,375 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:58,377 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:58,381 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,381 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,381 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,381 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,381 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:58,386 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:58,391 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:58,395 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:58,399 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,400 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,400 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:58,401 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:58,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:58,407 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,407 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,408 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,408 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,408 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:58,413 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:58,417 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:58,421 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:58,425 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,426 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,426 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:58,427 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:58,430 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:58,433 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,433 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,433 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,433 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,434 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:58,439 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:58,443 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:58,448 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:58,452 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,453 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,453 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:58,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:58,457 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:58,460 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,460 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,461 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,461 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,461 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:58,466 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:58,470 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:58,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:58,479 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,479 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,480 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:58,481 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:58,483 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:58,487 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,487 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,487 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,487 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,487 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:58,492 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:58,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:58,500 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:58,503 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,504 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,504 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:58,506 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:58,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:58,512 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,512 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,512 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,512 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,512 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:58,517 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:58,520 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:58,523 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:58,526 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,527 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,527 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:58,528 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:58,531 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:58,534 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,534 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,535 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,535 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,535 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:58,539 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:58,543 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:58,546 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:58,550 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,551 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,551 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:58,553 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:58,555 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:58,556 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,556 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-09 07:03:58,556 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,556 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-09 07:03:58,557 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:58,561 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:58,564 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:58,567 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:58,570 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-09 07:03:58,571 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-09 07:03:58,571 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:58,572 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:58,573 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:58,574 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,574 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:58,574 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,574 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:58,574 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:58,574 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:58,575 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:58,575 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:58,575 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:58,575 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:58,575 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:58,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:58,576 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:58,577 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,577 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:58,577 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,577 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:58,577 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:58,620 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:58,659 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:58,697 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:58,741 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:58,744 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:58,745 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:58,779 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:58,780 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:58,781 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 07:03:58,782 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:58,782 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 07:03:58,783 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:58,783 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-09 07:03:58,784 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-09 07:03:58,785 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-09 07:03:58,785 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-09 07:03:58,786 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:58,786 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:58,786 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-09 07:03:58,786 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-09 07:03:58,787 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:58,791 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,791 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:58,791 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,791 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:58,791 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-09 07:03:58,794 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-09 07:03:58,795 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-09 07:03:58,796 [forward.py:96 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-09 07:03:58,796 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:58,796 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:58,797 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-09 07:03:58,798 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-09 07:03:58,802 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:58,806 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,806 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,806 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,806 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,806 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-09 07:03:58,811 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-09 07:03:58,815 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-09 07:03:58,821 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-09 07:03:58,826 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:58,828 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:58,828 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-09 07:03:58,830 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-09 07:03:58,834 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:58,839 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,840 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,840 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,840 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,840 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-09 07:03:58,856 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-09 07:03:58,861 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-09 07:03:58,867 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-09 07:03:58,872 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:58,874 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:58,874 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-09 07:03:58,876 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-09 07:03:58,879 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:58,883 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,883 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,884 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,884 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-09 07:03:58,890 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-09 07:03:58,895 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-09 07:03:58,900 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-09 07:03:58,905 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:58,906 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:58,907 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-09 07:03:58,909 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-09 07:03:58,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:58,915 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,916 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,916 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,916 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,916 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-09 07:03:58,922 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-09 07:03:58,931 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-09 07:03:58,936 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-09 07:03:58,942 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:58,943 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:58,943 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-09 07:03:58,945 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-09 07:03:58,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:58,952 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,952 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,953 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,953 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,953 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-09 07:03:58,959 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-09 07:03:58,964 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-09 07:03:58,969 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-09 07:03:58,974 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:58,976 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:58,976 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-09 07:03:58,978 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-09 07:03:58,981 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:58,985 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:58,985 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:58,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:58,986 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:58,986 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-09 07:03:58,991 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-09 07:03:58,997 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-09 07:03:59,002 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-09 07:03:59,008 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,009 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,010 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-09 07:03:59,011 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-09 07:03:59,015 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:59,018 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,019 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,019 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,019 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,019 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-09 07:03:59,025 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-09 07:03:59,030 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-09 07:03:59,036 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-09 07:03:59,040 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,041 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,042 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-09 07:03:59,043 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-09 07:03:59,046 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:59,050 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,050 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,050 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,051 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,051 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-09 07:03:59,057 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-09 07:03:59,062 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-09 07:03:59,067 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-09 07:03:59,072 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,073 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-09 07:03:59,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-09 07:03:59,079 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:59,083 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,083 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,083 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,083 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-09 07:03:59,089 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-09 07:03:59,094 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-09 07:03:59,100 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-09 07:03:59,104 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,106 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,106 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-09 07:03:59,108 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-09 07:03:59,111 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:59,115 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,115 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,115 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,116 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,116 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-09 07:03:59,121 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-09 07:03:59,126 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-09 07:03:59,132 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-09 07:03:59,138 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,139 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,140 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-09 07:03:59,142 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-09 07:03:59,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:59,148 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,149 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,149 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,150 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,150 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-09 07:03:59,160 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-09 07:03:59,165 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-09 07:03:59,173 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-09 07:03:59,178 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,179 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,180 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-09 07:03:59,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-09 07:03:59,185 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:59,189 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,190 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,190 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,190 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,191 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-09 07:03:59,197 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-09 07:03:59,203 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-09 07:03:59,208 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-09 07:03:59,214 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,215 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,216 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-09 07:03:59,218 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-09 07:03:59,221 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:59,225 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,225 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,225 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,226 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,226 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-09 07:03:59,231 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-09 07:03:59,235 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-09 07:03:59,239 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-09 07:03:59,243 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,244 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,245 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-09 07:03:59,246 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-09 07:03:59,249 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:59,253 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,253 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,253 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,254 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,254 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-09 07:03:59,259 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-09 07:03:59,263 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-09 07:03:59,267 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-09 07:03:59,271 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,272 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,272 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-09 07:03:59,273 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-09 07:03:59,276 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:59,280 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,281 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,281 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,282 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,282 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-09 07:03:59,286 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-09 07:03:59,290 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-09 07:03:59,294 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-09 07:03:59,298 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,299 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,300 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-09 07:03:59,301 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-09 07:03:59,304 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:59,307 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,308 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,308 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,308 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,308 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-09 07:03:59,314 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-09 07:03:59,318 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-09 07:03:59,323 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-09 07:03:59,326 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,327 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,327 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-09 07:03:59,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-09 07:03:59,332 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:59,335 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,336 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,336 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,336 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,336 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-09 07:03:59,341 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-09 07:03:59,344 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-09 07:03:59,348 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-09 07:03:59,352 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,353 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,353 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-09 07:03:59,355 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-09 07:03:59,358 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:59,361 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,361 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,361 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,362 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,362 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-09 07:03:59,367 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-09 07:03:59,371 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-09 07:03:59,375 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-09 07:03:59,378 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,380 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,380 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-09 07:03:59,381 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-09 07:03:59,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:59,388 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,388 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,388 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,389 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,389 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-09 07:03:59,394 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-09 07:03:59,398 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-09 07:03:59,402 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-09 07:03:59,405 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,407 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,407 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-09 07:03:59,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-09 07:03:59,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:59,415 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,416 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,416 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,416 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-09 07:03:59,420 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-09 07:03:59,424 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-09 07:03:59,428 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-09 07:03:59,432 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,433 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,433 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-09 07:03:59,434 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-09 07:03:59,437 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:59,441 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,441 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,441 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,442 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,442 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-09 07:03:59,446 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-09 07:03:59,450 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-09 07:03:59,454 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-09 07:03:59,458 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,461 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,461 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-09 07:03:59,463 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-09 07:03:59,466 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:59,469 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,469 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,470 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,470 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,470 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-09 07:03:59,475 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-09 07:03:59,479 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-09 07:03:59,483 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-09 07:03:59,487 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,488 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,488 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-09 07:03:59,489 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-09 07:03:59,492 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:59,496 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,496 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,497 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,497 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,497 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-09 07:03:59,501 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-09 07:03:59,505 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-09 07:03:59,509 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-09 07:03:59,513 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,514 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,514 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-09 07:03:59,516 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-09 07:03:59,519 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:59,520 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,520 [forward.py:83 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-09 07:03:59,520 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,521 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-09 07:03:59,521 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-09 07:03:59,525 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-09 07:03:59,529 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-09 07:03:59,532 [forward.py:96 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-09 07:03:59,536 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-09 07:03:59,537 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-09 07:03:59,537 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-09 07:03:59,538 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-09 07:03:59,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:59,540 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,540 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:59,540 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,540 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:59,540 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-09 07:03:59,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-09 07:03:59,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-09 07:03:59,541 [forward.py:96 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-09 07:03:59,542 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-09 07:03:59,542 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-09 07:03:59,542 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-09 07:03:59,543 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 07:03:59,543 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-09 07:03:59,544 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-09 07:03:59,544 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 07:03:59,544 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-09 07:03:59,544 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 07:03:59,545 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 07:03:59,590 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 07:03:59,630 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 07:03:59,671 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 07:03:59,711 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-09 07:03:59,714 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-09 07:03:59,714 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 07:03:59,746 [test.py:40 in test_hf_gen] INFO - for i in range(10):  # for i in range(10):
        # print(i)
        # print(i)
        # print(i)
        # print(i)
       
2023-10-09 07:03:59,747 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,747 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you aware of your own body? Are you aware of your own mind? Are you aware of your own body and mind? Are you aware of
2023-10-09 07:03:59,748 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,748 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?", "www.dhl.de", "www.dhl.de", "www.dhl.de", "
2023-10-09 07:03:59,748 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,749 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro? The Huawei Mate 60 Pro is a flagship smartphone from Huawei. It is a flagship smartphone that is available in two variants, the Mate 60 Pro
2023-10-09 07:03:59,749 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,749 [test.py:40 in test_hf_gen] INFO - for i in range(10):  # for i in range(10):
        # print(i)
        # print(i)
        # print(i)
        # print(i)
       
2023-10-09 07:03:59,749 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,749 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you aware of your own body? Are you aware of your own mind? Are you aware of your own body and mind? Are you aware of
2023-10-09 07:03:59,750 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,750 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?", "www.dhl.de", "www.dhl.de", "www.dhl.de", "
2023-10-09 07:03:59,750 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,750 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro? The Huawei Mate 60 Pro is a flagship smartphone from Huawei. It is a flagship smartphone that is available in two variants, the Mate 60 Pro
2023-10-09 07:03:59,750 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 07:03:59,871 [forward.py:21 in reset_forward] DEBUG - transformer.word_embeddings from flexgen to old.
2023-10-09 07:03:59,871 [forward.py:21 in reset_forward] DEBUG - transformer.word_embeddings_layernorm from flexgen to old.
2023-10-09 07:03:59,871 [forward.py:21 in reset_forward] DEBUG - transformer.h.0 from flexgen to old.
2023-10-09 07:03:59,871 [forward.py:21 in reset_forward] DEBUG - transformer.h.1 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.2 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.3 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.4 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.5 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.6 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.7 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.8 from flexgen to old.
2023-10-09 07:03:59,872 [forward.py:21 in reset_forward] DEBUG - transformer.h.9 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.10 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.11 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.12 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.13 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.14 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.15 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.16 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.17 from flexgen to old.
2023-10-09 07:03:59,873 [forward.py:21 in reset_forward] DEBUG - transformer.h.18 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.h.19 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.h.20 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.h.21 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.h.22 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.h.23 from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - transformer.ln_f from flexgen to old.
2023-10-09 07:03:59,874 [forward.py:21 in reset_forward] DEBUG - lm_head from flexgen to old.
