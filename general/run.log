2023-10-31 06:13:23,723 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpzf1a4o4b
2023-10-31 06:13:23,723 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpzf1a4o4b/_remote_module_non_scriptable.py
2023-10-31 06:13:23,842 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 06:13:23,896 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:13:25,490 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-31 06:13:25,825 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:13:25,825 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:13:25,825 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:13:25,825 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:13:26,656 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:13:26,738 [model.py:130 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 06:13:26,738 [model.py:69 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 06:13:26,783 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:13:26,867 [model.py:79 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 06:13:26,871 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 06:13:26,871 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-31 06:13:26,872 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-31 06:13:26,873 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-31 06:13:26,873 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-31 06:13:26,874 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-31 06:13:26,875 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-31 06:13:26,876 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-31 06:13:26,877 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-31 06:13:26,878 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-31 06:13:26,879 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-31 06:13:26,880 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-31 06:13:26,881 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-31 06:13:26,882 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-31 06:13:26,883 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:13:26,883 [model.py:261 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:13:26,883 [model.py:267 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 06:13:26,885 [model.py:303 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-31 06:13:26,926 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:13:27,098 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 06:13:27,098 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 06:13:27,098 [model.py:393 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 06:13:27,099 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 06:13:27,100 [model.py:393 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 06:13:27,104 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:27,105 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 06:13:27,106 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:27,107 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 06:13:27,107 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:27,120 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 06:13:27,123 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:27,131 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 06:13:27,133 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:27,140 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 06:13:27,143 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:27,149 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 06:13:27,152 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:27,159 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 06:13:27,161 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:27,167 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 06:13:27,170 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:27,176 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 06:13:27,178 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:27,184 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 06:13:27,186 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:27,192 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 06:13:27,194 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:27,200 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 06:13:27,202 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:27,208 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 06:13:27,210 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:27,216 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 06:13:27,219 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:27,220 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 06:13:27,220 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:27,232 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 06:13:27,237 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 06:13:27,237 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 06:13:27,237 [model.py:401 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 06:13:27,238 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 06:13:27,239 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 06:13:27,239 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 06:13:27,239 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 06:13:27,239 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 06:13:27,239 [model.py:401 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 06:13:27,248 [model.py:516 in init_all_weights] DEBUG - init all weights...
2023-10-31 06:13:27,275 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 06:13:27,275 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 06:13:27,275 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 06:13:27,276 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 06:13:27,277 [wrapper.py:267 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 06:13:27,278 [wrapper.py:267 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 06:13:27,317 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:13:27,455 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:27,455 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-31 06:13:27,455 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,455 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:27,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:27,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:27,457 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-31 06:13:27,457 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,458 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-31 06:13:27,458 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,459 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,460 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:13:27,461 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:27,461 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-31 06:13:27,461 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,461 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:27,462 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:27,465 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:27,465 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-31 06:13:27,465 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,466 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-31 06:13:27,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,469 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:13:27,471 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:27,471 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,471 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,471 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:27,472 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:27,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:27,478 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,478 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,479 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,497 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,497 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:27,498 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,498 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,498 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:27,499 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:27,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:27,506 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,506 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,507 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,521 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,521 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:27,522 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,522 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,522 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:27,523 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:27,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:27,530 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,531 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,531 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,548 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,548 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:27,549 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,549 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,549 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:27,550 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:27,554 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:27,557 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,557 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,558 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,572 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,575 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,575 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:27,575 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,576 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,576 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:27,577 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:27,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:27,586 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,586 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,587 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,604 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,604 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:27,604 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,604 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,604 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:27,606 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:27,610 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:27,614 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,614 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,615 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,619 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,629 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,629 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:27,629 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,629 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,630 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:27,631 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:27,635 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:27,638 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,639 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,640 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,654 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,654 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:27,654 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,654 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,655 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:27,656 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:27,660 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:27,663 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,664 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,665 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,669 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,679 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,679 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:27,679 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,679 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,680 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:27,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:27,684 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:27,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,688 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,689 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,705 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,705 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:27,705 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,705 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:27,707 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:27,710 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:27,714 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,714 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,715 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,723 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,726 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,729 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,729 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:27,729 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,729 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,730 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:27,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:27,734 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:27,738 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,738 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,739 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,753 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,753 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:27,753 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,753 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,754 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:27,755 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:27,756 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:27,760 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,760 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,761 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,765 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,768 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:13:27,775 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:13:27,775 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:27,775 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,775 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,775 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:27,776 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:27,777 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:27,778 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,778 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,778 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:13:27,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:13:27,781 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:13:27,781 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:27,782 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:13:27,782 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,782 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:27,782 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:27,783 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:27,783 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:13:27,783 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,784 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:13:27,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:13:27,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:13:27,817 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:13:27,829 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-31 06:13:27,830 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:27,831 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:27,831 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,831 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:27,834 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:27,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:27,836 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:27,836 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:27,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,839 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,841 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:27,841 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:27,842 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-31 06:13:27,842 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:27,842 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:27,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:27,846 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:27,847 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-31 06:13:27,847 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:27,847 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-31 06:13:27,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,850 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:27,850 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:27,852 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:27,852 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,852 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,852 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:27,853 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:27,856 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:27,860 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,860 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,862 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,870 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,875 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:27,875 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:27,875 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,876 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,876 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:27,877 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:27,881 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:27,885 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,885 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,886 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,891 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,894 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,900 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:27,900 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:27,900 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,900 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,900 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:27,902 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:27,905 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:27,909 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,909 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,911 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,916 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,923 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,925 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:27,925 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:27,926 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,926 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,926 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:27,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:27,931 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:27,935 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,935 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,937 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,941 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,956 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:27,956 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:27,956 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,957 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,957 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:27,958 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:27,962 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:27,965 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,966 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,967 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:27,981 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:27,981 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:27,981 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:27,981 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,982 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:27,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:27,987 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:27,991 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:27,991 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:27,992 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:27,997 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,006 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,006 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:28,006 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,006 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,007 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:28,008 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:28,015 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,016 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,017 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,022 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,029 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,031 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,032 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:28,032 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,032 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,032 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:28,034 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,037 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,041 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,041 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,043 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,047 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,056 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,056 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:28,056 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,057 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,057 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:28,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,062 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,066 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,066 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,067 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,075 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,079 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,081 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,081 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:28,081 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,081 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,082 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:28,083 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:28,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,091 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,091 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,093 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,097 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,104 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,106 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,106 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:28,106 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,106 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,106 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:28,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:28,112 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:28,115 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,116 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,117 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,131 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,131 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:28,131 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,131 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,131 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:28,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:28,134 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:28,137 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,137 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,139 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,143 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:13:28,151 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:13:28,152 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:28,152 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,152 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,152 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:28,153 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:28,154 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:28,154 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,155 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,155 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:28,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,158 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,158 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:28,158 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,158 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,158 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:28,159 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:28,159 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:28,159 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,160 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,160 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:28,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,176 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,192 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:28,194 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:28,194 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:28,194 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,194 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:28,198 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:28,198 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:28,199 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:28,199 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,200 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:28,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,202 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,203 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:28,203 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-31 06:13:28,203 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,203 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:28,203 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:28,207 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:28,208 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-31 06:13:28,208 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,209 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-31 06:13:28,209 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,211 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,212 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,213 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:28,213 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,214 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,214 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:28,214 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:28,218 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:28,221 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,221 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,223 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,228 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,237 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,237 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:28,237 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,238 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,238 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:28,239 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:28,243 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:28,247 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,247 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,249 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,253 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,262 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,262 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:28,263 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,263 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,263 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:28,264 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:28,268 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:28,272 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,272 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,274 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,288 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,289 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:28,289 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,289 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,289 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:28,291 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:28,294 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:28,298 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,298 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,300 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,308 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,331 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,332 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:28,332 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,332 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,332 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:28,334 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:28,338 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:28,342 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,342 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,344 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,352 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,357 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,357 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:28,357 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,358 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,358 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:28,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:28,363 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:28,367 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,367 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,369 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,380 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,382 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,383 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:28,383 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,383 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,383 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:28,384 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,388 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:28,392 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,392 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,394 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,399 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,402 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,408 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,408 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:28,408 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,409 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,409 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:28,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,414 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,418 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,418 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,420 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,428 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,433 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,433 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:28,434 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,434 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,434 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:28,435 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,439 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,443 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,443 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,445 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,483 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,504 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,505 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:28,505 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,505 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,505 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:28,507 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:28,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,514 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,515 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,516 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,524 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,530 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,530 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:28,530 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,530 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,530 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:28,531 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:28,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:28,539 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,541 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,558 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,564 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,564 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:28,564 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,564 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,565 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:28,568 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:28,570 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:28,575 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,576 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,578 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:13:28,638 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:13:28,638 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:28,638 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,638 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,639 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:28,640 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:28,640 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:28,641 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,641 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,642 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:28,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,645 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,645 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:28,645 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,645 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,645 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:28,646 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:28,646 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:28,647 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,647 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,647 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:28,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,667 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:28,686 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:28,687 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:28,687 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:28,688 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,688 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:28,691 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:28,692 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:28,692 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:28,693 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,693 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:28,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,695 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,696 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:28,696 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-31 06:13:28,696 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:28,696 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:28,696 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:28,700 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:28,701 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-31 06:13:28,701 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:28,701 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-31 06:13:28,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:28,704 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:28,705 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:28,706 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,706 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,706 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:28,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:28,710 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:28,714 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,714 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,716 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,721 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,730 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:28,731 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,731 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:28,732 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:28,737 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:28,740 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,741 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,742 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,756 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,756 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:28,756 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,756 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,756 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:28,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:28,762 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:28,766 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,766 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,768 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,798 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,798 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:28,798 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,799 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,799 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:28,800 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:28,805 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:28,808 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,808 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,811 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,819 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,825 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,825 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:28,825 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,825 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,825 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:28,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:28,831 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:28,835 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,835 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,843 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,858 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,859 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:28,859 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,859 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,860 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:28,861 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:28,867 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:28,871 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,871 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,873 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,887 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,889 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,889 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:28,889 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,890 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,890 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:28,891 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,895 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:28,899 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,900 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,902 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,906 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,910 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,914 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,916 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,916 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:28,916 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,917 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,917 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:28,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:28,927 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,927 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,929 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,943 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,943 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:28,943 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,943 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,943 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:28,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,949 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:28,953 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,954 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,955 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,967 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,970 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,970 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:28,970 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,970 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,970 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:28,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:28,976 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:28,981 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:28,981 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,983 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:28,987 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,990 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:28,997 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:28,997 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:28,997 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:28,997 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:28,997 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:28,999 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,003 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:29,007 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,007 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,009 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,014 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,023 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:29,023 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:29,023 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,023 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,023 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:29,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,026 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,030 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,030 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,032 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,036 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,043 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:13:29,045 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:13:29,045 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:29,046 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,046 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,046 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:29,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,048 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,048 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,049 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,050 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,051 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,052 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:29,052 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,052 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,052 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:29,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,053 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,054 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,098 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:29,100 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:29,100 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:29,100 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,100 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:29,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,105 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:29,105 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,106 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:29,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,108 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,109 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:29,109 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-31 06:13:29,109 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,109 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:29,110 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,114 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,114 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-31 06:13:29,115 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,115 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-31 06:13:29,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,118 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,119 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:29,120 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,120 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,120 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:29,120 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,130 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,135 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,143 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,146 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,146 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:29,146 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,146 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,146 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:29,148 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,152 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,158 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,166 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,170 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,172 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,172 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:29,173 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,173 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,173 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:29,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:29,178 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,182 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,182 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,184 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,198 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,198 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:29,198 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,198 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,198 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:29,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:29,204 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:29,207 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,208 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,209 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,213 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,222 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,224 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,224 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:29,224 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,224 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,224 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:29,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:29,230 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:29,234 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,234 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,236 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,259 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,263 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,265 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,265 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:29,265 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,265 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,265 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:29,267 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:29,271 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:29,274 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,274 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,276 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,284 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,288 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,290 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,290 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:29,290 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,291 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,291 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:29,292 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:29,296 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:29,300 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,300 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,302 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,315 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,316 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:29,316 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,316 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,316 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:29,318 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:29,321 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:29,325 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,326 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,327 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,344 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,346 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,346 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:29,346 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,347 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,347 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:29,348 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:29,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:29,356 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,356 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,358 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,366 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,372 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,372 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:29,372 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,372 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,372 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:29,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:29,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:29,382 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,382 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,388 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,398 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,398 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:29,399 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,399 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,399 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:29,400 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,404 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:29,408 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,408 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,410 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,414 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,423 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,424 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:29,424 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,424 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,424 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:29,425 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,426 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,430 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,430 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,432 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:13:29,446 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:13:29,446 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:29,446 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,446 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,446 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:29,448 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,448 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,449 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,449 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,449 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,451 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,452 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,452 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:29,452 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,452 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,453 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:29,453 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,453 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,454 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,454 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,454 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,486 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:29,488 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:29,488 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:29,488 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,488 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:29,491 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,492 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,492 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:29,492 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,493 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:29,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,496 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,496 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:29,496 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-31 06:13:29,497 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,497 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:29,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,501 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-31 06:13:29,501 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,502 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-31 06:13:29,503 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,503 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,504 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,505 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,506 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:29,506 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,507 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,507 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:29,507 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,511 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,514 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,515 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,516 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,557 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,557 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:29,557 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,557 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,558 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:29,559 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,567 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,567 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,569 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,573 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,580 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,583 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,583 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:29,583 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,583 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,583 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:29,585 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:29,589 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,593 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,593 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,595 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,604 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,610 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,610 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:29,610 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,610 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,610 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:29,612 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:29,616 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:29,620 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,620 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,622 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,633 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,636 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,636 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:29,636 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,636 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,637 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:29,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:29,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:29,646 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,646 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,648 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,652 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,660 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,662 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,663 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:29,663 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,663 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,663 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:29,664 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:29,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:29,672 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,672 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,674 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,679 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,682 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,686 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,688 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,688 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:29,689 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,689 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,689 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:29,690 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:29,694 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:29,698 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,698 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,699 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,714 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,715 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:29,715 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,715 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,715 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:29,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:29,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:29,724 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,724 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,726 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,730 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,734 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,753 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,756 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,756 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:29,756 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,756 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,756 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:29,758 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:29,762 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:29,765 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,766 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,767 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,775 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,781 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,781 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:29,781 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,781 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,782 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:29,783 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:29,787 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:29,791 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,791 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,793 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,797 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,807 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,807 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:29,807 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,807 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,807 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:29,808 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,813 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:29,816 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,816 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,818 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,835 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,835 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:29,836 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,836 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,836 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:29,837 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,838 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:29,842 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,842 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,843 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:13:29,857 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:13:29,857 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:29,857 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,857 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,857 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:29,859 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,859 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:29,860 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,860 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,860 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,861 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,863 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,863 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:29,863 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,863 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,863 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:29,864 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,864 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:29,865 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,865 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,865 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:29,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,888 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:29,896 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:29,897 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:29,897 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:29,897 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,897 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:29,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:29,902 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:29,902 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,903 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:29,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,904 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,905 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,905 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:29,906 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-31 06:13:29,906 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:29,906 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:29,906 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,910 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:29,911 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-31 06:13:29,911 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:29,911 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-31 06:13:29,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,913 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:29,914 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:29,915 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:29,916 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,916 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,916 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:29,916 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,920 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:29,923 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,923 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,925 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,939 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:29,939 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:29,939 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,940 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,940 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:29,941 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,945 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:29,948 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,948 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,950 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,966 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:29,966 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:29,966 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,967 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,967 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:29,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:29,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:29,976 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:29,976 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,978 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:29,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:29,991 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:29,991 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:29,992 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:29,992 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:29,992 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:29,993 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:29,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:30,001 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,001 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,003 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,049 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,051 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,052 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:30,052 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,052 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,052 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:30,054 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:30,062 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,062 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,064 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,068 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,075 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,077 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,078 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:30,078 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,078 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,078 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:30,080 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,083 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,087 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,087 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,089 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,097 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,102 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,102 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:30,103 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,103 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,103 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:30,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,115 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,115 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,118 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,143 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,147 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,148 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:30,148 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,148 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,148 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:30,150 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,162 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,164 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,170 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,181 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,186 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,186 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:30,186 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,186 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,187 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:30,188 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:30,192 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,195 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,196 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,197 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,202 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,209 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,211 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,211 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:30,211 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,212 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,212 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:30,213 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:30,217 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:30,220 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,221 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,222 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,227 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,230 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,238 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,240 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:30,240 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,240 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,240 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:30,241 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:30,245 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:30,249 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,249 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,251 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,270 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,282 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,282 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:30,283 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,283 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,283 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:30,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:30,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:30,289 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,289 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,291 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,315 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,320 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:13:30,322 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:13:30,323 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:30,323 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,323 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,323 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:30,324 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:30,325 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:30,326 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,326 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,326 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:30,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,328 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,329 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,329 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,329 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:30,329 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,330 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,330 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:30,330 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:30,330 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:30,331 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,331 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,331 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:30,339 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,362 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:30,363 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:30,363 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:30,363 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,364 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:30,367 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:30,367 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:30,368 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:30,368 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,369 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:30,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,371 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,371 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,371 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:30,371 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-31 06:13:30,372 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,372 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:30,372 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:30,376 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:30,376 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-31 06:13:30,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,377 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-31 06:13:30,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,379 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,380 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,381 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:30,381 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,381 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,382 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:30,382 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:30,386 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:30,389 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,389 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,395 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,399 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,405 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,405 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:30,405 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,405 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,405 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:30,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:30,411 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:30,414 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,414 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,416 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,428 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,430 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,431 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:30,431 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,431 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,431 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:30,433 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:30,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:30,440 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,440 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,442 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,446 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,458 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,460 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,460 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:30,460 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,461 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,461 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:30,462 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:30,466 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:30,470 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,470 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,472 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,483 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,494 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,495 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:30,495 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,495 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,495 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:30,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:30,505 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,505 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,507 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,521 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,521 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:30,521 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,521 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,522 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:30,523 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,531 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,531 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,533 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,537 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,541 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,545 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,547 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,547 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:30,547 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,547 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,548 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:30,549 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,557 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,557 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,559 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,586 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,593 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,599 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,599 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:30,599 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,599 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,599 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:30,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,605 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,608 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,609 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,610 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,615 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,619 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,622 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,625 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,625 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:30,625 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,625 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,625 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:30,627 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:30,631 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,634 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,635 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,636 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,645 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,651 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,651 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:30,651 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,651 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,651 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:30,653 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:30,657 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:30,660 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,661 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,663 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,667 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,670 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,676 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,676 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:30,676 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,676 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,677 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:30,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:30,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:30,686 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,686 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,688 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,702 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,702 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:30,702 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,702 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,702 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:30,704 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:30,704 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:30,708 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,709 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,710 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,715 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:13:30,724 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:13:30,724 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:30,725 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,725 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,725 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:30,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:30,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:30,727 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,727 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,728 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:30,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,730 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,730 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,730 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:30,731 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,731 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:30,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:30,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:30,732 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,732 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,732 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:30,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,748 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:30,763 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:30,765 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:30,765 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:30,765 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,765 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:30,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:30,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:30,769 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:30,769 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,770 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:30,771 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,771 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,773 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,773 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:30,773 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-31 06:13:30,773 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:30,773 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:30,773 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:30,777 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:30,778 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-31 06:13:30,778 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:30,779 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-31 06:13:30,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:30,781 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:30,783 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:30,783 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,783 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,783 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:30,783 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:30,787 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:30,791 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,791 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,793 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,797 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,807 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,807 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:30,807 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,807 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,807 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:30,809 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:30,812 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:30,816 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,816 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,818 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,832 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,832 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:30,832 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,832 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,832 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:30,834 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:30,838 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:30,841 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,841 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,843 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,855 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,857 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,857 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:30,857 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,857 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,857 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:30,859 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:30,863 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:30,866 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,867 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,868 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,876 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,882 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,882 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:30,882 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,883 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,883 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:30,884 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,888 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:30,892 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,892 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,894 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,907 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,908 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:30,908 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,908 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,908 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:30,910 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,913 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:30,917 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,919 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,923 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,933 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,933 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:30,933 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,933 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,933 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:30,935 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:30,942 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,943 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,944 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,949 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,953 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,959 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,962 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,962 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:30,962 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,962 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,962 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:30,964 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:30,972 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,972 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,974 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:30,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,983 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:30,988 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:30,988 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:30,988 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:30,989 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:30,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:30,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:30,994 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:30,998 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:30,998 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,000 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,014 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:31,015 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:31,015 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,015 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,015 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:31,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:31,024 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,024 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,026 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,037 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,039 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:31,040 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:31,040 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,040 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,040 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:31,041 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,045 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,049 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,049 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,051 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,065 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:31,065 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:31,065 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,066 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,066 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:31,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,071 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,072 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,073 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,078 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:13:31,087 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:13:31,087 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:31,087 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,087 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,087 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:31,089 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:31,089 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,090 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,090 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,090 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:31,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,093 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,093 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:31,093 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,093 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,094 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:31,094 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:31,094 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:31,095 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,095 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,095 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:31,104 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,127 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:31,128 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:31,128 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:31,128 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,129 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:31,132 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:31,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:31,133 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:31,134 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,134 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:31,135 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,135 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,137 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,137 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:31,137 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-31 06:13:31,137 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,138 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:31,138 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:31,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:31,143 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-31 06:13:31,143 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,143 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-31 06:13:31,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,146 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,148 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:31,148 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,148 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,148 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:31,148 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:31,152 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:31,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,158 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,176 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,180 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,183 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,183 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:31,183 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,183 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,183 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:31,185 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:31,189 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:31,193 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,193 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,195 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,209 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,209 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:31,209 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,209 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,210 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:31,211 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:31,215 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:31,218 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,219 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,220 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,228 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,234 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,234 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:31,234 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,234 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,235 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:31,236 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:31,240 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:31,244 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,244 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,246 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,257 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,260 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,260 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:31,260 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,260 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,260 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:31,262 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:31,266 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:31,270 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,270 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,272 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,294 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,294 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:31,295 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,295 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,295 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:31,296 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:31,300 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:31,304 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,304 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,306 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,314 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,320 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,320 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:31,320 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,320 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,320 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:31,322 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:31,326 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:31,329 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,330 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,331 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,340 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,345 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,346 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:31,346 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,346 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,346 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:31,348 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:31,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:31,355 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,355 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,357 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,362 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,366 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,372 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,372 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:31,372 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,373 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,373 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:31,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:31,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:31,382 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,382 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,398 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,401 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,401 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:31,401 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,402 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,402 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:31,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:31,411 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,412 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,413 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,441 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,444 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,445 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:31,445 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,445 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,445 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:31,446 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,450 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,457 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,457 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,459 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,474 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,474 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:31,474 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,474 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,474 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:31,476 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,476 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,480 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,480 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,482 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,486 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,494 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:13:31,496 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:13:31,496 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:31,496 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,496 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,496 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:31,498 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:31,498 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,499 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,499 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,499 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:31,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,501 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,502 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,502 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,503 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:31,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,503 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,503 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:31,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:31,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:31,504 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,504 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,505 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:31,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:31,535 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:31,536 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:31,537 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:31,537 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,537 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:31,541 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:31,541 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:31,542 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:31,542 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,542 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:31,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,544 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,545 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,545 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,545 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:31,546 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-31 06:13:31,546 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,546 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:31,546 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:31,550 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:31,550 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-31 06:13:31,551 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,551 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-31 06:13:31,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:31,554 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:31,555 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:31,555 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,556 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,556 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:31,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:31,560 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:31,564 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,564 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,566 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,570 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,580 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,580 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:31,580 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,580 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,580 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:31,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:31,586 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:31,589 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,589 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,591 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,603 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,605 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,605 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:31,606 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,606 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,606 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:31,607 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:31,611 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:31,615 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,615 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,617 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,621 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,628 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,630 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,631 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:31,631 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,631 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,631 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:31,633 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:31,637 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:31,641 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,641 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,647 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,654 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,656 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,656 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:31,657 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,657 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,657 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:31,658 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:31,663 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:31,666 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,666 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,668 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,706 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,771 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,772 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:31,772 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,772 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,772 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:31,774 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:31,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:31,783 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,783 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,785 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,796 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,806 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,806 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:31,806 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,806 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,806 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:31,808 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:31,812 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:31,816 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,816 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,818 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,823 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,831 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,833 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,833 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:31,833 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,833 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,833 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:31,835 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:31,839 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:31,843 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,843 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,845 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,853 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,859 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,859 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:31,860 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,860 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,860 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:31,861 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:31,865 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:31,869 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,869 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,871 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,875 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,885 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,885 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:31,885 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,885 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,885 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:31,887 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,891 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:31,895 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,895 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,897 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,925 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,937 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:31,937 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,937 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,937 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:31,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,943 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:31,947 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,947 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,950 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,965 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,972 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,972 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:31,972 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,972 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,973 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:31,974 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,975 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:31,979 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,979 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:31,981 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:31,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:13:31,995 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:13:31,996 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:31,996 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:31,996 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:31,996 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:31,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:31,998 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:31,998 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:31,999 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:31,999 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,002 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,002 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:32,002 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,002 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,002 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:32,003 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,003 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:32,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,004 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,004 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,028 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,036 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:32,037 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:32,037 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:32,037 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,037 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:32,041 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,042 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,042 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:32,042 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,043 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:32,043 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,045 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,045 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:32,046 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-31 06:13:32,046 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,046 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:32,046 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,050 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,051 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-31 06:13:32,051 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,051 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-31 06:13:32,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,054 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,056 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:32,056 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,056 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,056 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:32,056 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,064 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,064 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,066 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,074 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,078 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,081 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,081 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:32,081 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,081 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,081 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:32,083 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,091 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,091 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,093 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,098 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,102 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,108 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,108 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:32,109 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,109 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,109 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:32,111 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,115 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,119 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,119 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,121 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,130 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,136 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,136 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:32,136 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,136 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,136 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:32,138 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,146 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,146 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,148 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,204 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,207 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,207 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:32,207 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,208 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,208 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:32,209 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:32,214 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,217 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,218 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,219 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,224 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,228 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,234 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,234 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:32,234 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,234 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,235 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:32,237 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:32,241 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:32,244 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,244 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,246 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,262 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,262 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:32,263 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,263 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,263 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:32,264 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:32,268 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:32,272 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,272 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,274 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,288 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,288 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:32,288 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,289 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,289 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:32,290 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:32,294 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:32,297 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,298 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,299 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,321 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,328 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,331 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,331 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:32,331 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,331 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,331 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:32,332 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:32,336 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:32,340 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,340 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,342 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,346 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,350 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,355 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,356 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:32,356 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,356 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,356 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:32,358 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:32,361 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:32,365 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,365 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,367 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,371 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,380 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,381 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:32,381 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,381 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,381 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:32,382 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:32,386 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:32,389 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,390 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,399 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,405 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,405 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:32,405 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,406 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,406 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:32,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:32,408 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:32,411 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,412 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,413 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:13:32,427 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:13:32,427 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:32,427 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,428 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,428 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:32,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:32,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:32,430 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,430 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,431 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,433 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,433 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,433 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:32,433 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,434 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,434 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:32,434 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,435 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:32,435 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,435 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,436 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,451 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,469 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:32,471 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:32,471 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:32,471 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,471 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:32,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,475 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:32,475 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,476 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:32,477 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,477 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,479 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,479 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:32,479 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-31 06:13:32,479 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,479 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:32,479 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,483 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,484 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-31 06:13:32,484 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,484 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-31 06:13:32,485 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,486 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,487 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,487 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,489 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:32,489 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,489 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,489 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:32,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,493 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,497 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,497 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,499 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,504 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,507 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,513 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,513 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:32,514 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,514 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,514 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:32,515 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,519 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,523 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,523 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,525 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,542 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,542 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:32,542 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,543 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,543 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:32,544 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,548 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,551 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,552 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,554 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,558 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,567 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,567 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:32,568 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,568 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,568 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:32,569 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,573 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,577 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,577 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,579 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,602 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,609 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,611 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,611 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:32,611 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,612 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,612 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:32,613 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:32,617 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,621 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,621 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,623 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,636 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,636 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:32,637 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,637 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,637 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:32,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:32,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:32,645 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,646 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,647 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,652 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,655 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,661 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,661 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:32,662 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,662 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,662 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:32,663 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:32,667 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:32,671 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,671 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,677 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,687 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,687 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:32,687 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,687 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,687 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:32,689 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:32,693 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:32,696 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,697 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,698 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,710 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,713 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,713 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:32,713 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,713 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,713 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:32,715 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:32,719 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:32,723 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,723 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,725 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,736 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,739 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,739 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:32,739 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,740 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,740 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:32,741 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:32,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:32,749 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,750 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,751 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,766 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,766 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:32,766 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,766 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,766 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:32,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:32,771 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:32,775 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,775 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,777 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,792 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,800 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,802 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,802 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:32,803 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,803 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,803 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:32,805 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:32,805 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:32,809 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,809 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,811 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,818 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:13:32,824 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:13:32,824 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:32,824 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,825 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,825 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:32,826 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:32,826 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:32,827 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,827 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,828 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,831 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,831 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:32,831 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,831 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,831 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:32,832 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,832 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:32,832 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,832 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,833 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:32,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:32,865 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:32,866 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:32,866 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:32,866 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,867 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:32,870 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,870 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:32,871 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:32,871 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,872 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:32,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,874 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,874 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:32,875 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-31 06:13:32,875 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:32,875 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:32,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,879 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:32,879 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-31 06:13:32,879 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:32,880 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-31 06:13:32,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:32,883 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:32,884 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:32,884 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,885 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,885 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:32,885 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,889 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:32,892 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,892 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,894 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,908 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:32,908 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:32,908 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,908 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,908 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:32,910 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,913 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:32,917 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,919 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,933 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:32,933 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:32,933 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,933 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,933 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:32,935 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,938 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:32,942 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,942 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,944 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,961 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:32,961 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:32,961 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,962 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,962 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:32,963 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,967 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:32,971 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,971 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,973 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:32,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,981 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:32,986 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:32,987 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:32,987 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:32,987 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,987 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:32,989 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:32,992 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:32,996 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:32,996 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:32,998 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,012 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,012 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:33,012 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,013 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,013 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:33,014 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,018 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:33,021 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,021 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,023 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,028 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,037 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,037 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:33,037 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,037 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,037 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:33,039 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,046 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,046 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,048 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,068 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,075 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,077 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,077 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:33,078 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,078 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,078 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:33,079 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,083 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,087 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,087 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,088 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,097 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,100 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,102 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,103 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:33,103 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,103 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,103 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:33,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,112 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,112 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,114 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,118 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,127 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,127 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:33,128 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,128 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,128 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:33,129 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,136 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,136 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,138 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,143 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,152 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,152 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:33,152 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,153 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,153 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:33,154 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,161 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,163 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,167 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,176 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,177 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:33,177 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,177 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,177 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:33,178 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,179 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,183 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,183 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,185 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:13:33,199 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:13:33,199 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:33,199 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,199 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,199 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:33,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,201 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,202 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,202 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,204 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,205 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,205 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:33,205 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,205 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,205 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:33,206 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:33,206 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,206 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,206 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,207 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,238 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:33,239 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:33,240 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:33,240 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,240 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:33,243 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:33,244 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:33,244 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:33,244 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,245 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:33,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,247 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,248 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:33,248 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-31 06:13:33,248 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,248 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:33,249 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:33,252 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:33,253 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-31 06:13:33,253 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,254 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-31 06:13:33,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,256 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,258 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:33,258 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,258 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,258 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:33,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:33,262 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:33,266 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,266 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,268 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,273 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,297 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,299 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,300 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:33,300 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,300 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,300 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:33,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:33,305 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:33,309 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,309 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,311 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,315 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,319 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,322 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,325 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,325 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:33,325 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,325 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,325 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:33,327 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:33,330 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:33,334 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,335 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,336 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,341 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,345 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,350 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,350 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:33,351 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,351 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,351 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:33,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:33,356 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:33,359 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,359 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,361 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,375 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,375 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:33,375 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,375 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,375 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:33,377 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:33,381 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:33,384 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,384 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,386 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,391 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,395 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,398 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,401 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,401 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:33,401 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,401 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,401 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:33,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,407 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:33,410 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,411 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,412 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,417 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,427 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,427 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:33,427 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,428 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,428 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:33,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,433 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,437 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,437 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,438 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,452 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,452 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:33,452 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,453 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,453 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:33,454 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,462 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,462 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,464 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,478 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,478 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:33,478 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,478 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,478 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:33,480 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,483 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,487 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,487 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,489 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,501 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,503 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,503 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:33,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,503 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,504 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:33,505 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,512 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,512 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,514 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,518 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,525 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,527 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,528 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:33,528 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,528 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,528 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:33,529 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,533 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,537 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,537 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,539 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,572 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,579 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,579 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:33,580 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,580 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,580 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:33,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,585 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,585 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,587 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,599 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:13:33,601 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:13:33,601 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:33,602 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,602 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,602 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:33,603 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,603 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,605 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,607 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,608 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:33,608 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,608 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,608 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:33,608 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:33,608 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,609 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,609 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,610 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,619 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:33,641 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:33,643 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:33,643 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:33,643 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,643 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:33,646 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:33,647 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:33,647 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:33,648 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,648 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:33,649 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,649 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,650 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,651 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,651 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:33,651 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-31 06:13:33,651 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,651 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:33,652 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:33,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:33,656 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-31 06:13:33,656 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,656 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-31 06:13:33,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,658 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,659 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,661 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:33,661 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,661 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,661 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:33,662 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:33,665 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:33,668 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,669 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,670 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,675 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,679 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,682 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,685 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,685 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:33,685 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,685 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,685 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:33,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:33,690 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:33,694 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,694 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,696 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,710 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,710 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:33,710 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,710 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,710 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:33,712 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:33,715 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:33,718 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,719 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,720 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,725 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,735 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,735 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:33,736 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,736 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,736 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:33,737 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:33,741 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:33,744 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,745 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,746 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,768 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,772 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,778 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,778 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:33,778 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,779 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,779 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:33,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:33,784 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:33,787 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,788 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,789 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,794 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,798 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,803 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,803 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:33,803 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,804 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,804 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:33,805 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,809 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:33,812 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,813 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,814 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,818 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,828 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,828 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:33,828 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,828 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,828 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:33,830 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,834 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:33,837 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,837 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,839 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,847 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,853 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,853 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:33,853 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,853 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,853 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:33,855 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,858 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:33,862 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,862 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,863 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,868 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,875 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,877 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,877 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:33,878 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,878 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,878 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:33,879 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,883 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:33,886 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,887 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,888 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,897 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,900 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,902 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,902 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:33,903 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,903 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,903 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:33,904 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:33,911 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,912 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,913 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,918 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,925 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,927 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,927 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:33,927 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,928 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,928 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:33,929 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,933 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:33,936 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,936 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,938 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,952 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,952 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:33,953 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,953 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,953 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:33,954 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:33,958 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,958 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:33,960 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:33,968 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:13:33,978 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:13:33,978 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:33,979 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,979 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:33,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:33,981 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,981 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,982 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,983 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:33,984 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:33,985 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:33,985 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:33,985 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:33,985 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:33,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:33,986 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:33,986 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:33,986 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:33,987 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:33,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,002 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,034 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:34,038 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:34,038 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:34,039 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,039 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:34,042 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,042 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:34,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:34,043 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,043 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:34,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,046 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,046 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:34,046 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-31 06:13:34,046 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,047 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:34,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,050 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,051 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-31 06:13:34,051 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,051 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-31 06:13:34,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,054 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,056 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:34,056 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,056 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,056 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:34,056 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,064 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,064 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,066 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,070 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,074 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,078 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,080 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,080 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:34,080 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,080 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,080 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:34,082 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,086 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,089 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,089 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,091 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,099 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,103 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,105 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,105 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:34,106 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,106 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,106 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:34,107 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,111 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,114 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,115 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,116 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,124 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,128 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,130 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,130 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:34,131 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,131 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,131 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:34,132 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,136 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,139 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,140 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,141 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,155 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,155 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:34,155 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,156 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,156 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:34,157 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:34,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,164 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,164 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,166 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,180 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,180 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:34,180 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,181 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,181 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:34,182 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:34,186 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:34,190 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,190 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,192 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,204 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,206 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,206 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:34,206 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,206 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,207 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:34,208 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:34,212 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:34,215 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,215 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,217 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,222 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,229 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,231 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,231 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:34,231 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,232 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,232 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:34,233 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:34,237 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:34,241 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,241 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,243 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,264 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,266 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,266 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:34,266 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,266 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,267 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:34,268 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:34,272 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:34,275 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,275 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,277 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,293 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,294 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:34,294 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,294 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,294 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:34,296 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:34,300 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:34,303 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,303 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,305 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,317 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,319 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,319 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:34,319 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,319 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,319 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:34,321 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:34,325 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:34,328 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,328 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,330 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,338 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,342 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,344 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,344 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:34,344 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,345 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,345 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:34,346 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:34,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:34,350 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,351 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,352 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,357 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:13:34,366 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:13:34,366 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:34,366 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,366 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,366 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:34,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:34,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:34,369 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,369 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,370 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:34,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,371 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,372 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,372 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,373 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:34,373 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,373 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,373 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:34,373 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:34,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:34,374 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,374 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,375 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:34,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,407 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:34,409 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:34,409 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:34,409 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,409 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:34,413 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,413 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:34,414 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:34,414 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,414 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:34,415 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,417 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,417 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:34,417 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-31 06:13:34,417 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,417 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:34,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,422 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,422 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-31 06:13:34,422 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,423 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-31 06:13:34,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,426 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,427 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:34,427 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,427 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,428 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:34,428 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,435 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,435 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,437 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,441 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,445 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,449 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,451 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,451 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:34,451 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,451 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,452 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:34,453 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,457 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,460 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,460 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,462 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,478 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,479 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:34,479 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,479 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,479 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:34,481 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,488 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,488 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,490 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,498 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,526 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,528 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,529 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:34,529 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,529 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,529 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:34,531 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,534 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,538 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,538 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,540 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,545 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,552 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,554 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,554 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:34,554 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,555 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,555 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:34,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:34,560 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,564 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,564 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,566 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,579 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,581 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,581 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:34,581 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,582 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,582 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:34,583 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:34,587 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:34,591 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,591 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,593 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,607 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,607 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:34,607 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,607 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,607 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:34,609 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:34,613 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:34,616 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,617 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,618 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,623 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,632 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,632 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:34,632 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,633 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,633 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:34,634 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:34,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:34,641 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,641 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,655 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,657 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,657 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:34,657 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,657 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,658 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:34,659 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:34,663 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:34,667 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,667 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,669 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,678 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,684 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,684 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:34,684 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,684 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,684 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:34,686 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:34,690 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:34,693 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,694 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,696 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,711 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,711 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:34,711 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,711 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,711 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:34,712 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:34,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:34,720 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,720 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,722 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,739 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,749 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,749 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:34,749 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,749 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,749 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:34,751 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:34,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:34,756 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,756 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,758 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,767 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,770 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:13:34,773 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:13:34,773 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:34,773 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,773 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,773 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:34,774 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:34,775 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:34,776 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,776 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,776 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:34,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,778 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,779 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,779 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:34,779 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,780 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,780 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:34,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:34,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:34,781 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,781 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,781 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:34,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,797 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:34,812 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:34,814 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:34,814 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:34,814 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,814 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:34,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,818 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:34,819 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:34,819 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,820 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:34,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,821 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,822 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,823 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:34,823 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-31 06:13:34,823 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:34,823 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:34,823 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,828 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:34,828 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-31 06:13:34,828 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:34,829 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-31 06:13:34,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,831 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:34,831 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:34,833 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:34,833 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,833 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,833 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:34,834 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,838 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:34,841 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,841 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,843 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,856 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,859 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:34,859 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:34,859 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,860 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,860 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:34,863 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,869 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:34,875 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,876 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,879 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,895 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,899 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,904 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,906 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:34,907 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:34,907 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,907 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,907 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:34,909 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,912 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:34,916 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,919 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,928 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,932 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:34,936 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:34,937 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,937 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,937 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:34,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,943 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:34,947 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,948 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,950 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,958 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,963 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,965 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:34,965 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:34,965 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:34,966 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,966 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:34,967 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:34,971 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:34,975 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:34,975 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:34,977 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:34,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:34,991 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,021 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,035 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,035 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:35,035 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,035 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,035 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:35,037 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,041 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:35,045 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,045 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,047 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,068 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,069 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:35,069 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,069 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,069 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:35,070 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:35,075 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,078 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,078 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,080 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,085 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,089 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,093 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,096 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,096 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:35,096 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,096 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,096 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:35,098 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:35,102 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:35,106 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,106 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,108 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,117 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,124 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,124 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:35,124 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,124 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,125 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:35,126 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:35,130 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:35,134 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,134 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,136 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,141 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,146 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,153 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,153 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:35,154 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,154 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,154 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:35,156 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:35,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:35,165 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,165 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,167 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,193 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,195 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,196 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:35,196 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,196 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,196 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:35,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:35,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:35,206 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,206 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,208 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,245 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,245 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:35,245 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,245 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,245 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:35,247 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:35,248 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:35,251 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,252 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,253 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:13:35,281 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:13:35,281 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:35,281 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,282 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,282 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:35,283 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:35,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:35,284 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,284 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,285 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:35,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,288 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,288 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:35,288 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,288 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,288 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:35,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:35,289 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:35,290 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,290 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,290 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:35,298 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,321 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:35,323 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:35,323 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:35,323 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,323 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:35,327 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:35,328 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:35,328 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:35,328 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,329 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:35,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,333 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,334 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:35,334 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-31 06:13:35,334 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,334 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:35,335 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:35,339 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:35,339 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-31 06:13:35,340 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,341 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-31 06:13:35,342 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,344 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,345 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,347 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:35,347 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,348 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,348 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:35,348 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:35,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:35,357 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,357 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,359 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,365 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,376 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,376 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:35,376 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,376 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,376 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:35,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:35,382 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:35,386 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,386 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,388 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,393 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,398 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,404 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,404 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:35,404 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,404 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,405 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:35,406 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:35,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:35,413 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,414 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,415 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,429 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,432 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,432 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:35,432 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,432 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,432 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:35,434 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:35,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:35,442 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,442 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,444 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,449 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,457 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,459 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,459 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:35,459 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,460 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,460 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:35,461 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:35,465 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:35,469 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,469 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,471 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,507 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,514 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,514 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:35,514 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,514 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,514 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:35,516 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,521 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:35,525 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,525 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,527 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,542 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,543 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:35,543 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,543 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,543 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:35,544 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:35,549 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,553 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,553 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,555 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,560 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,570 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,570 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:35,571 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,571 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,571 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:35,572 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:35,576 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:35,580 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,580 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,582 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,591 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,597 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,597 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:35,597 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,598 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,598 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:35,599 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:35,603 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:35,607 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,607 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,609 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,622 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,624 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,624 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:35,624 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,625 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,625 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:35,626 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:35,630 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:35,634 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,634 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,636 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,646 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,653 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,654 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:35,654 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,654 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,654 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:35,655 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:35,660 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:35,664 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,665 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,667 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,684 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,684 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:35,684 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,684 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,685 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:35,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:35,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:35,691 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,692 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,709 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:13:35,712 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:13:35,712 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:35,712 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,712 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,712 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:35,714 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:35,714 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:35,715 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,715 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,716 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:35,717 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,719 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,719 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:35,719 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,719 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,719 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:35,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:35,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:35,720 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,721 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,721 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:35,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,736 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,744 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:35,762 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:35,766 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:35,767 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:35,767 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,767 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:35,771 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:35,772 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:35,773 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:35,773 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,774 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:35,774 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,775 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,776 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,777 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:35,777 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-31 06:13:35,777 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:35,777 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:35,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:35,781 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:35,782 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-31 06:13:35,782 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:35,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-31 06:13:35,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:35,785 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:35,787 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:35,787 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,787 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,788 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:35,788 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:35,792 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:35,796 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,796 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,798 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,803 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,813 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,813 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:35,813 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,814 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,814 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:35,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:35,819 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:35,823 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,823 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,825 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,840 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,840 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:35,841 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,841 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,841 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:35,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:35,847 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:35,851 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,851 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,853 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,869 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,869 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:35,869 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,869 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,869 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:35,871 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:35,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:35,878 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,879 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,880 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,885 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,889 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,896 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,896 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:35,897 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,897 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,897 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:35,898 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:35,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:35,907 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,907 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,909 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,914 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,923 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,927 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,927 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:35,927 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,927 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,927 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:35,929 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,934 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:35,939 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,939 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,942 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,958 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,959 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:35,959 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,959 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,959 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:35,960 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:35,965 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:35,968 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:35,969 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,970 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:35,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,988 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:35,991 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:35,991 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:35,991 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:35,991 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:35,991 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:35,993 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:35,997 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:36,000 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,001 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,003 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,007 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,015 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,026 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:36,027 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:36,027 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,027 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,027 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:36,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:36,033 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:36,037 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,037 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,039 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,065 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:36,066 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:36,066 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,066 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,066 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:36,068 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:36,072 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:36,075 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,075 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,077 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,082 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,087 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,093 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:36,093 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:36,094 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,094 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,094 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:36,095 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:36,099 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:36,103 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,104 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,106 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,110 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,115 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,121 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:36,121 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:36,121 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,121 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,122 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:36,123 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:36,124 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:36,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,128 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,130 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:13:36,147 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:13:36,147 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:36,147 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,147 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,147 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:36,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:36,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:36,150 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,150 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,151 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:36,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,154 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,154 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:36,154 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,154 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,154 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:36,154 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:36,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:36,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,156 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:36,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,180 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,188 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:36,189 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:36,189 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:36,189 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,189 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:36,193 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:36,193 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:36,194 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:36,194 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,195 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:36,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,197 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,197 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,198 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:36,198 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-31 06:13:36,198 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,198 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:36,199 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:36,203 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:36,203 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-31 06:13:36,204 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,204 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-31 06:13:36,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,205 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,206 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,207 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,209 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:36,209 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,209 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,209 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:36,209 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:36,213 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:36,217 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,217 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,219 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,237 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,241 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,248 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,250 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,250 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:36,251 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,251 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,251 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:36,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:36,257 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:36,261 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,261 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,263 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,268 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,276 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,279 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,279 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:36,279 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,279 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,279 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:36,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:36,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:36,289 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,289 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,291 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,311 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,311 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:36,311 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,312 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,312 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:36,313 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:36,317 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:36,321 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,321 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,323 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,328 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,338 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,339 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:36,339 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,339 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,339 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:36,341 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:36,345 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:36,348 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,348 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,350 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,363 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,366 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,366 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:36,366 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,366 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,366 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:36,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:36,372 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:36,376 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,378 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,393 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,395 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,396 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:36,396 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,396 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,396 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:36,397 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:36,401 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:36,405 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,406 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,407 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,412 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,423 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,423 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:36,423 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,423 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,423 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:36,425 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:36,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:36,433 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,433 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,435 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,441 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,445 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,453 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,453 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:36,453 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,453 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,453 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:36,454 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:36,459 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:36,462 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,463 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,465 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,505 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,506 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:36,506 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,506 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,506 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:36,508 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:36,512 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:36,516 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,516 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,518 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,586 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,587 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:36,588 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,588 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,588 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:36,590 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:36,595 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:36,600 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,600 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,602 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,621 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,633 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,633 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:36,633 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,633 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,633 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:36,635 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:36,636 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:36,640 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,641 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,643 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,653 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,666 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:13:36,669 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:13:36,669 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:36,669 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,670 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,670 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:36,671 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:36,672 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:36,673 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,673 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,673 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:36,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,675 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,677 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,677 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:36,677 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,678 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,678 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:36,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:36,679 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:36,679 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,679 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,680 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:36,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,704 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:36,712 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:36,713 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:36,713 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:36,713 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,713 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:36,718 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:36,718 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:36,719 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:36,719 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,720 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:36,721 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,723 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,723 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,724 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:36,724 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-31 06:13:36,724 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:36,724 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:36,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:36,730 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:36,731 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-31 06:13:36,731 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:36,732 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-31 06:13:36,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,734 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:36,735 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:36,737 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:36,737 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,737 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,738 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:36,738 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:36,743 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:36,748 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,748 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,751 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,769 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,773 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,778 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,781 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,781 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:36,781 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,781 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,781 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:36,783 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:36,788 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:36,794 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,794 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,796 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,814 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,819 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,822 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,822 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:36,822 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,822 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,822 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:36,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:36,829 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:36,834 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,834 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,847 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,854 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,854 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:36,855 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,855 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,855 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:36,857 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:36,863 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:36,866 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,866 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,868 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,873 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,881 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,884 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,884 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:36,884 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,885 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,885 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:36,886 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:36,891 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:36,894 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,895 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,897 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,909 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,912 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,912 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:36,912 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,913 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,913 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:36,914 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:36,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:36,922 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,922 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,924 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,933 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,938 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,941 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,941 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:36,941 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,942 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,942 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:36,943 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:36,947 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:36,951 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,951 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,953 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,958 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,962 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,966 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,969 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,970 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:36,970 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,970 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,970 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:36,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:36,975 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:36,979 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:36,979 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,981 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:36,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,990 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,994 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:36,996 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:36,996 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:36,997 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:36,997 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:36,997 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:36,998 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,002 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:37,006 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,006 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,008 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,013 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,021 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,023 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:37,023 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:37,023 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,023 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,024 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:37,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,032 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,032 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,034 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,043 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,046 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,049 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:37,049 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:37,049 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,049 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,049 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:37,051 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,059 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,059 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,082 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,090 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,092 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:37,093 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:37,093 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,093 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,093 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:37,095 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,095 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,099 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,099 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,101 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:13:37,116 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:13:37,116 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:37,116 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,116 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,116 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:37,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,118 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,118 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,119 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,119 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,122 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,122 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:37,122 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,122 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,122 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:37,123 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,123 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,124 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,124 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,124 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,132 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,155 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:37,156 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:37,156 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:37,157 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,157 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:37,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:37,162 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,162 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:37,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,165 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,165 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:37,165 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-31 06:13:37,165 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,166 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:37,166 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,170 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,170 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-31 06:13:37,170 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,171 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-31 06:13:37,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,173 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,174 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,175 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:37,175 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,176 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,176 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:37,176 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,183 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,184 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,185 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,191 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,201 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,201 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:37,201 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,201 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,201 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:37,203 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:37,207 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,211 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,211 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,213 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,221 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,228 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,229 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:37,229 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,229 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,229 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:37,231 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:37,234 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:37,238 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,238 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,240 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,245 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,249 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,253 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,256 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,256 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:37,256 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,256 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,256 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:37,258 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:37,262 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:37,266 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,266 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,268 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,275 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,283 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,285 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,286 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:37,286 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,286 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,286 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:37,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:37,292 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:37,295 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,296 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,297 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,306 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,312 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,312 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:37,312 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,313 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,313 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:37,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:37,318 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:37,322 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,322 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,324 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,329 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,337 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,339 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,339 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:37,340 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,340 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,340 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:37,341 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:37,345 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:37,349 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,349 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,351 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,356 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,366 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,367 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:37,367 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,367 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,367 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:37,368 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:37,372 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:37,376 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,378 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,387 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,391 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,393 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,393 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:37,394 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,394 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,394 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:37,395 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,399 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:37,403 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,403 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,405 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,409 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,413 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,417 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,419 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,419 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:37,420 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,420 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,420 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:37,421 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,425 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,429 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,429 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,431 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,444 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,446 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,446 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:37,446 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,446 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,447 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:37,448 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,452 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,456 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,456 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,458 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,466 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,473 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,473 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:37,473 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,473 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,473 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:37,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,476 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,479 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,480 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,482 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,487 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:13:37,498 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:13:37,499 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:37,499 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,499 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,499 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:37,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,501 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,502 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,502 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,502 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,503 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,504 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,505 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,505 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,505 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:37,505 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,506 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,506 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:37,506 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,506 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,507 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,507 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,507 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,535 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,547 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:37,548 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:37,549 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:37,549 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,549 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:37,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,554 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,554 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:37,554 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,555 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:37,556 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,556 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,557 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,558 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,558 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:37,558 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-31 06:13:37,558 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,558 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:37,559 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,563 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,563 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-31 06:13:37,563 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,564 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-31 06:13:37,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,566 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,567 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,568 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:37,568 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,569 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,569 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:37,569 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,573 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,576 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,577 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,579 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,583 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,595 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,595 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:37,595 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,595 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,595 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:37,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:37,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,606 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,617 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,621 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,623 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,623 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:37,624 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,624 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,624 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:37,625 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:37,629 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:37,633 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,634 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,636 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,640 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,654 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,654 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:37,654 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,655 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,655 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:37,656 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:37,660 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:37,664 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,664 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,666 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,674 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,678 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,681 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,681 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:37,681 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,681 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,681 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:37,683 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:37,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:37,690 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,690 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,692 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,701 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,705 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,708 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,708 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:37,708 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,708 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,708 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:37,710 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:37,714 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:37,718 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,718 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,720 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,725 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,729 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,736 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,736 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:37,737 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,737 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,737 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:37,738 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:37,743 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:37,747 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,747 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,749 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,758 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,762 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,764 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,764 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:37,764 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,765 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,765 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:37,766 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:37,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:37,774 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,774 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,776 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,792 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,792 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:37,792 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,793 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,793 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:37,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,798 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:37,802 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,802 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,804 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,812 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,816 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,819 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,819 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:37,819 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,819 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,819 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:37,821 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:37,828 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,828 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,830 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,835 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,839 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,845 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,845 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:37,845 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,845 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,845 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:37,847 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,851 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:37,855 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,855 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,857 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,870 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,872 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,873 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:37,873 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,873 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,873 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:37,874 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:37,879 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,879 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,881 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,885 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,889 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:13:37,895 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:13:37,896 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:37,896 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,896 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,896 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:37,897 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,898 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:37,898 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,899 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,899 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,900 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,902 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,902 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:37,902 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,902 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,902 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:37,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:37,903 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,904 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,904 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:37,913 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,928 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:37,936 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:37,938 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:37,938 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:37,938 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,938 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:37,941 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,942 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:37,943 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:37,943 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,943 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:37,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,946 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,946 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:37,946 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-31 06:13:37,947 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:37,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:37,947 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,951 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:37,951 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-31 06:13:37,951 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:37,952 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-31 06:13:37,953 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,953 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:37,955 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:37,956 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:37,956 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,957 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,957 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:37,957 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:37,964 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,964 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,966 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:37,971 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:37,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:37,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:37,984 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:37,984 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:37,985 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:37,985 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,985 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:37,986 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:37,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:37,994 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:37,994 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:37,996 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,011 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,011 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:38,012 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,012 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,012 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:38,013 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:38,021 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,021 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,023 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,027 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,038 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,038 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:38,038 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,038 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,038 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:38,040 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,047 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,047 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,049 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,058 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,064 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,065 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:38,065 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,065 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,065 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:38,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,071 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,074 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,074 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,076 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,085 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,089 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,091 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,092 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:38,092 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,092 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,092 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:38,094 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,101 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,102 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,103 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,112 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,118 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,119 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:38,119 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,119 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,119 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:38,120 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:38,124 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,128 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,128 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,130 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,145 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,155 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,156 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:38,156 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,156 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,156 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:38,158 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:38,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:38,165 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,165 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,167 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,176 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,180 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,182 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,183 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:38,183 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,183 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,183 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:38,184 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:38,188 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:38,192 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,192 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,194 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,209 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,210 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:38,210 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,210 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,210 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:38,212 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:38,215 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:38,219 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,219 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,221 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,226 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,230 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,236 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,236 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:38,236 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,237 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,237 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:38,238 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:38,242 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:38,245 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,246 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,248 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,252 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,256 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,263 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,263 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:38,263 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,263 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,263 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:38,265 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:38,265 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:38,269 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,269 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,271 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,295 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:13:38,298 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:13:38,298 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:38,298 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,298 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,298 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:38,300 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:38,300 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:38,301 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,301 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,301 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:38,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,304 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,304 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:38,304 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,305 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,305 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:38,305 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:38,305 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:38,306 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,306 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,306 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:38,314 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,322 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,338 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:38,339 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:38,339 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:38,339 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,339 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:38,343 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:38,343 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:38,344 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:38,344 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,345 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:38,345 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,346 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,347 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,347 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:38,348 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-31 06:13:38,348 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,348 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:38,348 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:38,352 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:38,353 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-31 06:13:38,353 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,353 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-31 06:13:38,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,355 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,356 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,358 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:38,358 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,358 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,358 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:38,358 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:38,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:38,366 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,366 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,368 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,373 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,383 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,383 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:38,384 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,384 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,384 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:38,385 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:38,389 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:38,393 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,393 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,395 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,403 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,413 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,413 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:38,414 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,414 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,414 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:38,415 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,419 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:38,423 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,423 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,425 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,430 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,433 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,438 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,440 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,440 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:38,440 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,440 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,441 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:38,442 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,446 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,450 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,450 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,452 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,457 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,461 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,465 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,467 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,468 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:38,468 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,468 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,468 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:38,470 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,474 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,477 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,477 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,479 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,502 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,502 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:38,503 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,503 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,503 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:38,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,508 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,512 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,512 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,514 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,527 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,530 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,530 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:38,530 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,530 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,530 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:38,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:38,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,539 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,542 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,580 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,584 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,587 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,587 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:38,587 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,587 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,588 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:38,589 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:38,593 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:38,596 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,597 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,599 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,603 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,613 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,614 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:38,614 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,614 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,614 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:38,615 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:38,619 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:38,623 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,623 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,625 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,640 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,640 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:38,641 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,641 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,641 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:38,642 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:38,646 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:38,650 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,650 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,652 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,667 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,667 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:38,667 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,668 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:38,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:38,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:38,677 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,677 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,679 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,684 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,694 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,695 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:38,695 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,695 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,695 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:38,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:38,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:38,701 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,701 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,703 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:13:38,743 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:13:38,744 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:38,744 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,744 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,744 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:38,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:38,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:38,747 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,747 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,747 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:38,748 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,749 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,750 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,751 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:38,751 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,751 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,751 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:38,751 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:38,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:38,752 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,752 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,753 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:38,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,769 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:38,784 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:38,786 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:38,786 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:38,786 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,786 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:38,790 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:38,791 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:38,791 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:38,791 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,792 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:38,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,794 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,795 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,796 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:38,796 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-31 06:13:38,796 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:38,796 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:38,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:38,801 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:38,802 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-31 06:13:38,802 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:38,802 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-31 06:13:38,803 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:38,805 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:38,807 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:38,807 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,807 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,807 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:38,808 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:38,812 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:38,815 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,815 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,817 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,822 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,832 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,835 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,835 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:38,835 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,835 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,835 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:38,837 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:38,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:38,846 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,846 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,848 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,854 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,863 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,865 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,866 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:38,866 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,866 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,866 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:38,868 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,872 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:38,876 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,876 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,878 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,883 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,887 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,894 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,895 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:38,895 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,895 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,895 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:38,897 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:38,905 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,905 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,907 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,916 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,923 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,923 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:38,923 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,923 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,923 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:38,925 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,929 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:38,933 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,933 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,935 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,952 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,953 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:38,953 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,953 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,953 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:38,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,959 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:38,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,963 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,965 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:38,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,974 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:38,980 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:38,980 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:38,980 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:38,981 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,981 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:38,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:38,987 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:38,991 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:38,991 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:38,994 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,015 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,019 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,026 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,026 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:39,027 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,027 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,027 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:39,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,032 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:39,036 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,036 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,039 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,054 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,054 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:39,055 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,055 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,055 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:39,056 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,061 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,065 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,065 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,067 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,081 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,083 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,083 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:39,084 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,084 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,084 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:39,086 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,089 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,094 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,094 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,096 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,110 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,114 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,114 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:39,114 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,115 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,115 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:39,116 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:39,121 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,126 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,126 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,128 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,141 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,143 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,143 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:39,144 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,144 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,144 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:39,146 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:39,146 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:39,150 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,151 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,153 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,165 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,169 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:13:39,172 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:13:39,172 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:39,172 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,173 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,173 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:39,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:39,175 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:39,175 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,176 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,176 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:39,177 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,179 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,180 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:39,180 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,180 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,180 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:39,180 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:39,181 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:39,182 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,182 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,183 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:39,191 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,206 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,215 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:39,216 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:39,216 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:39,217 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,217 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:39,220 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:39,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:39,221 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:39,222 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,222 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:39,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,225 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,225 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,226 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:39,226 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-31 06:13:39,226 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,226 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:39,227 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:39,231 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:39,232 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-31 06:13:39,232 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,233 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-31 06:13:39,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,236 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,238 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:39,238 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,238 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,238 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:39,238 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:39,242 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:39,246 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,246 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,248 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,264 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,265 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:39,265 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,265 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,265 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:39,267 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:39,271 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:39,274 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,274 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,276 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,286 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,290 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,293 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,293 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:39,293 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,294 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,294 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:39,295 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:39,299 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:39,303 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,303 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,306 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,315 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,320 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,322 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,323 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:39,323 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,323 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,323 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:39,325 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:39,329 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:39,333 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,333 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,335 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,340 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,345 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,349 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,353 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,353 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:39,353 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,353 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,353 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:39,355 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:39,359 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:39,363 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,364 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,367 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,395 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,405 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,408 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,408 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:39,408 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,408 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,409 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:39,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:39,415 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:39,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,420 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,427 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,439 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,439 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:39,439 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,440 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,440 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:39,442 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:39,446 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:39,451 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,451 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,454 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,472 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,472 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:39,473 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,473 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,473 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:39,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,479 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:39,483 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,483 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,485 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,522 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,522 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:39,522 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,522 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,523 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:39,524 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,532 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,532 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,534 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,553 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,553 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:39,553 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,553 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,554 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:39,555 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,559 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,563 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,563 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,566 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,579 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,582 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,582 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:39,582 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,582 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,583 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:39,584 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:39,588 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,593 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,593 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,595 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,609 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,612 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,612 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:39,612 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,612 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,613 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:39,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:39,615 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:39,619 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,619 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,621 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,626 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:13:39,637 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:13:39,637 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:39,637 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,637 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,637 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:39,638 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:39,639 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:39,640 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,640 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,640 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:39,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,643 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,643 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:39,644 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,644 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,644 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:39,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:39,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:39,645 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,645 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,646 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:39,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,663 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:39,679 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:39,680 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:13:39,680 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:13:39,680 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,680 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:13:39,684 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:39,684 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:39,685 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:13:39,685 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,686 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:13:39,686 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,687 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,688 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,689 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:13:39,689 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-31 06:13:39,689 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:39,689 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:13:39,689 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:39,693 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:13:39,694 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-31 06:13:39,694 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:39,695 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-31 06:13:39,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:39,698 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:39,699 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:13:39,699 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,699 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,700 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:13:39,700 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:39,704 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:13:39,707 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,707 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,709 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,723 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,725 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,726 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:13:39,726 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,726 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,726 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:13:39,728 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:39,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:13:39,735 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,735 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,737 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,747 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,753 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,753 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:13:39,754 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,754 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,754 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:13:39,755 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:39,759 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:13:39,763 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,763 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,765 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,791 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,791 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:13:39,791 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,791 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,791 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:13:39,793 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:39,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:13:39,800 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,801 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,803 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,818 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,818 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:13:39,818 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,818 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,818 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:13:39,820 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:39,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:13:39,828 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,828 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,830 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,839 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,845 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,845 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:13:39,845 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,845 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,845 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:13:39,847 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:39,851 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:13:39,854 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,855 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,857 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,861 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,865 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,872 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,872 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:13:39,872 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,872 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,872 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:13:39,874 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:39,877 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:13:39,881 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,881 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,883 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,888 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,898 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,899 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:13:39,899 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,899 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:13:39,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,904 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:13:39,908 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,908 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,910 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,926 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,926 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:13:39,926 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,927 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,927 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:13:39,928 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:13:39,936 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,936 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,938 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,954 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,954 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:13:39,954 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,954 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:13:39,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,960 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:13:39,963 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,964 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,966 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,971 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:39,981 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:39,981 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:13:39,982 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:39,982 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,982 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:13:39,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:39,987 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:13:39,991 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:39,991 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:39,993 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:39,998 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,003 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,007 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,010 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:40,010 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:13:40,010 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:40,010 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:40,010 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:13:40,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:40,013 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:13:40,017 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:40,017 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:13:40,019 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:13:40,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,051 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:13:40,059 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:13:40,059 [wrapper.py:218 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:13:40,059 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:40,059 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:40,059 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:13:40,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:40,061 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:13:40,062 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:40,062 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:40,062 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:40,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:40,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:40,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:13:40,065 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:13:40,066 [wrapper.py:218 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:13:40,066 [wrapper.py:219 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:13:40,066 [wrapper.py:220 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:13:40,066 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:13:40,066 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:13:40,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:13:40,067 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:13:40,067 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:13:40,068 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:13:40,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:40,084 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:40,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:13:40,100 [wrapper.py:242 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:13:40,102 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 06:13:40,102 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:13:40,102 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 06:13:40,102 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:13:40,102 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 06:13:40,103 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:13:40,103 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 06:13:40,103 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:13:40,113 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 06:13:40,113 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 06:13:40,113 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 06:13:40,113 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 06:13:40,113 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 06:13:40,114 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 06:13:40,115 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 06:13:40,115 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 06:13:40,115 [wrapper.py:88 in layer_reset] DEBUG - lm_head from flexgen to old.
