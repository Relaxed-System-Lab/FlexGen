2023-10-12 02:55:14,532 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpzvji3itt
2023-10-12 02:55:14,533 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpzvji3itt/_remote_module_non_scriptable.py
2023-10-12 02:55:14,982 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-12 02:55:15,105 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:55:16,742 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-12 02:55:17,056 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-12 02:55:17,056 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-12 02:55:17,056 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-12 02:55:17,056 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-12 02:55:17,957 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:55:18,147 [model.py:159 in is_on_disk] INFO - [], []
2023-10-12 02:55:18,196 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-12 02:55:18,370 [model.py:159 in is_on_disk] INFO - [], []
2023-10-12 02:55:18,371 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/Salesforce.codegen-350M-mono'
2023-10-12 02:55:18,378 [model.py:138 in get_policy_weight_map] DEBUG - transformer.wte, [0. 0. 1.], size_todo: 304283648
2023-10-12 02:55:18,379 [model.py:138 in get_policy_weight_map] DEBUG - transformer.drop, [0. 0. 1.], size_todo: 304283648
2023-10-12 02:55:18,379 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.06454052 0.93545948], size_todo: 291693568
2023-10-12 02:55:18,380 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.10814092 0.89185908], size_todo: 279103488
2023-10-12 02:55:18,381 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.13956973 0.86043027], size_todo: 266513408
2023-10-12 02:55:18,382 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.16329946 0.83670054], size_todo: 253923328
2023-10-12 02:55:18,382 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.18185045 0.81814955], size_todo: 241333248
2023-10-12 02:55:18,383 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.19675122 0.80324878], size_todo: 228743168
2023-10-12 02:55:18,384 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.20898262 0.79101738], size_todo: 216153088
2023-10-12 02:55:18,385 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.7, [0.       0.219203 0.780797], size_todo: 203563008
2023-10-12 02:55:18,385 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.22787062 0.77212938], size_todo: 190972928
2023-10-12 02:55:18,386 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.23531438 0.76468562], size_todo: 178382848
2023-10-12 02:55:18,387 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.10, [0.        0.2417764 0.7582236], size_todo: 165792768
2023-10-12 02:55:18,388 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.11, [0.         0.24743886 0.75256114], size_todo: 153202688
2023-10-12 02:55:18,388 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.25244154 0.74755846], size_todo: 140612608
2023-10-12 02:55:18,389 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.25689339 0.74310661], size_todo: 128022528
2023-10-12 02:55:18,390 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.26088064 0.73911936], size_todo: 115432448
2023-10-12 02:55:18,390 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.26447241 0.73552759], size_todo: 102842368
2023-10-12 02:55:18,391 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.26772477 0.73227523], size_todo: 90252288
2023-10-12 02:55:18,392 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.27068364 0.72931636], size_todo: 77662208
2023-10-12 02:55:18,392 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27338705 0.72661295], size_todo: 65072128
2023-10-12 02:55:18,393 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.27586671 0.72413329], size_todo: 52482048
2023-10-12 02:55:18,394 [model.py:138 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.27586822 0.72413178], size_todo: 52480000
2023-10-12 02:55:18,394 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.23528213 0.76471787], size_todo: 0
2023-10-12 02:55:18,394 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-12 02:55:18,396 [model.py:148 in get_policy_weight_map] INFO - CausalLM Salesforce/codegen-350M-mono is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.16 GiB (23.53%), Disk Mem 0.51 Gib (76.47%)
2023-10-12 02:55:18,397 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-12 02:55:18,413 [forward.py:48 in to_test_forward] DEBUG - transformer.wte to test forward
2023-10-12 02:55:18,413 [forward.py:48 in to_test_forward] DEBUG - transformer.drop to test forward
2023-10-12 02:55:18,413 [forward.py:48 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-12 02:55:18,413 [forward.py:48 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-12 02:55:18,413 [forward.py:48 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-12 02:55:18,414 [forward.py:48 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-12 02:55:18,415 [forward.py:48 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-12 02:55:18,416 [forward.py:48 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-12 02:55:18,416 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-12 02:55:18,470 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-12 02:55:18,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:18,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:18,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:18,559 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:18,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:18,570 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:18,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:18,578 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:18,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:18,587 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:18,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:18,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:18,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:18,606 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:18,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:18,614 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:18,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:18,622 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:18,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:18,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:18,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:18,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:18,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:18,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:18,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:18,654 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:18,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:18,662 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:18,664 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:18,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:18,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:18,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:18,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:18,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:18,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:18,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:18,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:18,703 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:18,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:18,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:18,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:18,719 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:18,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:18,728 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:18,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:18,731 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:18,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:18,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:18,747 [test.py:40 in test_hf_gen] INFO - 0,
2023-10-12 02:55:18,747 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:18,756 [forward.py:28 in reset_forward] DEBUG - transformer.wte from test to old.
2023-10-12 02:55:18,756 [forward.py:28 in reset_forward] DEBUG - transformer.drop from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.0 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.1 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.2 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.3 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.4 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.5 from test to old.
2023-10-12 02:55:18,757 [forward.py:28 in reset_forward] DEBUG - transformer.h.6 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.7 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.8 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.9 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.10 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.11 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.12 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.13 from test to old.
2023-10-12 02:55:18,758 [forward.py:28 in reset_forward] DEBUG - transformer.h.14 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.h.15 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.h.16 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.h.17 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.h.18 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.h.19 from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - transformer.ln_f from test to old.
2023-10-12 02:55:18,759 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-12 02:55:18,759 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.wte to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.drop to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-12 02:55:18,760 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-12 02:55:18,761 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-12 02:55:18,762 [forward.py:125 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-12 02:55:18,802 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-12 02:55:18,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:18,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:18,873 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 8])",)
2023-10-12 02:55:18,873 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:18,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,877 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,878 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:55:18,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:18,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:18,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:18,881 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:55:18,881 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:18,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,883 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,883 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,884 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:18,884 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:55:18,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:18,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:18,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:18,889 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:18,889 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:18,898 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,923 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:18,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:18,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:18,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:18,929 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:18,930 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:18,938 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:18,961 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:18,961 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:18,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:18,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:18,968 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:18,968 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:18,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,003 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,061 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,061 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,062 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:19,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:19,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:19,067 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,068 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,076 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,084 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,102 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:19,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:19,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:19,108 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,108 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,117 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,142 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:19,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:19,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:19,148 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,148 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,157 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,183 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,183 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:19,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:19,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:19,189 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,189 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,212 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,219 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,219 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,219 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:19,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:19,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:19,226 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,255 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,256 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:19,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:19,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:19,262 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,262 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,294 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:19,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:19,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:19,302 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,302 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,334 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:19,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:19,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:19,340 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,341 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,348 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,381 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:19,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:19,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:19,387 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,387 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,421 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,421 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:19,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:19,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:19,427 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,428 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,465 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,465 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:19,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:19,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:19,471 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,497 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,505 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:19,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:19,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:19,512 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,512 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,534 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,542 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,543 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,543 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:19,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:19,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:19,550 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,550 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,569 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,577 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,584 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,585 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:19,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:19,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:19,591 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,591 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,599 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,606 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,621 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:19,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:19,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:19,627 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,627 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,689 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:19,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:19,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:19,695 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,695 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,722 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,730 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:19,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:19,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:19,734 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,734 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 8, 1024])", 'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 8])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 8])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 8, 64])"))
2023-10-12 02:55:19,768 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"))
2023-10-12 02:55:19,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:19,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:19,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:19,772 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:55:19,772 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:19,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:19,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:19,775 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:19,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 1024])
2023-10-12 02:55:19,776 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 8, 1024])
2023-10-12 02:55:19,776 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:19,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:19,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:19,778 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 8, 1024])",)
2023-10-12 02:55:19,779 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:19,794 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:55:19,808 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:55:19,823 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:55:19,837 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 8, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 8, 51200])
2023-10-12 02:55:19,841 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 8, 51200])
2023-10-12 02:55:19,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:19,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:19,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:19,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:19,848 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:19,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,851 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:19,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:19,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:19,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:19,854 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:19,854 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:19,855 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,857 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:19,857 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:19,857 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:19,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:19,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:19,861 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,869 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,889 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,889 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:19,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:19,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:19,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:19,895 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,895 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,916 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,923 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,923 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:19,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:19,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:19,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:19,929 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,930 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,952 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,960 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,973 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:19,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:19,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:19,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:19,979 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:19,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:19,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:19,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,006 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,006 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:20,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:20,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:20,012 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,012 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,032 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,039 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:20,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:20,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:20,045 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,046 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,068 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,075 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,076 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,076 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:20,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:20,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:20,082 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,082 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,089 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,096 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,111 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,112 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:20,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:20,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:20,118 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,118 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,125 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,146 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,147 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,147 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:20,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:20,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:20,154 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,179 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,186 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,192 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,200 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,200 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:20,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:20,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:20,206 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,206 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,213 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,219 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,229 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:20,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:20,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:20,242 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,242 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,273 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:20,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:20,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:20,279 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,280 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,307 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,307 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:20,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:20,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:20,313 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,314 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,328 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,351 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:20,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:20,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:20,357 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,358 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,367 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,386 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,387 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:20,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:20,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:20,393 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,393 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,400 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,406 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,424 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,424 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:20,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:20,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:20,430 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,431 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,476 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,476 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:20,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:20,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:20,482 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,482 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,509 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:20,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:20,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:20,515 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,516 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,542 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,542 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,542 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:20,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:20,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:20,548 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,575 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:20,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:20,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:20,579 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,579 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 8, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 9])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 9, 64])"))
2023-10-12 02:55:20,838 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"))
2023-10-12 02:55:20,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:20,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:20,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:20,842 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:20,842 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:20,843 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,844 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,846 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:20,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:20,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:20,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:20,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:20,848 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:20,859 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:20,872 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:20,883 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:20,894 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:20,895 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:20,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:20,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:20,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:20,903 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:20,903 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:20,904 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,906 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:20,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:20,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:20,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:20,909 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:20,909 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:20,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,911 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,911 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:20,912 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:20,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:20,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:20,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:20,916 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,916 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:20,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:20,944 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:20,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:20,989 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:20,990 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:20,990 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:20,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:20,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:20,995 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:20,996 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,003 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,010 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,016 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,024 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:21,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:21,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:21,030 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,031 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,059 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,059 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:21,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:21,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:21,065 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,079 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,086 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,093 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,094 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:21,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:21,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:21,099 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,100 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,107 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,120 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,127 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,127 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,127 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:21,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:21,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:21,133 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,133 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,166 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,166 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,166 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:21,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:21,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:21,172 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,172 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,179 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,186 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,193 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,200 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,201 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,201 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:21,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:21,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:21,207 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,208 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,259 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,260 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,260 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:21,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:21,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:21,265 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,266 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,274 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,281 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,294 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:21,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:21,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:21,300 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,300 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,308 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,322 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,328 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,329 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,329 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:21,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:21,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:21,335 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,335 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,343 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,364 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,365 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:21,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:21,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:21,371 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,386 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,394 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,401 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:21,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:21,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:21,407 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,407 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,415 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,430 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,437 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:21,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:21,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:21,443 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,478 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,478 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,478 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:21,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:21,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:21,484 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,484 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,492 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,514 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:21,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:21,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:21,520 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,520 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,567 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,637 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:21,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:21,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:21,643 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,659 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,674 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:21,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:21,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:21,680 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,680 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,687 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,694 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,701 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,719 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,720 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:21,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:21,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:21,725 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,725 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,749 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,756 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,756 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:21,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:21,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:21,760 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,761 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,775 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 9, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 10, 64])"))
2023-10-12 02:55:21,789 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"))
2023-10-12 02:55:21,789 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:21,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:21,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:21,792 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:21,792 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:21,793 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,795 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,796 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:21,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:21,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:21,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:21,798 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:21,798 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:21,809 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:21,819 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:21,829 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:21,839 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:21,841 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:21,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:21,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:21,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:21,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:21,848 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:21,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,851 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:21,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:21,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:21,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:21,854 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:21,854 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:21,855 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,857 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:21,857 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:21,857 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:21,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:21,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:21,862 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,869 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,890 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:21,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:21,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:21,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:21,896 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,896 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,920 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,927 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:21,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:21,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:21,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:21,933 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,933 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,940 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,976 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:21,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:21,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:21,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:21,983 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:21,983 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:21,990 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:21,997 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,012 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:22,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:22,015 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:22,018 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,025 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,032 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,045 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:22,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:22,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:22,052 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,052 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,066 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,080 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,080 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,080 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:22,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:22,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:22,086 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,087 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,109 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,116 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:22,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:22,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:22,122 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,129 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,136 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,142 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,149 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,150 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,150 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:22,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:22,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:22,156 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,156 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,170 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,183 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,184 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:22,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:22,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:22,190 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,190 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,229 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,238 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,238 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:22,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:22,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:22,244 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,244 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,260 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,266 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,273 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,273 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,274 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:22,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:22,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:22,279 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,280 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,307 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,308 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,308 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:22,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:22,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:22,314 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,314 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,328 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,342 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,342 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:22,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:22,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:22,348 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,348 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,355 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,375 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,376 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:22,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:22,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:22,381 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,381 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,388 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,409 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:22,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:22,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:22,415 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,415 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,428 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,442 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:22,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:22,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:22,448 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,448 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,466 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,488 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,488 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:22,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:22,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:22,494 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,563 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,667 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,667 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:22,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:22,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:22,673 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,673 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,773 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,773 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:22,774 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:22,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:22,777 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,778 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,793 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,800 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 10, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 11, 64])"))
2023-10-12 02:55:22,807 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"))
2023-10-12 02:55:22,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:22,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:22,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:22,811 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:22,811 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:22,812 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,819 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,819 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:22,819 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:22,820 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:22,821 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:22,822 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:22,822 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:22,833 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:22,843 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:22,857 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:22,872 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:22,879 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:22,879 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:22,886 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:22,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:22,887 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:22,887 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:22,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,889 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,890 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,890 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:22,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:22,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:22,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:22,893 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:22,893 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:22,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,896 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:22,896 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:22,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:22,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:22,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:22,901 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,901 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,917 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,925 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,932 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,932 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:22,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:22,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:22,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:22,938 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,938 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,946 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,960 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,967 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:22,968 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:22,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:22,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:22,974 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:22,974 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:22,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:22,995 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,001 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,002 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,002 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:23,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:23,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:23,008 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,008 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,022 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,039 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:23,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:23,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:23,046 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,046 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,078 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:23,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:23,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:23,085 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,085 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,102 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,118 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,118 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,118 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:23,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:23,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:23,126 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,126 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,135 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,144 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,151 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,158 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,159 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:23,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:23,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:23,165 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,166 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,181 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,198 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:23,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:23,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:23,204 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,204 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,213 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:23,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:23,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:23,271 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,272 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,365 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,438 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:23,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:23,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:23,446 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,446 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,588 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:23,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:23,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:23,602 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,622 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,628 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:23,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:23,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:23,634 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,634 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,663 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,663 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:23,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:23,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:23,669 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,669 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,697 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,697 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:23,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:23,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:23,702 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,716 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,722 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,729 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,729 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:23,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:23,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:23,735 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,749 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,763 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,763 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:23,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:23,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:23,768 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,769 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,799 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:23,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:23,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:23,804 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,804 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,822 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,836 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:23,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:23,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:23,843 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,843 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,857 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,863 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,870 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,870 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,870 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:23,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:23,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:23,874 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,874 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:23,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,895 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 11, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 12, 64])"))
2023-10-12 02:55:23,902 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"))
2023-10-12 02:55:23,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:23,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:23,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:23,905 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:23,905 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:23,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,910 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,910 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:23,911 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:23,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:23,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:23,914 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:23,914 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:23,940 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:23,950 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:23,960 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:23,970 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:23,972 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:23,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:23,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:23,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:23,980 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:23,980 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:23,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,982 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,983 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:23,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:23,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:23,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:23,986 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:23,986 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:23,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,989 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:23,989 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:23,989 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:23,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:23,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:23,994 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:23,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,018 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,031 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,032 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:24,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:24,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:24,037 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,038 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,081 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,091 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,098 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,099 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,099 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:24,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:24,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:24,105 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,105 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,134 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,134 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:24,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:24,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:24,140 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,140 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,148 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,320 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:24,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:24,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:24,326 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,326 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,349 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,356 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,369 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,369 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:24,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:24,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:24,375 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,375 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,382 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,388 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,402 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,402 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:24,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:24,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:24,407 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,408 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,415 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,428 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,435 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:24,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:24,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:24,440 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,440 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,454 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,468 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:24,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:24,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:24,473 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,474 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,509 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:24,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:24,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:24,515 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,515 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,523 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,535 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,542 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,542 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,543 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:24,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:24,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:24,548 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,575 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,575 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:24,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:24,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:24,581 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,581 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,588 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,609 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,609 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:24,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:24,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:24,614 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,641 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,641 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:24,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:24,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:24,647 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,655 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,661 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,668 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,675 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:24,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:24,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:24,681 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,681 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,702 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,709 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:24,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:24,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:24,715 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,715 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,722 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,729 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,742 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:24,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:24,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:24,748 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,748 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,843 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,852 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,859 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,859 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:24,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:24,863 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:24,865 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,865 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,875 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,889 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,897 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,898 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:24,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:24,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:24,904 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,905 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,920 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,934 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,934 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:24,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:24,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:24,938 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:24,938 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:24,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,954 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 12, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 13, 64])"))
2023-10-12 02:55:24,967 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"))
2023-10-12 02:55:24,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:24,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:24,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:24,970 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:24,970 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:24,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:24,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:24,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:24,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:24,974 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:24,974 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:24,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:24,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:24,977 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:24,977 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:24,990 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,001 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,012 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,023 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,025 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:25,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:25,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:25,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:25,033 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:25,033 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,034 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,036 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:25,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:25,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:25,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:25,039 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:25,039 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,041 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,042 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,042 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:25,042 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:25,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:25,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:25,046 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,046 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,067 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,074 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:25,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:25,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:25,079 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,079 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,120 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,127 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,134 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,134 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:25,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:25,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:25,140 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,140 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,170 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,170 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,170 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:25,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:25,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:25,177 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,177 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,185 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,198 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,205 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,206 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:25,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:25,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:25,211 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,211 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,221 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,235 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,243 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,243 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:25,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:25,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:25,249 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,249 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,278 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,278 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:25,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:25,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:25,284 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,298 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,312 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:25,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:25,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:25,317 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,325 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,337 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,344 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,351 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,351 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:25,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:25,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:25,357 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,357 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,400 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,423 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,423 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:25,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:25,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:25,430 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,430 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,445 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,452 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,459 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:25,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:25,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:25,465 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,466 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,495 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,495 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:25,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:25,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:25,501 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,501 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,521 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,528 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,529 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:25,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:25,532 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:25,534 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,534 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,542 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,549 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,562 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,562 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:25,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:25,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:25,568 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,568 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,576 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,582 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,615 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,615 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:25,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:25,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:25,620 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,643 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,650 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:25,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:25,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:25,656 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,656 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,687 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,687 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:25,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:25,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:25,693 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,693 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,701 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,716 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,723 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,723 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,723 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:25,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:25,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:25,729 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,729 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,758 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,758 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:25,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:25,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:25,764 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,764 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,771 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,792 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,792 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:25,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:25,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:25,796 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 13, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 14, 64])"))
2023-10-12 02:55:25,824 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"))
2023-10-12 02:55:25,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:25,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:25,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:25,827 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:25,827 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,828 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,831 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:25,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:25,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:25,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:25,833 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:25,833 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,843 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,858 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,876 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,888 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:25,889 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:25,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:25,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:25,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:25,897 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:25,897 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,898 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,898 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,900 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:25,900 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:25,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:25,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:25,903 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:25,903 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:25,903 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,904 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,905 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:25,906 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:25,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:25,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:25,908 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:25,910 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,910 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,918 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,924 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,938 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,938 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:25,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:25,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:25,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:25,943 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,943 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,951 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,971 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:25,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:25,973 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:25,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:25,976 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:25,976 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:25,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:25,997 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,005 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:26,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:26,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:26,010 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,010 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,024 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,037 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,037 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:26,039 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:26,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:26,043 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,043 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,064 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,071 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,072 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,072 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:26,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:26,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:26,077 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,077 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,084 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,091 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,117 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,117 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,117 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:26,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:26,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:26,123 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,123 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,130 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,144 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,150 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,151 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,151 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:26,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:26,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:26,156 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,156 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,170 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,183 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,184 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:26,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:26,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:26,189 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,189 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,196 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,217 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,217 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:26,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:26,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:26,223 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,251 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:26,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:26,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:26,257 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,257 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,285 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:26,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:26,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:26,291 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,299 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,319 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:26,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:26,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:26,325 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,325 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,338 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,346 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,353 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,353 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:26,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:26,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:26,358 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,358 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,382 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,397 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,405 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:26,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:26,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:26,410 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,439 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,440 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,440 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:26,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:26,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:26,445 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,445 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,452 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,466 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,472 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,473 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:26,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:26,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:26,478 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,478 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,492 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,498 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,505 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,505 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:26,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:26,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:26,511 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,511 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,539 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,539 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:26,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:26,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:26,544 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,544 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,551 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,564 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,572 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,572 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:26,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:26,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:26,576 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,577 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,584 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,591 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,612 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 14, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 15, 64])"))
2023-10-12 02:55:26,619 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"))
2023-10-12 02:55:26,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:26,621 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:26,621 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:26,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:26,622 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:26,623 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,626 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:26,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:26,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:26,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:26,628 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:26,628 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:26,639 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:26,651 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:26,661 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:26,672 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:26,674 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:26,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:26,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:26,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:26,682 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:26,682 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:26,682 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,684 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:26,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:26,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:26,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:26,687 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:26,687 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:26,688 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:26,690 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:26,690 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:26,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:26,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:26,694 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,695 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,702 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,716 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,724 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,724 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:26,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:26,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:26,729 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,758 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,759 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:26,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:26,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:26,764 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,764 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,791 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,792 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:26,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:26,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:26,797 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,797 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,825 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,825 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:26,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:26,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:26,831 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,831 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,839 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,862 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,862 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,863 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:26,864 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:26,866 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:26,868 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,868 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,914 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:26,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:26,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:26,919 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,920 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,949 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:26,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:26,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:26,955 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,955 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,962 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,969 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:26,984 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:26,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:26,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:26,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:26,990 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:26,990 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:26,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,019 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,019 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:27,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:27,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:27,026 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,047 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,054 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,054 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:27,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:27,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:27,060 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,060 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,067 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,074 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,081 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,087 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,088 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,088 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:27,090 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:27,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:27,094 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,094 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,124 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,131 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,146 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,146 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:27,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:27,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:27,152 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,152 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,160 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,167 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,180 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,181 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,181 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:27,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:27,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:27,186 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,186 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,201 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,214 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:27,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:27,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:27,220 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,221 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,249 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,249 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:27,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:27,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:27,255 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,262 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,269 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,275 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,282 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,283 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,283 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:27,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:27,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:27,288 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,289 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,316 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,316 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:27,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:27,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:27,322 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,322 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,329 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,336 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,342 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,349 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,350 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:27,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:27,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:27,355 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,355 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,363 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,372 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,419 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:27,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:27,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:27,424 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,441 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 15, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 16, 64])"))
2023-10-12 02:55:27,456 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"))
2023-10-12 02:55:27,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:27,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:27,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:27,459 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:27,459 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:27,460 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,462 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,463 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:27,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:27,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:27,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:27,465 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:27,465 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:27,477 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:27,487 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:27,498 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:27,508 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:27,510 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:27,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:27,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:27,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:27,517 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:27,517 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:27,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,519 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,521 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:27,521 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:27,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:27,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:27,524 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:27,524 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:27,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:27,527 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:27,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:27,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:27,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:27,531 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,540 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,548 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,563 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,563 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:27,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:27,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:27,568 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,568 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,577 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,584 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,592 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,600 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,600 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:27,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:27,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:27,606 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,606 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,644 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:27,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:27,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:27,658 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,682 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,690 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,690 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:27,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:27,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:27,695 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,695 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,711 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,727 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:27,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:27,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:27,732 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,733 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,748 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,764 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,764 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,764 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:27,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:27,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:27,770 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,770 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,793 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,801 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,801 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:27,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:27,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:27,807 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,807 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,815 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,823 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,838 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:27,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:27,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:27,844 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,844 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,852 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,889 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,889 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:27,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:27,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:27,896 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,896 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,930 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,930 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:27,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:27,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:27,936 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,944 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,952 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,959 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,967 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:27,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:27,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:27,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:27,973 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:27,973 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:27,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,989 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:27,996 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,004 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,004 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:28,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:28,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:28,010 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,010 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,018 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,034 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,042 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,042 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,042 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:28,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:28,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:28,048 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,048 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,056 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,079 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,079 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:28,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:28,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:28,085 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,085 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,109 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,126 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,126 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:28,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:28,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:28,132 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,132 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,140 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,161 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:28,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:28,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:28,167 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,167 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,175 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,196 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:28,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:28,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:28,201 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,209 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,223 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,230 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:28,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:28,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:28,235 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,236 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,243 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,250 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,265 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:28,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:28,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:28,269 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,270 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 16, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 17, 64])"))
2023-10-12 02:55:28,298 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"))
2023-10-12 02:55:28,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:28,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:28,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:28,301 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:28,301 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:28,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,302 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,304 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:28,304 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:28,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:28,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:28,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:28,307 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:28,318 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:28,332 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:28,352 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:28,366 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:28,368 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:28,369 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:28,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:28,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:28,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:28,378 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:28,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,382 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,383 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:28,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:28,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:28,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:28,386 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:28,386 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:28,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,388 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,389 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:28,389 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:28,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:28,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:28,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:28,394 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,394 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,426 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:28,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:28,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:28,432 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,432 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,463 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:28,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:28,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:28,469 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,469 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,501 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:28,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:28,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:28,507 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,507 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,553 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:28,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:28,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:28,563 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,563 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,572 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,579 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:28,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:28,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:28,602 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,645 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:28,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:28,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:28,658 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,665 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,696 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,696 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:28,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:28,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:28,702 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,710 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,724 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,731 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,731 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:28,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:28,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:28,737 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,746 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,767 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,767 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,767 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:28,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:28,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:28,773 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,773 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,803 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:28,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:28,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:28,808 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,809 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,816 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,823 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,838 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:28,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:28,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:28,843 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,844 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,891 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,891 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:28,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:28,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:28,897 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,897 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,926 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:28,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:28,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:28,932 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,932 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,940 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,959 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,960 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:28,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:28,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:28,965 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,965 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:28,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,979 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,992 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:28,993 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:28,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:28,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:28,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:28,998 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:28,998 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,012 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,026 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:29,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:29,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:29,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:29,031 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,059 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:29,059 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:29,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:29,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:29,065 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,079 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,086 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,092 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,093 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:29,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:29,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:29,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:29,098 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,098 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,107 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,144 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,153 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:29,153 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:29,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:29,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:29,156 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,157 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,172 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,185 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 17, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 18, 64])"))
2023-10-12 02:55:29,185 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"))
2023-10-12 02:55:29,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:29,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:29,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:29,188 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:29,188 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:29,189 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,190 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,190 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,191 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:29,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:29,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:29,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:29,194 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:29,194 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:29,204 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:29,215 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:29,225 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:29,235 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:29,237 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:29,238 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:29,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:29,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:29,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:29,245 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:29,246 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,247 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,247 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,248 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:29,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:29,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:29,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:29,251 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:29,251 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:29,252 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,252 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,253 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,254 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:29,254 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:29,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:29,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:29,258 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,258 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,279 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,286 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,286 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:29,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:29,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:29,292 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,292 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,299 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,321 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,321 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:29,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:29,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:29,327 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,327 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,348 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,355 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,355 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:29,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:29,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:29,360 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,361 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,408 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,408 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,408 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:29,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:29,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:29,414 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,414 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,430 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,445 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,445 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,446 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:29,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:29,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:29,451 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,451 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,458 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,472 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,479 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:29,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:29,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:29,485 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,492 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,512 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,512 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,512 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:29,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:29,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:29,517 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,518 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,539 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,546 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,546 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:29,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:29,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:29,551 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,552 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,559 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,566 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,573 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,580 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:29,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:29,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:29,586 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,586 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,600 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,607 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,616 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:29,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:29,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:29,621 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,633 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,646 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,653 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,653 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:29,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:29,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:29,659 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,659 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,692 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:29,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:29,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:29,697 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,697 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,704 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,710 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,717 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,724 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,724 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:29,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:29,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:29,730 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,757 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,758 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,758 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:29,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:29,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:29,763 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,770 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,777 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,790 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:29,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:29,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:29,795 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,824 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:29,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:29,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:29,830 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,830 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,844 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,858 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,858 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,859 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:29,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:29,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:29,863 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,915 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,915 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,916 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:29,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:29,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:29,921 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,921 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,928 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,949 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:29,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:29,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:29,953 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:29,953 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:29,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,975 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,982 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 18, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 19, 64])"))
2023-10-12 02:55:29,982 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"))
2023-10-12 02:55:29,982 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:29,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:29,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:29,985 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:29,985 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:29,986 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:29,989 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:29,989 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:29,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:29,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:29,991 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:29,991 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,003 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,014 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,024 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,034 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,036 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:30,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:30,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:30,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:30,043 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:30,043 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,044 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,046 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:30,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:30,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:30,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:30,049 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:30,049 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,052 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:30,052 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:30,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:30,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:30,056 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,056 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,077 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,084 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,084 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:30,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:30,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:30,089 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,090 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,104 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,111 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,133 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,133 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,133 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:30,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:30,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:30,138 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,139 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,161 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,168 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,168 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:30,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:30,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:30,173 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,173 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,181 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,201 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,202 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,202 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:30,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:30,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:30,207 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,207 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,215 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,229 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,236 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:30,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:30,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:30,241 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,241 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,270 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:30,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:30,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:30,276 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,276 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,305 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,305 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:30,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:30,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:30,310 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,310 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,318 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,325 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,339 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:30,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:30,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:30,345 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,345 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,359 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,373 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:30,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:30,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:30,379 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,425 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,426 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,426 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:30,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:30,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:30,431 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,431 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,439 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,445 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,452 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,460 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:30,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:30,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:30,466 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,466 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,480 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,494 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,494 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:30,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:30,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:30,499 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,499 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,507 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,527 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:30,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:30,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:30,533 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,533 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,541 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,547 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,554 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,561 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,561 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,561 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:30,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:30,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:30,567 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,567 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,582 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,589 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,596 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:30,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:30,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:30,601 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,601 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,634 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,642 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,642 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:30,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:30,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:30,647 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,657 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,677 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,678 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:30,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:30,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:30,683 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,683 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,710 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,710 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,710 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:30,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:30,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:30,716 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,716 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,723 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,730 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,744 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,744 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:30,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:30,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:30,748 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,748 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,763 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,769 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 19, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 20, 64])"))
2023-10-12 02:55:30,776 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"))
2023-10-12 02:55:30,776 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:30,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:30,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:30,779 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:30,779 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,782 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,782 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:30,783 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:30,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:30,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:30,785 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:30,785 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,796 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,806 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,816 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,826 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:30,828 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:30,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:30,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:30,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:30,835 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:30,835 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,836 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,838 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:30,838 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:30,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:30,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:30,840 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:30,840 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:30,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,842 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,842 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,843 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:30,843 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:30,843 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:30,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:30,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:30,847 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,847 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,854 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,861 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,868 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,875 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:30,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:30,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:30,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:30,880 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,909 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,916 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,924 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,931 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:30,931 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:30,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:30,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:30,936 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,944 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,951 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,958 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,965 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,965 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:30,965 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:30,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:30,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:30,970 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:30,970 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:30,978 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:30,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,000 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,000 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,000 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:31,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:31,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:31,005 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,006 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,013 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,020 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,033 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:31,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:31,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:31,038 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,066 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,066 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:31,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:31,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:31,071 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,071 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,085 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,099 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,100 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:31,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:31,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:31,105 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,105 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,131 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,138 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,145 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,145 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:31,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:31,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:31,150 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,150 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,158 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,164 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,171 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,178 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:31,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:31,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:31,189 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,190 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,239 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,239 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:31,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:31,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:31,245 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,245 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,279 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,280 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,280 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:31,281 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:31,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:31,285 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,285 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,301 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,318 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,318 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:31,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:31,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:31,324 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,324 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,334 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,349 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,357 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,358 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:31,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:31,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:31,364 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,406 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,416 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,416 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:31,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:31,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:31,422 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,445 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,452 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,452 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:31,454 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:31,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:31,458 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,458 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,472 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,488 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:31,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:31,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:31,493 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,516 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,523 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,524 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:31,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:31,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:31,529 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,530 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,537 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,553 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,560 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,560 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:31,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:31,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:31,566 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,566 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,595 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,596 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:31,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:31,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:31,600 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,600 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,609 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,622 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,644 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 20, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 21, 64])"))
2023-10-12 02:55:31,645 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"))
2023-10-12 02:55:31,645 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:31,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:31,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:31,647 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:31,648 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:31,648 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,651 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,652 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:31,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:31,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:31,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:31,654 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:31,654 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:31,666 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:31,676 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:31,686 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:31,696 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:31,699 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:31,699 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:31,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:31,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:31,706 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:31,706 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:31,707 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,707 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,708 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,708 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,709 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:31,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:31,709 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:31,709 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:31,711 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:31,711 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:31,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,714 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:31,714 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:31,714 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:31,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:31,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:31,718 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,740 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,747 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:31,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:31,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:31,753 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,753 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,767 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,781 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,781 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,781 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:31,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:31,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:31,786 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,787 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,801 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,814 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,815 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:31,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:31,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:31,820 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,820 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,827 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,835 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,842 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,849 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,849 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:31,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:31,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:31,857 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,857 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,865 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,885 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,885 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,885 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:31,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:31,889 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:31,891 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,891 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,922 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,936 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,943 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,943 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:31,945 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:31,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:31,949 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,949 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,963 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,970 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,977 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:31,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:31,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:31,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:31,983 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:31,983 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:31,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:31,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,012 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:32,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:32,015 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:32,017 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,032 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,046 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:32,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:32,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:32,052 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,053 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,067 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,080 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,081 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:32,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:32,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:32,086 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,087 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,108 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,115 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,115 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:32,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:32,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:32,121 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,121 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,139 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,146 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,160 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,160 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,160 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:32,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:32,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:32,166 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,166 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,180 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,187 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,194 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:32,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:32,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:32,199 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,199 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,221 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,228 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:32,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:32,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:32,234 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,262 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,263 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:32,264 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:32,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:32,268 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,268 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,276 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,296 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,297 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:32,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:32,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:32,302 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,302 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,309 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,332 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,332 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:32,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:32,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:32,338 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,338 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,359 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,366 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,366 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:32,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:32,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:32,371 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,372 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,422 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,423 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,423 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:32,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:32,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:32,427 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,427 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,441 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,449 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 21, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 22, 64])"))
2023-10-12 02:55:32,456 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"))
2023-10-12 02:55:32,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:32,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:32,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:32,458 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:32,458 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:32,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,460 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,462 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,462 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:32,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:32,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:32,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:32,464 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:32,465 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:32,475 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:32,486 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:32,497 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:32,507 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:32,509 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:32,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:32,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:32,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:32,516 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:32,517 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:32,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,519 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,520 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,520 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:32,520 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:32,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:32,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:32,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:32,523 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:32,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,526 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:32,526 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:32,526 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:32,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:32,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:32,530 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,545 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,555 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,579 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,579 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:32,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:32,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:32,585 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,615 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,615 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:32,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:32,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:32,621 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,664 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,664 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:32,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:32,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:32,669 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,670 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,677 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,700 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:32,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:32,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:32,706 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,706 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,714 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,722 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,738 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,739 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,739 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:32,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:32,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:32,745 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,745 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,767 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,774 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:32,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:32,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:32,781 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,781 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,789 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,803 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,810 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,810 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:32,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:32,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:32,816 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,816 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,846 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:32,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:32,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:32,851 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,880 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:32,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:32,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:32,886 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,886 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,921 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,936 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,936 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,936 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:32,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:32,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:32,941 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,942 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,965 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,972 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:32,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:32,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:32,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:32,978 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:32,978 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:32,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:32,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,000 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,007 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,007 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:33,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:33,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:33,014 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,014 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,035 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,042 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,042 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,042 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:33,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:33,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:33,048 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,048 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,056 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,071 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,078 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:33,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:33,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:33,084 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,084 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,092 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,099 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,106 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,113 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:33,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:33,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:33,119 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,119 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,130 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,145 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,159 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:33,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:33,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:33,165 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,180 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,187 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,194 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:33,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:33,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:33,200 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,200 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,221 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,228 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,229 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:33,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:33,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:33,234 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,235 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,265 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:33,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:33,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:33,269 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,269 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,299 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 22, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 23, 64])"))
2023-10-12 02:55:33,299 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"))
2023-10-12 02:55:33,299 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:33,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:33,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:33,302 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:33,302 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:33,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,306 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,306 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:33,306 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:33,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:33,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:33,308 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:33,308 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:33,320 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:33,335 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:33,350 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:33,361 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:33,362 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:33,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:33,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:33,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:33,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:33,370 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:33,371 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,372 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,374 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:33,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:33,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:33,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:33,377 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:33,378 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:33,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,379 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:33,381 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:33,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:33,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:33,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:33,385 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,385 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,424 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,435 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:33,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:33,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:33,441 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,441 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,449 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,456 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,471 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:33,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:33,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:33,477 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,477 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,508 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:33,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:33,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:33,514 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,514 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,543 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,543 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:33,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:33,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:33,550 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,550 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,557 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,564 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,572 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,579 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,579 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:33,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:33,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:33,585 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,615 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:33,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:33,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:33,621 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,673 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:33,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:33,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:33,678 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,679 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,701 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,707 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,708 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:33,709 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:33,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:33,713 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,713 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,728 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,735 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,742 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,742 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:33,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:33,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:33,748 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,748 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,763 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,770 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,777 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,777 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:33,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:33,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:33,783 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,783 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,797 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,811 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:33,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:33,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:33,817 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,817 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,839 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,847 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,847 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:33,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:33,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:33,852 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,853 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,881 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:33,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:33,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:33,887 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,887 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,901 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,937 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,937 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:33,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:33,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:33,942 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,943 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,950 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,972 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:33,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:33,973 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:33,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:33,977 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:33,978 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:33,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:33,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,005 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:34,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:34,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:34,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:34,011 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,011 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,032 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,039 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:34,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:34,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:34,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:34,044 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,045 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,053 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,060 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,066 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,073 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:34,073 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:34,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:34,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:34,079 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,079 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,088 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,095 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,104 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,113 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,113 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:34,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:34,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:34,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:34,117 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,117 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,127 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,137 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,165 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 23, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 24, 64])"))
2023-10-12 02:55:34,173 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"))
2023-10-12 02:55:34,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:34,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:34,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:34,176 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:34,176 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:34,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,179 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,179 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:34,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:34,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:34,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:34,182 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:34,182 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:34,193 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:34,203 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:34,213 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:34,223 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:34,224 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:34,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:34,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:34,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:34,231 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:34,231 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:34,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,234 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:34,234 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:34,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:34,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:34,237 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:34,237 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:34,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,238 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,239 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,240 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:34,240 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:34,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:34,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:34,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:34,244 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,244 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,252 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,260 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,268 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,275 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,276 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:34,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:34,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:34,281 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,281 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,311 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:34,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:34,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:34,317 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,317 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,324 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,331 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,338 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,345 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,345 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:34,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:34,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:34,351 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,358 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,384 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,385 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:34,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:34,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:34,391 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,407 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,437 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:34,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:34,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:34,443 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,451 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,458 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,465 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,472 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,473 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:34,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:34,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:34,478 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,479 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,487 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,508 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:34,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:34,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:34,514 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,515 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,543 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,544 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:34,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:34,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:34,549 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,550 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,557 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,564 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,579 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,579 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:34,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:34,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:34,585 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,601 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,616 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:34,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:34,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:34,621 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,663 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,664 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,664 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:34,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:34,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:34,670 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,670 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,687 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,695 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,703 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,703 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:34,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:34,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:34,710 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,710 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,718 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,732 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,740 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:34,741 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:34,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:34,746 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,746 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,754 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,761 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,769 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,777 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,777 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:34,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:34,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:34,784 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,795 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,820 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:34,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:34,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:34,827 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,827 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,855 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,864 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,864 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,865 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:34,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:34,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:34,872 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,872 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,884 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,902 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,925 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,926 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:34,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:34,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:34,934 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,934 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,942 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,964 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,964 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,964 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:34,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:34,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:34,970 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:34,970 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:34,977 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,984 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,991 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,998 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:34,999 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:34,999 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:35,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:35,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:35,003 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,003 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:35,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:35,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:35,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 24, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 25, 64])"))
2023-10-12 02:55:35,033 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"))
2023-10-12 02:55:35,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:35,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:35,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:35,037 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,037 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,040 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,040 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:35,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:35,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:35,042 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,043 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,053 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,064 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,074 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,084 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,086 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:35,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:35,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:35,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:35,094 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:35,094 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,095 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,096 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,096 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,097 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,097 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:35,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:35,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:35,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,099 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,102 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,102 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,103 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:35,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:35,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:35,107 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,107 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,129 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,136 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,136 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,137 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:35,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:35,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:35,142 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,142 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,160 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,167 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,182 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:35,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:35,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:35,187 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,188 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,195 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,212 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,219 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,219 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:35,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:35,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:35,225 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,225 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,240 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,260 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,261 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,261 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:35,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:35,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:35,270 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,270 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,311 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,311 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:35,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:35,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:35,316 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,316 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,330 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,338 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,345 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,345 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:35,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:35,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:35,351 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,359 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,366 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,373 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,381 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,381 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:35,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:35,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:35,386 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,387 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,394 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,433 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,443 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,443 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,443 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:35,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:35,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:35,449 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,449 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,457 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,471 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,479 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:35,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:35,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:35,485 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,508 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,515 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:35,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:35,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:35,521 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,521 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,529 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,537 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,553 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:35,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:35,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:35,558 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,558 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,566 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,574 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,581 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,589 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,589 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,589 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:35,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:35,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:35,595 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,595 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,603 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,617 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,625 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,625 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:35,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:35,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:35,630 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,646 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,664 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,672 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:35,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:35,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:35,677 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,707 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,707 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:35,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:35,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:35,712 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,712 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,733 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,740 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,740 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:35,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:35,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:35,745 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,746 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,767 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,774 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:35,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:35,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:35,779 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,780 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,787 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,801 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,808 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:35,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:35,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:35,813 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,813 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,820 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,827 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,834 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,841 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,842 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:35,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:35,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:35,845 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,853 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 25, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 26, 64])"))
2023-10-12 02:55:35,875 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"))
2023-10-12 02:55:35,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:35,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:35,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:35,878 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,878 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,880 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,881 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,882 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,882 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:35,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:35,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:35,884 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,884 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,895 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,905 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,921 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,937 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:35,939 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:35,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:35,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:35,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:35,946 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:35,947 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,949 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:35,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:35,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:35,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:35,952 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:35,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,954 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,955 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:35,955 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:35,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:35,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:35,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:35,959 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:35,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:35,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:35,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:35,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:35,988 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:35,988 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:35,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:35,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:35,993 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:35,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,001 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,008 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,022 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,022 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:36,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:36,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:36,028 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,028 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,036 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,043 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,057 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:36,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:36,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:36,063 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,063 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,070 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,077 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,083 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,090 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,090 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:36,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:36,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:36,095 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,096 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,103 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,116 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,122 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:36,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:36,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:36,127 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,128 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,136 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,143 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,169 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,170 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,170 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:36,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:36,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:36,175 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,176 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,190 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,197 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,203 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,203 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,203 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:36,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:36,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:36,209 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,209 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,230 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,237 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:36,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:36,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:36,242 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,242 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,250 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,256 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,263 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,270 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:36,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:36,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:36,276 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,276 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,303 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:36,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:36,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:36,309 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,309 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,316 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,322 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,329 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,336 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,336 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:36,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:36,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:36,341 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,341 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,348 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,355 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,361 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,367 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,368 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:36,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:36,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:36,373 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,394 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,400 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,400 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,400 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:36,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:36,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:36,405 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,406 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,414 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,442 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,449 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,455 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:36,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:36,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:36,461 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,461 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,469 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,475 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,489 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:36,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:36,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:36,494 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,522 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:36,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:36,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:36,528 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,528 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,566 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,598 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,655 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,671 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:36,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:36,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:36,677 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,694 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,701 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,709 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,710 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:36,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:36,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:36,715 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,716 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,732 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,747 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,747 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:36,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:36,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:36,751 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,751 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 26, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 27, 64])"))
2023-10-12 02:55:36,779 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"))
2023-10-12 02:55:36,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:36,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:36,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:36,782 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:36,782 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:36,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,784 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,785 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,785 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:36,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:36,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:36,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:36,787 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:36,788 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:36,801 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:36,812 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:36,822 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:36,831 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:36,832 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:36,833 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:36,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:36,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:36,840 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:36,840 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:36,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,841 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,842 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,842 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,842 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:36,843 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:36,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:36,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:36,845 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:36,845 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:36,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,847 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,847 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,848 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:36,848 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:36,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:36,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:36,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:36,852 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,880 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,880 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:36,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:36,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:36,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:36,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,914 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:36,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:36,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:36,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:36,919 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,920 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,936 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,943 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,950 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,957 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,957 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:36,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:36,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:36,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:36,963 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:36,963 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:36,971 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,978 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:36,993 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:36,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:36,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:36,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:36,999 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,000 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,008 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,015 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,029 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:37,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:37,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:37,034 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,042 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,064 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,064 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:37,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:37,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:37,070 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,070 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,078 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,085 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,092 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,101 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:37,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:37,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:37,108 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,108 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,141 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,141 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:37,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:37,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:37,146 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,146 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,160 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,167 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,174 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,175 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:37,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:37,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:37,180 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,180 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,187 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,198 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,211 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,211 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:37,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:37,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:37,216 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,216 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,238 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,245 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:37,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:37,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:37,250 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,250 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,265 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,271 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,278 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,278 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:37,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:37,281 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:37,283 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,283 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,290 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,303 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,310 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,310 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:37,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:37,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:37,315 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,323 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,330 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,336 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,342 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,343 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,343 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:37,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:37,346 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:37,348 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,348 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,356 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,376 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,376 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:37,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:37,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:37,381 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,381 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,403 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,409 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:37,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:37,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:37,415 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,415 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,429 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,455 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,455 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,455 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:37,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:37,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:37,461 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,461 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,476 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,483 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,490 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,491 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:37,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:37,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:37,497 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,497 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,521 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,527 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:37,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:37,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:37,532 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,532 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,540 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,546 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,558 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 27, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 28, 64])"))
2023-10-12 02:55:37,559 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"))
2023-10-12 02:55:37,559 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:37,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:37,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:37,562 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:37,562 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:37,563 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,564 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,565 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,565 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,566 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:37,566 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:37,567 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:37,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:37,568 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:37,568 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:37,578 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:37,589 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:37,598 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:37,607 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:37,608 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:37,609 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:37,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:37,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:37,616 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:37,616 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:37,617 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,619 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,619 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:37,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:37,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:37,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:37,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:37,622 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:37,623 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,623 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,624 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:37,625 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:37,625 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:37,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:37,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:37,629 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,629 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,637 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,643 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,656 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:37,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:37,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:37,662 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,663 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,671 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,677 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,690 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,691 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:37,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:37,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:37,697 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,697 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,720 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,738 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,746 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:37,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:37,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:37,751 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,751 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,772 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,780 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:37,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:37,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:37,785 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,793 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,800 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,813 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,814 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:37,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:37,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:37,819 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,819 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,827 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,833 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,839 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,846 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:37,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:37,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:37,851 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,852 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,880 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:37,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:37,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:37,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:37,896 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,931 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:37,987 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:37,987 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:37,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:37,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:37,997 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:37,997 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,038 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,055 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,055 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:38,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:38,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:38,063 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,063 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,075 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,082 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,090 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,097 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,097 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:38,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:38,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:38,104 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,104 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,112 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,132 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,133 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,133 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:38,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:38,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:38,138 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,138 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,147 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,166 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,172 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,173 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:38,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:38,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:38,178 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,178 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,186 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,193 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,200 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,206 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,207 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,207 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:38,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:38,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:38,212 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,212 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,291 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:38,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:38,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:38,297 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,297 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,311 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,339 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,339 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:38,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:38,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:38,344 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,344 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,353 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,377 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,378 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:38,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:38,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:38,383 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,384 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,392 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,405 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,412 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,412 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:38,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:38,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:38,417 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,418 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,425 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,432 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,438 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,444 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,444 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,445 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:38,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:38,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:38,450 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,450 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,457 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,478 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,478 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:38,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:38,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:38,481 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,481 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,489 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,502 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 28, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 29, 64])"))
2023-10-12 02:55:38,510 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"))
2023-10-12 02:55:38,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:38,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:38,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:38,513 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:38,513 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:38,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,516 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,517 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,517 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:38,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:38,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:38,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:38,519 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:38,520 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:38,535 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:38,545 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:38,555 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:38,565 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:38,567 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:38,567 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:38,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:38,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:38,575 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:38,575 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:38,576 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,576 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,577 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,578 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,578 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:38,578 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:38,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:38,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:38,581 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:38,582 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:38,582 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,583 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,584 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,585 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:38,585 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:38,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:38,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:38,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:38,590 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,590 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,598 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,617 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,617 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:38,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:38,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:38,622 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,629 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,635 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,647 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,647 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,647 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:38,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:38,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:38,653 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,653 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,661 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,667 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,680 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,680 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:38,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:38,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:38,686 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,686 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,711 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,712 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,712 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:38,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:38,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:38,717 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,717 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,737 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,743 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,743 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:38,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:38,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:38,748 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,784 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,785 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:38,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:38,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:38,790 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,790 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,804 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,810 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,817 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,824 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,824 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:38,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:38,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:38,829 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,830 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,858 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,858 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:38,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:38,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:38,863 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,872 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,878 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,886 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,894 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,894 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:38,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:38,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:38,900 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,900 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,921 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,927 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,928 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:38,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:38,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:38,933 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,933 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,948 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,954 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,962 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:38,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:38,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:38,967 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,967 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:38,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:38,994 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:38,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:38,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:38,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:38,999 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:38,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,007 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,014 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,022 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,028 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,028 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,028 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:39,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:39,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:39,033 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,069 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,077 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,077 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,077 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:39,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:39,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:39,083 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,084 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,092 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,107 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,114 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:39,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:39,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:39,119 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,120 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,135 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,142 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,148 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,149 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,149 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:39,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:39,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:39,154 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,162 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,169 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,184 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:39,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:39,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:39,189 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,190 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,207 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,213 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,220 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:39,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:39,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:39,226 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,241 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,249 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,257 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,257 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:39,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:39,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:39,261 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,261 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,308 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 29, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 30, 64])"))
2023-10-12 02:55:39,309 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"))
2023-10-12 02:55:39,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:39,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:39,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:39,312 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:39,312 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:39,312 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,313 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,316 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:39,316 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:39,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:39,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:39,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:39,318 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:39,329 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:39,339 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:39,348 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:39,357 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:39,359 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:39,359 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:39,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:39,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:39,367 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:39,367 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:39,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,370 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,370 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:39,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:39,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:39,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:39,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:39,373 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:39,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,375 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:39,376 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:39,376 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:39,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:39,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:39,380 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,380 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,388 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,395 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,402 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,409 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,409 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:39,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:39,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:39,415 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,415 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,430 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,446 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,447 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,447 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:39,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:39,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:39,452 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,466 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,480 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,480 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:39,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:39,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:39,485 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,485 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,506 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,513 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,513 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:39,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:39,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:39,518 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,519 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,533 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,540 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,548 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,549 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:39,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:39,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:39,554 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,554 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,569 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,578 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,586 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,593 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,594 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,594 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:39,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:39,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:39,600 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,600 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,608 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,615 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,628 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,628 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,629 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:39,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:39,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:39,634 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,634 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,649 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,662 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,663 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,663 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:39,664 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:39,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:39,668 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,668 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,689 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,695 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,696 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:39,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:39,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:39,701 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,701 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,709 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,716 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,723 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,730 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,730 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:39,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:39,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:39,735 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,736 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,758 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,765 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:39,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:39,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:39,770 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,771 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,778 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,805 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,812 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:39,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:39,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:39,817 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,817 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,838 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,845 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,845 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:39,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:39,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:39,850 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,858 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,865 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,880 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:39,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:39,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:39,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,906 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,912 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,913 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,913 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:39,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:39,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:39,918 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,918 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,926 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,932 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,939 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,945 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,946 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,946 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:39,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:39,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:39,951 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,951 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,959 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,966 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,972 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,979 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,979 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:39,979 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:39,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:39,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:39,984 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:39,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:39,992 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:39,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,006 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,013 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,013 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:40,013 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:40,015 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:40,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:40,018 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,026 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,033 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,058 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,058 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:40,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:40,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:40,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:40,062 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,062 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,080 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,087 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 30, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 31, 64])"))
2023-10-12 02:55:40,094 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"))
2023-10-12 02:55:40,095 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:40,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:40,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:40,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,097 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,098 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,099 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,100 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:40,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:40,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:40,103 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,103 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,113 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,123 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,132 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,141 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,143 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:40,143 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:40,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:40,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:40,150 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:40,150 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,151 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,153 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,153 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:40,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:40,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:40,155 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,156 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,157 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,158 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,158 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,158 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:40,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:40,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:40,163 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,163 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,171 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,177 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,191 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:40,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:40,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:40,196 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,211 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,217 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,224 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,225 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:40,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:40,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:40,230 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,230 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,258 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,258 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:40,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:40,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:40,263 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,297 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,304 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,305 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,305 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:40,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:40,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:40,310 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,310 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,340 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,341 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,341 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:40,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:40,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:40,346 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,346 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,355 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,369 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,376 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,376 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:40,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:40,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:40,382 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,382 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,397 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,404 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,411 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,411 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,411 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:40,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:40,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:40,418 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,418 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,441 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,448 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,449 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:40,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:40,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:40,454 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,454 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,477 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,484 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,484 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,484 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:40,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:40,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:40,491 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,491 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,507 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,522 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,522 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:40,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:40,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:40,528 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,528 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,536 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,568 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,577 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,577 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,577 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:40,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:40,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:40,583 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,583 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,592 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,599 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,607 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,614 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,614 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,614 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:40,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:40,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:40,621 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,632 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,641 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,648 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,655 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,656 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,656 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:40,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:40,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:40,662 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,662 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,670 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,678 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,694 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,694 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:40,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:40,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:40,700 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,700 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,708 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,715 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,724 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,731 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,732 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,732 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:40,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:40,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:40,737 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,753 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,760 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,773 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,773 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:40,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:40,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:40,782 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,783 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,812 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,831 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,831 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:40,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:40,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:40,836 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,836 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,844 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,860 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,866 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:40,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:40,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:40,871 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,872 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,879 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,885 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,899 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,899 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,899 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:40,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:40,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:40,903 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,903 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:40,911 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,917 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,923 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,929 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 31, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 32, 64])"))
2023-10-12 02:55:40,930 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"))
2023-10-12 02:55:40,930 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:40,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:40,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:40,932 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,932 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,933 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,936 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,936 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,936 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:40,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:40,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:40,938 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,938 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,949 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,959 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,968 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,978 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:40,980 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:40,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:40,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:40,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:40,987 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:40,987 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,987 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,989 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,989 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,989 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:40,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:40,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:40,992 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:40,992 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:40,992 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,993 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:40,995 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:40,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:40,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:40,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:40,999 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:40,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,007 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,013 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,019 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,025 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,026 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:41,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:41,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:41,031 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,031 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,039 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,045 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,051 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,057 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,057 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:41,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:41,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:41,062 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,062 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,079 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,085 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,091 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,097 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,097 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:41,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:41,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:41,103 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,103 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,116 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,128 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:41,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:41,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:41,133 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,134 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,142 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,148 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,154 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,160 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,160 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,160 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:41,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:41,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:41,165 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,178 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,190 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,190 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:41,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:41,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:41,196 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,196 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,204 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,210 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,222 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:41,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:41,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:41,228 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,236 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,242 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,248 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,254 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,254 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,254 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:41,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:41,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:41,260 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,260 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,268 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,274 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,279 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,286 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:41,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:41,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:41,291 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,307 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,314 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,326 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:41,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:41,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:41,332 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,332 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,340 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,347 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,353 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,360 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,360 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,360 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:41,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:41,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:41,365 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,366 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,380 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,387 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,393 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,394 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:41,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:41,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:41,399 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,399 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,407 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,420 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,427 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:41,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:41,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:41,432 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,432 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,453 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,459 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,459 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:41,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:41,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:41,465 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,465 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,473 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,479 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,485 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,491 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,491 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:41,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:41,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:41,496 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,496 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,524 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,524 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:41,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:41,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:41,530 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,530 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,571 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,571 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:41,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:41,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:41,577 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,577 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,590 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,597 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,612 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,612 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:41,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:41,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:41,618 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,618 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,626 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,632 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,639 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,647 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,647 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,647 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:41,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:41,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:41,651 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,660 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,666 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 32, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 33, 64])"))
2023-10-12 02:55:41,679 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"))
2023-10-12 02:55:41,680 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:41,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:41,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:41,682 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:41,682 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:41,683 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,684 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,686 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:41,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:41,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:41,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:41,688 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:41,688 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:41,699 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:41,709 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:41,719 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:41,728 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:41,730 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:41,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:41,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:41,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:41,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:41,738 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:41,738 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,740 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,740 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,741 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:41,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:41,741 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:41,741 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:41,743 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:41,743 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:41,744 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,746 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:41,746 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:41,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:41,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:41,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:41,751 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,751 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,779 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,780 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:41,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:41,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:41,785 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,798 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,813 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,819 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,826 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:41,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:41,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:41,831 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,831 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,839 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,851 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,857 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,858 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:41,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:41,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:41,863 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,863 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,870 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,876 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,888 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,889 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:41,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:41,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:41,894 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,902 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,908 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,915 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,921 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,921 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,921 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:41,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:41,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:41,927 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,927 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,935 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,940 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,953 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,953 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,953 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:41,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:41,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:41,959 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,959 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,967 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,979 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,985 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:41,985 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:41,985 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:41,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:41,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:41,991 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:41,991 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:41,999 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,005 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,011 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,017 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,018 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:42,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:42,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:42,023 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,023 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,031 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,037 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,043 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,049 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,049 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:42,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:42,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:42,055 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,055 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,081 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,088 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,094 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,100 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,101 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:42,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:42,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:42,106 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,106 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,114 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,119 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,131 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,131 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:42,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:42,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:42,137 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,137 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,144 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,150 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,156 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,163 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,163 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:42,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:42,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:42,169 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,169 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,182 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,188 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,194 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,194 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:42,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:42,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:42,200 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,200 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,208 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,214 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,220 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,226 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,226 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:42,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:42,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:42,232 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,232 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,240 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,246 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,252 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,258 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,258 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:42,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:42,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:42,264 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,264 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,272 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,278 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,283 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,290 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:42,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:42,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:42,296 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,296 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,315 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,333 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:42,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:42,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:42,338 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,338 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,346 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,359 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,365 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,365 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:42,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:42,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:42,370 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,385 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,392 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,400 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,400 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:42,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:42,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:42,404 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,404 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,413 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,419 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,425 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,431 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 33, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 34, 64])"))
2023-10-12 02:55:42,431 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"))
2023-10-12 02:55:42,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:42,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:42,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:42,434 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:42,434 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:42,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,435 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,436 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,437 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:42,438 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:42,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:42,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:42,440 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:42,440 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:42,451 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:42,463 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:42,473 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:42,483 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:42,485 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:42,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:42,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:42,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:42,492 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:42,492 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:42,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,493 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,494 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,495 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,495 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:42,495 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:42,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:42,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:42,498 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:42,498 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:42,498 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,499 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,500 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,501 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:42,501 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:42,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:42,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:42,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:42,505 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,505 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,514 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,521 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,527 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,533 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,533 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,534 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:42,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:42,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:42,539 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,539 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,547 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,553 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,577 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,583 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,583 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:42,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:42,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:42,589 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,589 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,598 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,616 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:42,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:42,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:42,622 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,630 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,636 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,642 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,648 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,648 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:42,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:42,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:42,654 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,654 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,661 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,667 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,674 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,680 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,680 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,680 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:42,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:42,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:42,686 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,686 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,693 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,699 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,706 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,712 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:42,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:42,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:42,718 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,734 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,751 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,751 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:42,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:42,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:42,758 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,758 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,765 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,788 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,788 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:42,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:42,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:42,794 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,794 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,802 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,808 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,830 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,837 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,837 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:42,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:42,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:42,843 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,843 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,850 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,856 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,862 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,868 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,868 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,869 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:42,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:42,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:42,874 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,874 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,882 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,887 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,900 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,900 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:42,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:42,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:42,905 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,905 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,913 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,919 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,925 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,930 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,931 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,931 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:42,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:42,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:42,936 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,943 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,955 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,961 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,961 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,961 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:42,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:42,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:42,967 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,967 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:42,975 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,981 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,988 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,994 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:42,994 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:42,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:42,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:42,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:42,999 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:42,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,007 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,014 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,021 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,027 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,027 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:43,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:43,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:43,032 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,032 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,040 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,046 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,052 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,059 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,059 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,059 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:43,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:43,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:43,064 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,064 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,097 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,104 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,110 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,110 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,110 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:43,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:43,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:43,115 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,115 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,140 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,140 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:43,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:43,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:43,145 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,145 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,153 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,164 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,170 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,171 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,171 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:43,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:43,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:43,174 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,175 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,209 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,215 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,222 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,228 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 34, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 35, 64])"))
2023-10-12 02:55:43,228 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"))
2023-10-12 02:55:43,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:43,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:43,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:43,231 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:43,231 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:43,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,232 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,233 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,234 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,234 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:43,234 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:43,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:43,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:43,236 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:43,236 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:43,247 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:43,256 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:43,266 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:43,275 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:43,277 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:43,277 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:43,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:43,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:43,285 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:43,285 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:43,285 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,286 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,287 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,287 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:43,287 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:43,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:43,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:43,290 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:43,290 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:43,291 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,292 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,293 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:43,293 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:43,293 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:43,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:43,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:43,297 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,297 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,333 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,340 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,352 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,363 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,363 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:43,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:43,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:43,369 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,369 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,386 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,393 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,401 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,401 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:43,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:43,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:43,406 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,407 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,416 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,423 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,430 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,437 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,438 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,438 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:43,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:43,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:43,443 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,451 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,457 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,464 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,470 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,471 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:43,472 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:43,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:43,476 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,476 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,484 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,491 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,497 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,504 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:43,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:43,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:43,510 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,510 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,525 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,538 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,539 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,539 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:43,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:43,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:43,544 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,544 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,560 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,582 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,591 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,591 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,591 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:43,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:43,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:43,597 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,597 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,611 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,625 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,625 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:43,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:43,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:43,630 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,630 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,638 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,645 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,652 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,659 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,659 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,659 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:43,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:43,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:43,665 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,672 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,686 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,692 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,692 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:43,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:43,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:43,698 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,698 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,705 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,712 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,721 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,727 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,728 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,728 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:43,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:43,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:43,733 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,733 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,741 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,748 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,755 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,762 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,762 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:43,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:43,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:43,767 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,768 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,776 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,783 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,811 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,811 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:43,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:43,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:43,817 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,817 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,831 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,839 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,846 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,846 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:43,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:43,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:43,851 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,866 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,874 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,880 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,881 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:43,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:43,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:43,885 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,886 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,893 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,907 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,914 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,914 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:43,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:43,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:43,919 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,919 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,941 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,947 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,948 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,948 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:43,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:43,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:43,953 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,962 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,969 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,983 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:43,983 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:43,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:43,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:43,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:43,988 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:43,988 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:43,996 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,003 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,010 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,017 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,018 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:44,018 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:44,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:44,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:44,021 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,030 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,036 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,043 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,050 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 35, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 36, 64])"))
2023-10-12 02:55:44,050 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"))
2023-10-12 02:55:44,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:44,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:44,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:44,053 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:44,053 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,054 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,055 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,056 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,056 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:44,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:44,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:44,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:44,059 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:44,059 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,079 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,091 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,102 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,111 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,112 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:44,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:44,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:44,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:44,120 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-12 02:55:44,120 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,121 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,122 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,123 [forward.py:104 in new_forward] DEBUG - layer: transformer.wte, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,123 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:44,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-12 02:55:44,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-12 02:55:44,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:44,125 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:44,126 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,126 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,127 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,128 [forward.py:104 in new_forward] DEBUG - layer: transformer.drop, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,128 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:44,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-12 02:55:44,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-12 02:55:44,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:44,132 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,133 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,140 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,146 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,152 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,159 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,159 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-12 02:55:44,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-12 02:55:44,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:44,164 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,173 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,179 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,186 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,192 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,192 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-12 02:55:44,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-12 02:55:44,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:44,197 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,205 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,212 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,218 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,225 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,225 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-12 02:55:44,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-12 02:55:44,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:44,230 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,231 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,238 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,244 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,251 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,257 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,257 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,257 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-12 02:55:44,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-12 02:55:44,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:44,263 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,277 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,284 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,289 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,290 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-12 02:55:44,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-12 02:55:44,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:44,295 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,295 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,319 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,326 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,332 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,339 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,340 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-12 02:55:44,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-12 02:55:44,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:44,345 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,345 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,353 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,360 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,367 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,378 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,378 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-12 02:55:44,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-12 02:55:44,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:44,384 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,384 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,392 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,399 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,406 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,412 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,413 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-12 02:55:44,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-12 02:55:44,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:44,418 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,418 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,427 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,434 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,440 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,448 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,448 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-12 02:55:44,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-12 02:55:44,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:44,453 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,453 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,461 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,475 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,482 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,482 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,483 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-12 02:55:44,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-12 02:55:44,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:44,487 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,496 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,511 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,518 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,518 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,518 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-12 02:55:44,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-12 02:55:44,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:44,523 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,523 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,531 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,537 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,552 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-12 02:55:44,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-12 02:55:44,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:44,557 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,567 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,585 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,597 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,604 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,605 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-12 02:55:44,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-12 02:55:44,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:44,610 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,610 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,618 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,625 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,632 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,638 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,639 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-12 02:55:44,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-12 02:55:44,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:44,644 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,644 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,654 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,662 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,669 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,676 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,676 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,676 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-12 02:55:44,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-12 02:55:44,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:44,681 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,682 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,690 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,697 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,703 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,710 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,710 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-12 02:55:44,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-12 02:55:44,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:44,717 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,717 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,725 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,732 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,739 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,745 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,746 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-12 02:55:44,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-12 02:55:44,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:44,751 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,752 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,759 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,766 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,773 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,780 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,780 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-12 02:55:44,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-12 02:55:44,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:44,785 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,786 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,794 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,818 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,825 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,832 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,833 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,833 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-12 02:55:44,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-12 02:55:44,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:44,837 [forward.py:84 in new_forward] DEBUG - args: ()
2023-10-12 02:55:44,837 [forward.py:85 in new_forward] DEBUG - kwargs: {'hidden_states': "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", 'layer_past': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([8, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}
2023-10-12 02:55:44,845 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,852 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,859 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,867 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: (), kwargs: {'hidden_states': "<class 'torch.Tensor'>: torch.Size([2, 1, 1024])", 'layer_past': ("<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 16, 36, 64])"), 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'position_ids': "<class 'torch.Tensor'>: torch.Size([2, 1])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 16, 37, 64])"))
2023-10-12 02:55:44,867 [forward.py:118 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 16, 37, 64])"))
2023-10-12 02:55:44,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-12 02:55:44,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-12 02:55:44,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:44,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:44,870 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,871 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,872 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,873 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 1024])
2023-10-12 02:55:44,873 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 1024])
2023-10-12 02:55:44,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-12 02:55:44,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-12 02:55:44,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-12 02:55:44,875 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1, 1024])",)
2023-10-12 02:55:44,875 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-12 02:55:44,886 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,895 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,904 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,914 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 51200])
2023-10-12 02:55:44,915 [forward.py:118 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 51200])
2023-10-12 02:55:44,916 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-12 02:55:44,923 [test.py:40 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print
2023-10-12 02:55:44,923 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,923 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man?
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,924 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,924 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,924 [test.py:40 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,924 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man?
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,924 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed
2023-10-12 02:55:44,924 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,925 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the
2023-10-12 02:55:44,925 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.wte from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.drop from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.h.0 from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.h.1 from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.h.2 from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.h.3 from flexgen to old.
2023-10-12 02:55:44,935 [forward.py:23 in reset_forward] DEBUG - transformer.h.4 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.5 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.6 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.7 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.8 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.9 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.10 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.11 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.12 from flexgen to old.
2023-10-12 02:55:44,936 [forward.py:23 in reset_forward] DEBUG - transformer.h.13 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.14 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.15 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.16 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.17 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.18 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.h.19 from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - transformer.ln_f from flexgen to old.
2023-10-12 02:55:44,937 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
