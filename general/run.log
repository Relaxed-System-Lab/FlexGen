2023-10-08 06:41:01,098 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpuu5m0ybz
2023-10-08 06:41:01,098 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpuu5m0ybz/_remote_module_non_scriptable.py
2023-10-08 06:41:01,516 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-08 06:41:01,573 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 06:41:03,060 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-08 06:41:03,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-08 06:41:03,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-08 06:41:03,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-08 06:41:03,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-08 06:41:04,118 [flexgen_init.py:38 in policy_init] DEBUG - Got empty CausalLM: 'facebook/opt-125m' on meta device.
2023-10-08 06:41:04,128 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-08 06:41:04,128 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-08 06:41:04,129 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-08 06:41:04,130 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-08 06:41:04,131 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-08 06:41:04,132 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-08 06:41:04,133 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-08 06:41:04,134 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-08 06:41:04,134 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-08 06:41:04,135 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-08 06:41:04,136 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-08 06:41:04,137 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-08 06:41:04,138 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-08 06:41:04,139 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-08 06:41:04,140 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-08 06:41:04,140 [flexgen_init.py:235 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-08 06:41:04,141 [flexgen_init.py:239 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-08 06:41:04,143 [flexgen_init.py:245 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-08 06:41:04,181 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 06:41:04,309 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 06:41:04,388 [flexgen_init.py:115 in check_disk] INFO - [], ['lm_head.weight']
2023-10-08 06:41:04,429 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 06:41:04,506 [flexgen_init.py:115 in check_disk] INFO - [], ['lm_head.weight']
2023-10-08 06:41:04,507 [flexgen_init.py:143 in get_model_on_disk] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-08 06:41:04,555 [flexgen_init.py:64 in policy_init] INFO - model has been loaded by policy.
2023-10-08 06:41:04,555 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-08 06:41:04,555 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-08 06:41:04,555 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-08 06:41:04,555 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-08 06:41:04,556 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-08 06:41:04,557 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-08 06:41:04,557 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-08 06:41:04,557 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-08 06:41:04,557 [flexgen_forward.py:38 in to_test_forward] DEBUG - lm_head to test forward
2023-10-08 06:41:04,598 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-08 06:41:04,780 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:04,781 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_tokens forward pass:
2023-10-08 06:41:04,781 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:04,782 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:04,783 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_positions forward pass:
2023-10-08 06:41:04,783 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:04,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:04,790 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.0 forward pass:
2023-10-08 06:41:04,802 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:04,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:04,811 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.1 forward pass:
2023-10-08 06:41:04,815 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:04,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:04,827 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.2 forward pass:
2023-10-08 06:41:04,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:04,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:04,844 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.3 forward pass:
2023-10-08 06:41:04,857 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:04,859 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:04,867 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.4 forward pass:
2023-10-08 06:41:04,870 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:04,872 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:04,878 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.5 forward pass:
2023-10-08 06:41:04,882 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:04,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:04,890 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.6 forward pass:
2023-10-08 06:41:04,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:04,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:04,902 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.7 forward pass:
2023-10-08 06:41:04,905 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:04,906 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:04,913 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.8 forward pass:
2023-10-08 06:41:04,916 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:04,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:04,924 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.9 forward pass:
2023-10-08 06:41:04,927 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:04,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:04,936 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.10 forward pass:
2023-10-08 06:41:04,939 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:04,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:04,948 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.11 forward pass:
2023-10-08 06:41:04,951 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:04,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:04,954 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.final_layer_norm forward pass:
2023-10-08 06:41:04,954 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:04,955 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:04,955 [flexgen_forward.py:47 in new_forward] DEBUG - lm_head forward pass:
2023-10-08 06:41:04,964 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:04,969 [flexgen_test.py:39 in test_hf_gen] INFO - 0.
2023-10-08 06:41:04,970 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-08 06:41:04,980 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-08 06:41:04,981 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-08 06:41:04,982 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-08 06:41:04,982 [flexgen_forward.py:27 in to_old_forward] DEBUG - lm_head from test to old.
2023-10-08 06:41:04,982 [flexgen_init.py:87 in policy_init] INFO - layer order: ['model.decoder.embed_tokens', 'model.decoder.embed_positions', 'model.decoder.layers.0', 'model.decoder.layers.1', 'model.decoder.layers.2', 'model.decoder.layers.3', 'model.decoder.layers.4', 'model.decoder.layers.5', 'model.decoder.layers.6', 'model.decoder.layers.7', 'model.decoder.layers.8', 'model.decoder.layers.9', 'model.decoder.layers.10', 'model.decoder.layers.11', 'model.decoder.final_layer_norm', 'lm_head']
2023-10-08 06:41:04,982 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-08 06:41:04,982 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-08 06:41:04,982 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-08 06:41:04,982 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-08 06:41:04,983 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-08 06:41:04,984 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-08 06:41:04,984 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-08 06:41:04,984 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-08 06:41:04,984 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-08 06:41:04,984 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-08 06:41:05,039 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-08 06:41:05,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:05,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:05,224 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9]),)
2023-10-08 06:41:05,224 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,224 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9]),)
2023-10-08 06:41:05,225 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:05,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:05,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:05,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:05,226 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-08 06:41:05,226 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-08 06:41:05,226 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:05,227 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:05,227 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:05,235 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9]), 0)
2023-10-08 06:41:05,235 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9]), 0)
2023-10-08 06:41:05,235 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:05,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:05,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:05,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:05,237 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-08 06:41:05,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-08 06:41:05,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:05,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:05,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:05,252 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,253 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,253 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,253 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:05,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:05,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:05,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:05,267 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,267 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,267 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:05,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:05,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:05,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,284 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,284 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:05,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:05,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:05,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:05,296 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,296 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,296 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:05,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:05,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:05,312 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,313 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,313 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,313 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:05,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:05,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:05,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:05,327 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:05,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:05,337 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:05,344 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,344 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,344 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,344 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:05,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:05,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:05,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:05,357 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:05,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:05,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:05,374 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,374 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,374 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,374 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:05,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:05,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:05,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:05,387 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,388 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,388 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:05,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:05,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:05,405 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,406 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,406 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:05,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:05,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:05,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:05,421 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,422 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,422 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:05,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:05,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:05,438 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,439 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,439 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,439 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:05,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:05,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:05,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:05,454 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,455 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,455 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:05,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:05,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:05,472 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,472 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,472 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:05,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:05,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:05,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:05,487 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,488 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,488 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:05,490 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:05,497 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:05,505 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,505 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:05,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:05,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:05,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:05,522 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,522 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,523 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:05,524 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:05,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:05,539 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,539 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,540 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:05,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:05,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:05,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:05,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:05,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:05,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:05,568 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,569 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,569 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,569 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:05,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:05,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:05,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:05,582 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,583 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,583 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:05,585 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:05,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:05,593 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,593 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,593 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:05,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:05,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:05,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:05,605 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-08 06:41:05,606 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-08 06:41:05,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:05,607 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:05,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:05,609 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,609 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,609 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:05,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:05,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:05,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:05,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-08 06:41:05,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-08 06:41:05,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:05,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:05,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:05,613 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-08 06:41:05,613 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,614 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-08 06:41:05,614 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:05,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:05,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:05,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:05,654 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 50272])
2023-10-08 06:41:05,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 50272])
2023-10-08 06:41:05,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:05,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:05,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:05,663 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:05,664 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,664 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:05,664 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:05,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:05,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:05,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:05,665 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:05,665 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:05,665 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:05,666 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:05,666 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:05,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]), 9)
2023-10-08 06:41:05,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:05,674 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]), 9)
2023-10-08 06:41:05,674 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:05,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:05,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:05,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:05,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:05,676 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:05,676 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:05,676 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:05,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:05,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:05,691 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,691 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,691 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,691 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:05,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:05,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:05,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:05,702 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:05,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:05,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:05,719 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,720 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,720 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,720 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:05,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:05,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:05,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:05,729 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,730 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,730 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:05,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:05,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:05,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:05,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:05,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:05,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:05,756 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:05,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:05,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:05,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,773 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,773 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,773 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:05,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:05,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:05,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:05,784 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,784 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,785 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:05,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:05,794 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:05,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,802 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,802 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:05,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:05,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:05,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:05,811 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,811 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,811 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:05,813 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:05,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:05,829 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,830 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,830 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:05,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:05,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:05,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:05,843 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,844 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,844 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:05,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:05,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:05,862 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,862 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,863 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:05,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:05,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:05,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:05,873 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,873 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,873 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:05,875 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:05,885 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:05,895 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,895 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,895 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,895 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:05,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:05,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:05,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:05,905 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,906 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,906 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:05,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:05,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:05,924 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,924 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:05,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:05,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:05,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:05,936 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,936 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,936 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:05,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:05,946 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:05,954 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,954 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,954 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,954 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:05,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:05,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:05,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:05,965 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:05,965 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:05,965 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:05,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:05,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:05,986 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:05,987 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,987 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:05,987 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:05,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:05,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:05,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:05,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:06,000 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:06,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:06,000 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:06,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:06,010 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,011 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,012 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,012 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,012 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:06,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:06,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:06,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:06,022 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-08 06:41:06,023 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-08 06:41:06,023 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:06,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,026 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,026 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,027 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:06,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:06,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:06,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:06,033 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,033 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,033 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:06,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,035 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,035 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,035 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,036 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:06,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:06,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:06,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:06,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:06,064 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:06,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:06,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,072 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:06,072 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:06,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:06,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:06,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:06,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:06,073 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,074 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:06,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 11]), 10)
2023-10-08 06:41:06,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 11]), 10)
2023-10-08 06:41:06,083 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:06,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:06,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:06,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:06,084 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,084 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,084 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:06,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,099 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,099 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:06,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:06,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:06,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:06,109 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,109 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:06,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,126 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:06,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:06,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:06,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:06,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,136 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,136 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:06,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,156 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,156 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,156 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,156 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:06,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:06,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:06,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:06,166 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,167 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,167 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:06,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,182 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,183 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,183 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,183 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:06,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:06,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:06,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:06,192 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,193 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:06,195 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,202 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:06,210 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,210 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,210 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,211 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:06,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:06,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:06,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:06,220 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,220 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,221 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:06,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:06,230 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:06,237 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,237 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,238 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,238 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:06,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:06,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:06,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:06,248 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,248 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,248 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:06,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:06,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:06,264 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,265 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,265 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:06,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:06,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:06,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:06,274 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,274 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,274 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:06,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:06,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:06,290 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,290 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,291 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,291 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:06,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:06,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:06,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:06,300 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,300 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,300 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:06,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:06,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:06,317 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,317 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,317 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:06,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:06,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:06,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:06,327 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:06,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:06,336 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:06,344 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,344 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,344 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,344 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:06,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:06,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:06,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:06,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,356 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:06,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:06,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:06,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,372 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,372 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:06,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:06,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:06,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:06,381 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,382 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:06,384 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:06,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,392 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,392 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,392 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,392 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:06,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:06,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:06,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:06,401 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-08 06:41:06,402 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-08 06:41:06,402 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:06,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,405 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,405 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:06,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:06,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:06,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:06,406 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,407 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,407 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:06,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,409 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:06,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:06,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:06,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:06,438 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:06,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:06,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:06,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,446 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:06,447 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,447 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:06,447 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:06,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:06,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:06,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:06,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,448 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,448 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:06,449 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,457 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 12]), 11)
2023-10-08 06:41:06,458 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,458 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 12]), 11)
2023-10-08 06:41:06,458 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:06,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:06,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:06,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:06,460 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,460 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,460 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:06,461 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,476 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,476 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,476 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,476 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:06,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:06,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:06,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:06,486 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,486 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,486 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:06,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,495 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,503 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,503 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,504 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,504 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:06,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:06,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:06,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:06,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:06,516 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,523 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,531 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,531 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,532 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,532 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:06,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:06,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:06,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:06,576 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,577 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,577 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:06,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,585 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,592 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,592 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,593 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:06,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:06,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:06,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:06,629 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,630 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,630 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:06,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:06,647 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,647 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,647 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,647 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:06,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:06,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:06,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:06,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,656 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,656 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:06,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:06,665 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:06,672 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,672 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,672 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,672 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:06,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:06,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:06,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:06,681 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,681 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,682 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:06,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:06,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:06,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,696 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:06,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:06,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:06,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:06,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,706 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,706 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:06,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:06,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:06,721 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,721 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,721 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,722 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:06,724 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:06,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:06,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:06,730 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,731 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,731 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:06,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:06,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:06,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:06,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:06,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:06,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:06,754 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,755 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,755 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:06,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:06,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:06,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:06,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:06,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:06,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:06,778 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,778 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:06,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:06,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:06,792 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,793 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:06,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:06,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:06,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:06,801 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,801 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,801 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:06,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:06,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:06,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:06,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:06,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:06,819 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-08 06:41:06,819 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-08 06:41:06,819 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:06,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:06,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,822 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,822 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,822 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,822 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:06,823 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,824 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,824 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:06,824 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:06,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,825 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,825 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,825 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,825 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:06,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:06,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:06,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:06,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:06,857 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:06,857 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:06,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:06,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,863 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:06,863 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,863 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:06,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:06,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:06,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:06,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:06,864 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,864 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,864 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:06,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:06,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,872 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 13]), 12)
2023-10-08 06:41:06,872 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:06,872 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 13]), 12)
2023-10-08 06:41:06,872 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:06,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:06,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:06,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:06,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:06,874 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:06,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:06,874 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:06,874 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:06,881 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,888 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,888 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,888 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,888 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:06,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:06,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:06,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:06,896 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:06,897 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:06,897 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:06,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:06,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,912 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,912 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,912 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,912 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:06,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:06,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:06,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:06,921 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:06,921 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:06,921 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:06,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:06,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,937 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,937 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,937 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,937 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:06,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:06,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:06,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:06,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:06,946 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:06,946 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:06,948 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:06,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,961 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,961 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,961 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,961 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:06,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:06,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:06,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:06,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:06,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:06,971 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:06,972 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:06,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:06,986 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:06,986 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,986 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:06,986 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:06,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:06,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:06,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:06,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:06,996 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:06,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:06,996 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:06,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:07,004 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,011 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,011 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,011 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,011 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:07,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:07,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:07,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:07,021 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,021 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:07,022 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,036 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,036 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,036 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,036 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:07,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:07,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:07,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:07,044 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,044 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,044 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:07,046 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,059 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,059 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,059 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,059 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:07,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:07,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:07,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:07,067 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,068 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,068 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:07,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,082 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:07,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:07,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:07,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:07,091 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,092 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:07,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:07,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:07,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:07,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:07,115 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,115 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,115 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:07,117 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,129 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,130 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,130 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,130 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:07,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:07,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:07,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:07,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,144 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:07,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,152 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,153 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,153 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,153 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,154 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:07,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:07,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:07,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:07,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-08 06:41:07,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-08 06:41:07,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:07,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,164 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,165 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,165 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,165 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,165 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:07,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:07,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:07,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:07,166 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,166 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,166 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:07,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:07,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:07,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:07,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:07,195 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:07,196 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:07,196 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:07,202 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,203 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:07,204 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,204 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:07,204 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:07,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:07,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:07,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:07,205 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,205 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,205 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:07,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 14]), 13)
2023-10-08 06:41:07,213 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,213 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 14]), 13)
2023-10-08 06:41:07,213 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:07,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:07,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:07,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:07,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:07,215 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,221 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,227 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,228 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,228 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,228 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:07,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:07,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:07,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:07,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:07,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:07,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:07,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:07,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:07,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,260 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,260 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:07,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:07,275 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,276 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,276 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,276 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:07,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:07,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:07,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:07,285 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,285 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,285 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:07,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:07,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:07,300 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,300 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,301 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,301 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:07,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:07,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:07,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:07,309 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:07,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:07,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:07,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,325 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,325 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:07,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:07,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:07,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:07,334 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,334 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,334 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:07,336 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:07,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,349 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,349 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,349 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,350 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:07,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:07,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:07,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:07,358 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,358 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,358 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:07,360 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,367 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,374 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,374 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,374 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,374 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:07,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:07,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:07,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:07,419 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,420 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,420 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:07,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,440 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,440 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,441 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,441 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:07,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:07,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:07,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:07,462 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,462 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,462 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:07,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,471 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,479 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,479 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:07,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:07,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:07,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:07,488 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,489 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,489 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:07,490 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,497 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,504 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,505 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:07,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:07,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:07,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:07,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:07,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,530 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,530 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,530 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,530 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:07,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:07,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:07,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:07,539 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,539 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:07,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,548 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,549 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,549 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,549 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:07,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:07,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:07,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:07,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-08 06:41:07,561 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-08 06:41:07,561 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:07,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,564 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,564 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,564 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,564 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:07,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:07,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:07,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:07,565 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,566 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:07,566 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,567 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,567 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,567 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,567 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:07,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:07,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:07,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:07,595 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:07,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:07,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:07,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,601 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,601 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:07,601 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,601 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:07,601 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:07,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:07,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:07,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:07,602 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,602 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,603 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:07,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,610 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 15]), 14)
2023-10-08 06:41:07,611 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,611 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 15]), 14)
2023-10-08 06:41:07,611 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:07,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:07,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:07,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:07,612 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:07,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,626 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,626 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,626 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,626 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:07,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:07,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:07,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:07,634 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:07,636 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,650 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,650 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,650 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,650 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:07,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:07,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:07,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:07,659 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,659 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,660 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:07,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:07,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,675 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:07,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:07,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:07,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:07,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:07,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:07,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:07,698 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,698 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,698 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,698 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:07,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:07,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:07,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:07,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,706 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,707 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:07,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:07,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:07,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,722 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,722 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,722 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:07,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:07,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:07,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:07,730 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,731 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,731 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:07,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:07,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:07,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:07,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:07,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:07,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:07,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:07,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,771 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,771 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,771 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,771 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:07,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:07,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:07,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:07,779 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,779 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,780 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:07,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:07,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,794 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,794 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,795 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,795 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:07,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:07,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:07,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:07,803 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,803 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,803 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:07,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:07,811 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,818 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,818 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,818 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,818 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:07,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:07,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:07,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:07,826 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,827 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,827 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:07,828 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:07,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,842 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,842 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,842 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,842 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:07,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:07,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:07,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:07,850 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,851 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,851 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:07,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:07,859 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,866 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,866 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,866 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,866 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:07,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:07,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:07,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:07,875 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,876 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,876 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:07,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:07,884 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,885 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,885 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,885 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,886 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:07,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:07,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:07,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:07,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-08 06:41:07,895 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-08 06:41:07,895 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:07,896 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:07,897 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,898 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,898 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,898 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,898 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:07,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:07,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:07,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:07,899 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,899 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,899 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:07,900 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:07,900 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,901 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,901 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,901 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:07,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:07,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:07,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:07,928 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:07,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:07,929 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:07,933 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:07,934 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,934 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:07,935 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,935 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:07,935 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:07,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:07,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:07,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:07,936 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,936 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,936 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:07,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:07,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,944 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 16]), 15)
2023-10-08 06:41:07,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:07,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 16]), 15)
2023-10-08 06:41:07,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:07,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:07,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:07,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:07,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:07,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:07,945 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:07,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:07,946 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:07,952 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,959 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,959 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,959 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,959 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:07,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:07,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:07,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:07,967 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:07,967 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:07,967 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:07,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:07,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,982 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:07,982 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,982 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:07,983 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:07,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:07,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:07,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:07,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:07,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:07,991 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:07,991 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:07,992 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:07,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,006 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,006 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,006 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,007 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,007 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:08,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:08,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:08,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:08,015 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,015 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,016 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:08,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,031 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,031 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,031 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,031 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:08,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:08,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:08,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:08,039 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,039 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,040 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:08,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,054 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,055 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:08,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:08,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:08,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:08,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,063 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:08,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,078 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,078 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,078 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,078 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:08,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:08,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:08,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:08,086 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,087 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:08,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,101 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,101 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,102 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,102 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:08,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:08,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:08,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:08,115 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,115 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,116 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:08,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,132 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,132 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,132 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,132 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:08,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:08,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:08,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:08,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,143 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,143 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:08,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,161 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,161 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,161 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,161 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:08,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:08,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:08,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:08,170 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,171 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,171 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:08,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,187 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,188 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,188 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,188 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:08,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:08,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:08,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:08,197 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,198 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,198 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:08,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,213 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,214 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:08,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:08,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:08,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:08,223 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,223 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,223 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:08,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,233 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,234 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,234 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:08,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:08,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:08,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:08,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-08 06:41:08,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-08 06:41:08,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:08,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,246 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,247 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,247 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:08,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:08,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:08,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:08,248 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,249 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,249 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:08,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:08,250 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,251 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,251 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:08,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:08,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:08,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:08,278 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:08,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:08,279 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:08,284 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:08,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:08,286 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:08,286 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,286 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:08,286 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:08,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:08,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:08,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:08,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,287 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,287 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:08,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:08,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:08,296 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 17]), 16)
2023-10-08 06:41:08,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,296 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 17]), 16)
2023-10-08 06:41:08,296 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:08,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:08,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:08,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:08,297 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,298 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,298 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:08,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:08,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:08,312 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,312 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,312 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,312 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:08,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:08,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:08,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:08,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,323 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,323 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:08,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:08,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:08,340 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,340 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,340 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,340 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:08,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:08,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:08,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:08,349 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,350 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,350 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:08,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:08,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,366 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,366 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,366 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:08,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:08,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:08,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:08,376 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,376 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,376 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:08,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,392 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,392 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,393 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,393 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:08,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:08,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:08,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:08,403 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,403 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,404 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:08,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,419 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,420 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,420 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,420 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:08,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:08,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:08,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:08,429 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,430 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,430 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:08,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,445 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,446 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,446 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,446 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:08,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:08,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:08,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:08,455 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:08,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,472 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,473 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:08,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:08,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:08,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:08,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,483 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:08,484 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,499 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:08,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:08,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:08,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:08,511 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,512 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,512 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:08,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,528 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,528 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:08,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:08,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:08,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:08,538 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,539 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,539 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:08,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,548 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,555 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,555 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,555 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,556 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:08,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:08,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:08,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:08,565 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,565 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:08,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,581 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,581 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,582 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:08,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:08,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:08,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:08,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,592 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,592 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:08,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,601 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,602 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,602 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,602 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:08,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:08,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:08,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:08,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-08 06:41:08,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-08 06:41:08,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:08,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,615 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,615 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,615 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,615 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:08,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:08,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:08,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:08,617 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,617 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:08,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,618 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:08,619 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,619 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,619 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,619 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:08,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:08,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:08,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:08,651 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:08,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:08,652 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:08,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:08,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:08,659 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:08,659 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:08,660 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:08,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:08,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:08,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:08,661 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,661 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,662 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:08,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:08,664 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:08,671 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 18]), 17)
2023-10-08 06:41:08,671 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,671 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 18]), 17)
2023-10-08 06:41:08,671 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:08,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:08,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:08,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:08,673 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,673 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,673 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:08,673 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:08,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:08,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,688 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,688 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:08,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:08,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:08,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:08,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,697 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:08,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:08,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:08,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:08,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:08,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:08,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:08,723 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,723 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:08,725 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:08,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,739 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,739 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,740 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,740 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:08,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:08,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:08,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:08,749 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,749 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,749 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:08,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:08,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,766 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,766 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:08,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:08,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:08,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:08,775 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,775 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,776 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:08,777 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:08,785 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,793 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,793 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:08,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:08,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:08,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:08,802 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,803 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,803 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:08,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:08,812 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,820 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,820 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,820 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,821 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:08,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:08,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:08,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:08,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,831 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,831 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:08,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:08,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,848 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,848 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:08,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:08,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:08,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:08,857 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,858 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,858 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:08,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:08,867 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,874 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,874 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,875 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:08,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:08,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:08,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:08,885 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,885 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,885 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:08,887 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:08,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,901 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,901 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,901 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:08,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:08,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:08,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:08,910 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,911 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:08,912 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:08,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,928 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,928 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,929 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,929 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:08,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:08,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:08,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:08,942 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,942 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,943 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:08,944 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:08,951 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,958 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,958 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,959 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,959 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:08,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:08,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:08,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:08,968 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,968 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,969 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:08,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:08,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,978 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,978 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,978 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,978 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:08,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:08,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:08,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:08,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:08,989 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-08 06:41:08,989 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-08 06:41:08,990 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:08,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:08,992 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,993 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,993 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:08,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:08,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:08,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:08,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:08,994 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:08,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:08,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:08,996 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:08,996 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:08,996 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:08,996 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:08,996 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:08,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:09,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:09,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:09,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:09,035 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:09,036 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:09,036 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:09,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:09,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,043 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:09,043 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,043 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:09,043 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:09,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:09,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:09,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:09,044 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,044 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,044 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:09,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 19]), 18)
2023-10-08 06:41:09,053 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,053 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 19]), 18)
2023-10-08 06:41:09,053 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:09,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:09,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:09,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:09,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,055 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,055 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:09,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,069 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,070 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,070 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,070 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:09,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:09,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:09,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:09,079 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,080 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,080 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:09,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,096 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,096 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,097 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,097 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:09,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:09,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:09,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:09,106 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,107 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,107 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:09,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,123 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,123 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,123 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,124 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:09,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:09,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:09,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:09,133 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,134 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,134 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:09,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,150 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:09,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:09,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:09,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:09,160 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,161 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,161 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:09,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,177 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,177 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,177 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:09,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:09,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:09,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:09,187 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,188 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,188 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:09,190 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,204 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,204 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,204 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,205 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:09,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:09,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:09,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:09,215 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,215 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:09,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,231 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,231 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,232 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,232 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:09,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:09,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:09,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:09,241 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,241 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,242 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:09,243 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,256 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,256 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,257 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,257 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:09,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:09,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:09,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:09,265 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,266 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,266 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:09,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,281 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:09,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:09,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:09,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:09,289 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,290 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,290 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:09,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:09,305 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,305 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,305 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,305 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:09,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:09,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:09,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:09,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,314 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,314 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:09,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:09,322 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:09,329 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,329 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,330 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,330 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:09,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:09,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:09,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:09,339 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,339 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,339 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:09,341 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:09,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:09,348 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,349 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,349 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,349 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:09,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:09,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:09,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:09,358 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-08 06:41:09,358 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-08 06:41:09,359 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:09,360 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:09,361 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:09,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,362 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,362 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:09,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:09,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:09,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:09,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,363 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,363 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:09,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:09,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:09,365 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,365 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,365 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,365 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:09,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:09,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:09,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:09,394 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:09,395 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:09,395 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:09,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:09,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,404 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:09,404 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,404 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:09,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:09,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:09,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:09,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:09,405 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,406 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:09,406 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,414 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 20]), 19)
2023-10-08 06:41:09,414 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,414 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 20]), 19)
2023-10-08 06:41:09,414 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:09,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:09,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:09,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:09,415 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,415 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,416 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:09,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,422 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,429 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,429 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,429 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,429 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:09,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:09,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:09,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:09,438 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:09,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,454 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,454 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,454 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,454 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:09,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:09,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:09,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:09,463 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,463 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:09,465 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,478 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,479 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:09,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:09,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:09,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:09,489 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,489 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,489 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:09,491 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,497 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,504 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,504 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:09,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:09,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:09,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:09,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:09,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,529 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,529 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:09,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:09,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:09,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:09,538 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,538 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,538 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:09,540 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,553 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,553 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:09,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:09,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:09,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:09,562 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,563 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,563 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:09,564 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,571 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,577 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,577 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,578 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,578 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:09,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:09,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:09,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:09,590 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,591 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,591 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:09,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,605 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,606 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:09,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:09,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:09,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:09,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,614 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,615 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:09,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,629 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,629 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,629 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,630 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:09,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:09,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:09,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:09,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,638 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,638 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:09,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,646 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:09,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,653 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:09,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:09,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:09,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:09,662 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,662 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,663 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:09,664 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:09,671 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:09,677 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,678 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,678 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,678 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:09,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:09,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:09,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:09,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,687 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,687 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:09,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:09,695 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:09,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,696 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:09,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:09,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:09,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:09,705 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-08 06:41:09,705 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-08 06:41:09,705 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:09,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:09,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:09,708 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,708 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,708 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,709 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:09,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:09,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:09,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:09,709 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,710 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,710 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:09,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:09,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:09,711 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,711 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,712 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,712 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:09,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:09,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:09,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:09,743 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:09,744 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:09,744 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:09,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:09,750 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,751 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:09,751 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,751 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:09,751 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:09,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:09,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:09,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:09,752 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,752 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,752 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:09,753 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:09,753 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,760 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 21]), 20)
2023-10-08 06:41:09,760 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:09,760 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 21]), 20)
2023-10-08 06:41:09,760 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:09,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:09,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:09,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:09,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:09,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:09,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:09,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:09,762 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:09,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,775 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,776 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:09,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:09,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:09,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:09,784 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,784 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,784 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:09,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:09,792 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,799 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,799 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,799 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,799 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:09,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:09,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:09,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:09,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:09,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:09,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,822 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,822 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,823 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,823 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:09,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:09,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:09,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:09,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:09,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:09,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:09,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:09,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:09,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:09,855 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,856 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,856 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:09,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:09,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,871 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:09,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:09,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:09,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:09,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:09,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:09,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,895 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,896 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,896 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,896 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:09,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:09,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:09,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:09,904 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,904 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,905 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:09,906 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:09,912 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,919 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,919 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,920 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,920 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:09,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:09,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:09,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:09,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,929 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:09,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:09,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,944 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:09,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:09,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:09,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:09,952 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,953 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,953 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:09,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:09,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,967 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,967 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,968 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,968 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:09,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:09,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:09,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:09,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:09,976 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:09,976 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:09,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:09,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:09,991 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:09,991 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,991 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:09,991 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:09,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:09,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:09,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:09,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:10,000 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:10,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:10,000 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:10,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:10,008 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,015 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,015 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,015 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,015 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:10,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:10,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:10,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:10,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:10,024 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:10,024 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:10,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,032 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,033 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,033 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,033 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,033 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:10,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:10,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:10,041 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:10,042 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-08 06:41:10,043 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-08 06:41:10,043 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:10,044 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,046 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,046 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,046 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,046 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:10,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:10,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:10,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:10,047 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,047 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,047 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:10,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,048 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,049 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,049 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,049 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,049 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:10,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:10,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:10,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:10,082 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:10,083 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:10,083 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:10,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:10,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,089 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:10,089 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:10,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:10,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:10,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:10,090 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,091 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:10,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 22]), 21)
2023-10-08 06:41:10,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 22]), 21)
2023-10-08 06:41:10,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:10,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:10,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:10,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:10,100 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,100 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,100 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:10,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,107 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,113 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,113 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,114 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,114 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:10,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:10,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:10,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:10,123 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,124 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,124 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:10,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,138 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,139 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:10,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:10,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:10,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:10,147 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,148 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,148 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:10,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,156 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,163 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,163 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,163 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,163 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:10,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:10,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:10,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:10,171 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,172 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,172 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:10,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,186 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,187 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:10,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:10,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:10,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:10,195 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,196 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,196 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:10,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,211 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,211 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,211 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:10,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:10,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:10,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:10,220 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,220 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,220 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:10,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,235 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,235 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,235 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:10,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:10,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:10,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:10,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:10,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,258 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,259 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,259 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,259 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:10,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:10,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:10,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:10,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,268 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,268 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:10,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,283 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,283 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:10,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:10,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:10,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:10,292 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,292 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,292 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:10,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,300 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:10,307 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,307 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,307 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,307 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:10,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:10,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:10,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:10,316 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:10,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:10,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:10,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:10,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:10,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:10,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:10,340 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,340 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,340 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:10,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:10,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,355 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,355 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,355 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,355 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:10,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:10,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:10,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:10,364 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:10,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,373 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,373 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,374 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:10,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:10,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:10,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:10,382 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-08 06:41:10,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-08 06:41:10,382 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:10,384 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,385 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,385 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,385 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,385 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:10,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:10,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:10,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:10,386 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,386 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,387 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:10,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,388 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,388 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,388 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:10,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:10,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:10,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:10,418 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:10,419 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:10,419 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:10,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,425 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:10,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:10,425 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:10,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:10,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:10,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:10,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,427 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,427 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:10,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,434 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 23]), 22)
2023-10-08 06:41:10,435 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,435 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 23]), 22)
2023-10-08 06:41:10,435 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:10,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:10,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:10,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:10,436 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,436 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,436 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:10,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,449 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,450 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,450 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,450 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:10,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:10,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:10,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:10,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,459 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:10,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,467 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,473 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,473 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:10,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:10,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:10,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:10,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,483 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:10,484 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,491 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,497 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,497 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,497 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,498 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:10,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:10,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:10,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:10,506 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,506 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,506 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:10,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,521 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,521 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,521 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,521 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:10,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:10,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:10,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:10,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,530 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,530 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:10,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,545 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,545 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,546 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:10,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:10,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:10,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:10,554 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,555 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:10,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:10,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:10,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:10,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:10,579 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,579 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,580 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:10,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,595 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,595 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,595 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,595 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:10,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:10,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:10,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:10,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,605 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:10,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,619 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,620 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,620 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,620 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:10,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:10,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:10,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:10,629 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,630 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,630 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:10,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:10,645 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,645 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,645 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,645 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:10,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:10,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:10,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:10,654 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,654 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,654 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:10,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:10,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:10,669 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,669 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,669 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:10,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:10,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:10,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:10,678 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,679 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,679 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:10,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:10,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,693 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,693 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,694 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:10,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:10,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:10,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:10,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,703 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:10,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:10,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,712 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,712 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,713 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:10,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:10,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:10,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:10,721 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-08 06:41:10,721 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-08 06:41:10,721 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:10,723 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:10,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,724 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,724 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,725 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,725 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:10,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:10,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:10,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:10,726 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,726 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,726 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:10,726 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:10,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,728 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,728 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:10,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:10,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:10,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:10,757 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:10,758 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:10,758 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:10,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:10,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,764 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:10,764 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,764 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:10,764 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:10,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:10,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:10,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:10,765 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,766 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,766 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:10,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:10,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 24]), 23)
2023-10-08 06:41:10,774 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:10,774 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 24]), 23)
2023-10-08 06:41:10,774 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:10,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:10,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:10,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:10,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:10,775 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:10,775 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:10,775 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:10,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:10,782 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,789 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,789 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,790 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,790 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:10,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:10,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:10,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:10,800 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,800 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,800 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:10,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:10,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,815 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,815 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,816 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,816 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:10,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:10,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:10,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:10,825 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,825 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,825 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:10,827 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:10,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,840 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,840 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,840 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,840 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:10,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:10,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:10,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:10,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,850 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:10,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:10,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,865 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,865 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,865 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,865 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:10,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:10,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:10,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:10,873 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,874 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:10,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:10,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,889 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,889 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:10,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:10,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:10,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:10,898 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,899 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,899 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:10,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:10,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,914 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,914 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,914 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,914 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:10,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:10,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:10,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:10,924 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,924 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,924 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:10,925 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:10,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,939 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,939 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,940 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,940 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:10,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:10,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:10,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:10,948 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,949 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,949 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:10,951 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:10,957 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,964 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,964 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,964 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,964 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:10,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:10,970 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:10,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:10,973 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:10,974 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:10,974 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:10,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:10,982 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:10,989 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:10,989 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,989 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:10,990 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:10,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:10,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:10,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:10,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:10,999 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:11,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:11,000 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:11,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:11,008 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,015 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,015 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,015 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,015 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:11,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:11,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:11,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:11,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:11,024 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:11,024 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:11,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,032 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,038 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,039 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,039 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,039 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:11,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:11,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:11,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:11,050 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:11,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:11,051 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:11,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,059 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,060 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,060 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,060 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,060 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:11,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:11,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:11,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:11,068 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-08 06:41:11,069 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-08 06:41:11,069 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:11,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,072 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,072 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:11,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:11,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:11,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:11,073 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,073 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:11,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,075 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,075 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,075 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,075 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:11,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:11,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:11,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:11,105 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:11,106 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:11,106 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:11,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,112 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:11,112 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,112 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:11,112 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:11,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:11,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:11,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:11,113 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,113 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,114 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:11,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,115 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,121 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 25]), 24)
2023-10-08 06:41:11,121 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,121 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 25]), 24)
2023-10-08 06:41:11,122 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:11,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:11,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:11,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:11,123 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,123 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,123 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:11,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,137 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,137 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:11,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:11,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:11,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:11,146 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,146 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,147 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:11,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,161 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,162 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,162 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,162 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:11,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:11,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:11,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:11,170 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,171 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,171 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:11,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,186 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,186 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,186 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:11,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:11,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:11,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:11,195 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,196 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,196 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:11,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,211 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,211 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,211 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:11,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:11,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:11,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:11,220 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,221 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,221 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:11,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,236 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,236 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,236 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:11,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:11,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:11,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:11,245 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,246 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,246 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:11,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,260 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,260 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,261 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,261 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:11,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:11,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:11,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:11,269 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,270 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,270 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:11,271 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:11,284 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,285 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,285 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,285 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,285 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:11,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:11,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:11,292 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:11,293 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,294 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,294 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:11,295 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:11,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:11,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:11,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:11,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:11,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:11,317 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,318 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:11,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:11,326 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:11,333 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,333 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,333 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,333 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:11,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:11,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:11,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:11,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,342 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,342 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:11,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:11,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,357 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,357 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,357 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:11,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:11,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:11,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:11,366 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,367 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,367 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:11,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,375 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,381 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,382 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,382 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,382 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:11,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:11,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:11,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:11,390 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,390 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,391 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:11,392 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,400 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,400 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,400 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:11,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:11,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:11,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:11,409 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-08 06:41:11,410 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-08 06:41:11,410 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:11,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,413 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,413 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,413 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,413 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:11,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:11,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:11,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:11,414 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,414 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,415 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:11,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,416 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,416 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,416 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,416 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:11,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:11,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:11,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:11,445 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:11,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:11,446 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:11,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,451 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:11,452 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,452 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:11,452 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:11,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:11,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:11,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:11,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,453 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,453 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:11,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,454 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,461 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 26]), 25)
2023-10-08 06:41:11,461 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 26]), 25)
2023-10-08 06:41:11,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:11,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:11,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:11,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:11,462 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,463 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,463 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:11,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,469 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,476 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,476 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,476 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,476 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:11,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:11,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:11,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:11,485 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,486 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,486 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:11,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,494 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,501 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,501 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:11,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:11,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:11,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:11,510 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,510 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:11,512 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,519 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,525 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,525 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,526 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,526 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:11,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:11,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:11,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:11,535 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,536 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,536 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:11,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,551 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,551 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,551 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,551 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:11,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:11,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:11,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:11,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,561 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,561 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:11,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,576 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,576 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,576 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,577 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:11,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:11,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:11,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:11,586 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,587 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,587 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:11,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,595 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,602 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,602 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,602 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,603 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:11,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:11,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:11,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:11,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:11,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:11,627 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,627 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,627 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,627 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:11,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:11,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:11,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:11,635 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,636 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,636 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:11,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:11,644 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:11,651 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,651 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,651 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,651 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:11,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:11,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:11,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:11,661 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,662 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,662 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:11,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:11,669 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:11,676 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,676 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,677 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,677 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:11,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:11,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:11,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:11,685 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,685 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,685 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:11,687 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:11,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,700 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,700 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,700 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,700 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:11,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:11,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:11,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:11,710 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,711 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,711 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:11,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:11,719 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,726 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,726 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,726 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,726 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:11,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:11,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:11,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:11,736 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,736 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,736 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:11,738 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:11,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,745 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:11,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:11,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:11,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:11,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-08 06:41:11,755 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-08 06:41:11,755 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:11,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:11,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,758 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,758 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,759 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,759 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:11,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:11,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:11,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:11,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,765 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,765 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:11,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:11,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,767 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,767 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,767 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,767 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:11,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:11,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:11,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:11,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:11,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:11,797 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:11,801 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:11,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,803 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:11,803 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,803 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:11,803 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:11,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:11,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:11,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:11,804 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,804 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,804 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:11,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:11,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,812 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 27]), 26)
2023-10-08 06:41:11,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:11,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 27]), 26)
2023-10-08 06:41:11,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:11,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:11,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:11,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:11,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:11,814 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:11,814 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:11,814 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:11,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:11,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,827 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,828 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,828 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,828 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:11,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:11,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:11,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:11,837 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,837 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,838 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:11,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:11,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,852 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,853 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,853 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:11,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:11,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:11,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:11,864 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,864 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,864 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:11,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:11,873 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,879 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,880 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,880 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,880 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:11,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:11,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:11,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:11,889 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,890 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,890 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:11,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:11,898 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,905 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,905 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,905 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,905 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:11,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:11,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:11,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:11,914 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,914 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,914 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:11,916 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:11,922 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,929 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,930 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,930 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,930 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:11,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:11,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:11,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:11,938 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,939 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,939 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:11,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:11,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,956 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,956 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,957 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,957 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:11,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:11,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:11,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:11,969 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,969 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,969 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:11,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:11,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:11,984 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:11,984 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,984 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:11,984 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:11,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:11,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:11,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:11,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:11,993 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:11,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:11,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:11,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:12,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,008 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,008 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,008 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,008 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:12,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:12,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:12,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:12,017 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:12,017 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:12,017 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:12,018 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,032 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,032 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,032 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,032 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:12,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:12,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:12,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:12,040 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:12,041 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:12,041 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:12,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,056 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,056 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,056 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,056 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:12,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:12,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:12,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:12,065 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:12,065 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:12,065 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:12,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,080 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,080 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,080 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,080 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:12,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:12,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:12,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:12,089 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:12,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:12,089 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:12,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:12,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:12,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:12,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:12,107 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-08 06:41:12,107 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-08 06:41:12,108 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:12,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,110 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,111 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,111 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,111 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:12,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:12,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:12,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:12,112 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,112 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,112 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:12,112 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,113 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,113 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,114 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,114 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,114 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:12,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:12,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:12,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:12,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:12,144 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:12,145 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:12,149 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:12,151 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:12,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:12,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:12,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:12,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:12,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,152 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:12,152 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,159 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 28]), 27)
2023-10-08 06:41:12,160 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,160 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 28]), 27)
2023-10-08 06:41:12,160 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:12,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:12,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:12,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:12,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,161 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,161 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:12,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,174 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,175 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,175 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:12,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:12,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:12,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:12,184 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,184 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,184 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:12,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,199 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,199 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,199 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,199 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:12,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:12,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:12,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:12,208 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,209 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,209 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:12,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,224 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,224 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,224 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:12,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:12,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:12,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:12,232 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,233 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,233 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:12,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:12,247 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,248 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,248 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,248 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:12,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:12,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:12,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:12,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:12,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:12,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:12,271 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,272 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,272 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,272 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:12,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:12,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:12,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:12,280 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,281 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,281 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:12,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:12,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:12,295 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,295 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,296 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,296 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:12,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:12,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:12,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:12,306 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,307 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,307 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:12,308 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:12,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:12,322 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,322 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,322 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,322 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:12,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:12,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:12,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:12,331 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,331 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,331 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:12,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:12,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:12,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:12,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:12,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:12,354 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,355 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,355 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:12,356 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,369 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,369 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,370 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,370 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:12,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:12,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:12,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:12,378 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,379 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:12,380 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,393 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,394 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,394 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,394 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:12,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:12,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:12,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:12,402 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,403 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,403 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:12,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,417 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,417 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,418 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,418 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:12,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:12,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:12,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:12,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,427 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,427 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:12,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,435 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,436 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,436 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,436 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,436 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:12,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:12,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:12,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:12,445 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-08 06:41:12,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-08 06:41:12,445 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:12,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,448 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,448 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,448 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,448 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,449 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:12,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:12,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:12,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:12,449 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,450 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,450 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:12,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,451 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,451 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,451 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,452 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:12,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:12,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:12,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:12,500 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:12,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:12,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:12,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:12,523 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,523 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:12,523 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:12,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:12,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:12,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:12,525 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,525 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,525 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:12,525 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,533 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 29]), 28)
2023-10-08 06:41:12,533 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,533 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 29]), 28)
2023-10-08 06:41:12,533 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:12,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:12,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:12,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:12,534 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,534 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,535 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:12,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,548 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,548 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,548 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,548 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:12,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:12,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:12,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:12,556 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,557 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,557 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:12,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,565 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,572 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,572 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,572 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,572 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:12,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:12,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:12,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:12,581 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,581 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,581 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:12,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,596 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,596 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,596 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,596 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:12,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:12,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:12,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:12,605 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,605 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,605 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:12,607 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:12,620 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,620 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,620 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,620 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:12,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:12,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:12,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:12,629 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,630 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,630 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:12,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:12,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:12,644 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,645 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,645 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,645 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:12,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:12,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:12,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:12,653 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,653 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,654 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:12,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:12,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:12,668 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,668 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,668 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:12,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:12,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:12,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:12,677 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,677 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,677 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:12,679 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:12,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:12,692 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,692 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,692 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:12,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:12,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:12,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:12,701 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,701 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,701 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:12,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:12,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,716 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,716 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,717 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:12,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:12,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:12,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:12,726 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,727 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,727 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:12,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:12,735 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,741 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,741 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,742 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,742 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:12,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:12,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:12,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:12,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,763 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,763 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:12,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:12,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,785 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,785 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,785 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,785 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:12,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:12,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:12,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:12,795 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,795 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:12,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:12,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:12,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:12,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:12,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:12,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,823 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,823 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:12,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:12,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,832 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,832 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,832 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,832 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:12,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:12,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:12,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:12,843 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-08 06:41:12,843 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-08 06:41:12,843 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:12,845 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:12,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,846 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:12,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:12,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:12,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:12,852 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,853 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,853 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:12,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:12,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,855 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,855 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,855 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,855 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:12,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:12,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:12,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:12,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:12,895 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:12,895 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:12,900 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:12,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:12,901 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,902 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:12,902 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:12,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:12,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:12,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:12,903 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:12,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:12,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,911 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 30]), 29)
2023-10-08 06:41:12,911 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:12,911 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 30]), 29)
2023-10-08 06:41:12,911 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:12,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:12,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:12,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:12,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:12,912 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:12,913 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:12,913 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:12,913 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:12,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,926 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,926 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,926 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:12,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:12,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:12,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:12,937 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:12,938 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:12,938 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:12,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:12,946 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,953 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,953 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,953 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,953 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:12,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:12,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:12,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:12,964 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:12,965 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:12,965 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:12,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:12,973 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,980 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:12,980 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,980 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:12,980 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:12,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:12,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:12,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:12,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:12,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:12,990 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:12,990 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:12,992 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:12,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,005 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,005 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,006 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,006 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:13,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:13,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:13,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:13,015 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,016 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,016 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:13,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,031 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,031 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,031 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,031 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:13,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:13,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:13,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:13,041 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,042 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,042 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:13,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,050 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,057 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,057 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,057 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,057 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:13,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:13,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:13,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:13,066 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,067 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,067 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:13,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,082 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:13,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:13,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:13,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:13,090 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,091 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:13,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,106 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,106 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:13,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:13,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:13,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:13,115 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,115 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,115 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:13,117 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,130 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,130 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,131 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,131 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:13,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:13,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:13,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:13,140 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,140 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,140 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:13,142 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,155 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,155 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,155 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,156 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:13,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:13,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:13,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:13,165 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,165 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,166 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:13,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,180 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,181 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,181 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,181 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:13,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:13,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:13,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:13,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,191 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,191 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:13,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,200 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,200 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,200 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,200 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:13,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:13,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:13,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:13,209 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-08 06:41:13,211 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-08 06:41:13,211 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:13,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,213 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,214 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,214 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,214 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:13,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:13,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:13,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:13,215 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,215 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:13,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,217 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,217 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,217 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,217 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:13,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:13,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:13,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:13,248 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:13,249 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:13,249 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:13,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,255 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:13,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:13,256 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:13,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:13,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:13,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:13,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:13,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 31]), 30)
2023-10-08 06:41:13,265 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 31]), 30)
2023-10-08 06:41:13,265 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:13,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:13,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:13,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:13,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,267 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,267 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:13,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,280 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,280 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,280 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:13,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:13,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:13,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:13,289 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,290 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,290 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:13,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:13,305 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,305 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,305 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,305 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:13,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:13,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:13,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:13,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,314 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,314 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:13,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:13,323 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:13,329 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,329 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,329 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,330 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:13,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:13,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:13,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:13,338 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,339 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,339 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:13,341 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:13,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,354 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,354 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,354 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,354 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:13,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:13,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:13,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:13,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:13,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,379 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,379 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:13,383 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:13,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:13,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:13,389 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,389 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,389 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:13,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,404 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,404 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,405 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:13,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:13,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:13,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:13,414 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,415 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,415 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:13,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,429 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,430 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,430 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:13,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:13,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:13,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:13,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:13,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,454 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,454 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,455 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,455 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:13,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:13,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:13,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:13,463 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:13,465 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,479 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,479 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:13,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:13,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:13,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:13,487 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,488 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,488 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:13,490 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,503 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,503 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,503 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,503 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:13,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:13,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:13,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:13,512 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,512 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,512 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:13,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,527 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,527 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,528 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:13,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:13,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:13,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:13,536 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,536 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,537 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:13,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,546 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,546 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,546 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:13,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:13,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:13,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:13,555 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-08 06:41:13,556 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-08 06:41:13,556 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:13,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,558 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,559 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,559 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,559 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,559 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:13,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:13,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:13,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:13,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,560 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,560 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:13,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,562 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,562 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,562 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,562 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:13,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:13,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:13,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:13,593 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:13,594 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:13,594 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:13,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,600 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:13,600 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,600 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:13,600 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:13,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:13,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:13,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:13,601 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,601 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,601 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:13,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,609 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 32]), 31)
2023-10-08 06:41:13,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,609 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 32]), 31)
2023-10-08 06:41:13,609 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:13,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:13,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:13,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:13,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,611 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,611 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:13,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,618 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,624 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,625 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,625 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,625 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:13,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:13,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:13,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:13,634 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:13,637 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:13,650 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,650 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,650 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,650 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:13,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:13,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:13,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:13,659 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,660 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,660 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:13,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:13,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:13,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,675 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:13,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:13,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:13,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:13,684 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:13,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:13,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,699 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,699 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,699 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,700 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:13,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:13,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:13,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:13,709 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,709 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,710 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:13,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:13,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,724 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,725 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,725 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,725 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:13,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:13,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:13,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:13,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,734 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:13,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:13,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,749 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,749 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,749 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,749 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:13,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:13,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:13,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:13,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,758 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,758 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:13,760 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:13,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,773 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,773 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,774 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:13,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:13,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:13,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:13,782 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,783 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,783 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:13,785 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:13,791 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,798 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,798 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,798 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,798 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:13,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:13,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:13,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:13,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,807 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:13,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:13,816 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,822 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,823 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,823 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,823 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:13,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:13,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:13,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:13,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:13,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:13,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:13,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:13,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:13,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:13,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,856 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,856 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:13,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:13,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,871 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:13,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:13,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:13,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:13,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:13,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:13,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,890 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,890 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:13,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:13,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:13,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:13,898 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-08 06:41:13,899 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-08 06:41:13,899 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:13,900 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:13,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,902 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,902 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,902 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,902 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:13,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:13,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:13,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:13,903 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:13,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:13,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,905 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,905 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,905 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,905 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:13,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:13,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:13,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:13,934 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:13,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:13,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:13,940 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:13,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,942 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:13,942 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,942 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:13,943 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:13,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:13,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:13,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:13,944 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,944 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,944 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:13,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:13,946 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,952 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 33]), 32)
2023-10-08 06:41:13,953 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:13,953 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 33]), 32)
2023-10-08 06:41:13,953 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:13,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:13,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:13,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:13,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:13,954 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:13,954 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:13,955 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:13,955 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:13,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,968 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,968 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,968 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,968 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:13,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:13,973 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:13,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:13,977 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:13,978 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:13,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:13,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:13,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:13,992 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:13,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:13,993 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:13,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:13,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:13,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:14,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:14,002 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,003 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,003 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:14,004 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:14,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,018 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,018 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,018 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,018 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:14,021 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:14,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:14,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:14,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:14,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,036 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,042 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,043 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,043 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,043 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:14,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:14,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:14,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:14,052 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,052 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,052 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:14,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,067 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,067 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,068 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:14,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:14,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:14,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:14,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:14,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,091 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,091 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:14,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:14,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:14,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:14,100 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,101 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:14,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,115 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,115 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,116 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,116 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:14,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:14,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:14,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:14,124 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,125 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:14,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:14,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:14,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:14,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:14,148 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,149 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,149 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:14,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,156 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,163 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,163 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,164 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,164 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:14,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:14,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:14,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:14,174 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,175 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,175 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:14,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,189 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:14,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:14,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:14,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:14,198 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,199 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:14,200 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,213 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,214 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:14,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:14,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:14,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:14,223 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,224 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,224 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:14,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,233 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,233 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,233 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:14,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:14,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:14,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:14,242 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-08 06:41:14,242 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-08 06:41:14,243 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:14,244 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:14,245 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,245 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,246 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:14,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:14,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:14,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:14,247 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,247 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,247 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:14,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:14,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:14,248 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,249 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,249 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,249 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:14,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:14,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:14,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:14,278 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:14,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:14,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:14,284 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:14,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:14,285 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:14,286 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,286 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:14,286 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:14,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:14,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:14,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:14,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,287 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,287 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:14,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:14,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:14,295 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 34]), 33)
2023-10-08 06:41:14,295 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,295 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 34]), 33)
2023-10-08 06:41:14,295 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:14,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:14,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:14,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:14,296 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,296 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,296 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:14,297 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:14,303 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:14,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,310 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,310 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,310 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:14,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:14,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:14,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:14,319 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,319 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,320 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:14,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:14,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:14,334 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,334 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,335 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,335 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:14,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:14,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:14,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:14,344 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,344 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:14,346 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:14,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,359 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,359 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,359 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,359 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:14,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:14,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:14,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:14,368 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,369 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,369 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:14,371 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,384 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,384 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,384 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,384 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:14,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:14,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:14,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:14,393 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,393 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,393 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:14,395 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,401 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,408 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:14,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:14,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:14,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:14,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,448 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,448 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:14,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,463 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,463 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:14,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:14,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:14,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:14,472 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,472 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,473 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:14,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,487 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,487 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:14,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:14,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:14,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:14,496 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,496 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,496 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:14,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,504 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,511 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,511 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,511 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,511 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:14,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:14,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:14,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:14,519 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,520 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,520 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:14,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,528 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,534 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,534 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,534 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,534 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:14,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:14,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:14,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:14,543 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,543 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,543 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:14,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,558 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,558 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,558 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,558 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:14,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:14,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:14,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:14,567 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,568 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:14,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,575 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,582 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,582 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,583 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:14,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:14,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:14,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:14,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,592 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,592 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:14,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,600 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,600 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,601 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,601 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:14,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:14,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:14,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:14,609 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-08 06:41:14,609 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-08 06:41:14,610 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:14,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:14,612 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,613 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,613 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:14,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:14,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:14,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:14,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,614 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,614 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:14,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:14,615 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:14,615 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,615 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,616 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,616 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:14,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:14,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:14,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:14,645 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:14,646 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:14,646 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:14,650 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:14,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:14,652 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:14,652 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,652 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:14,652 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:14,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:14,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:14,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:14,654 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,654 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,654 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:14,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:14,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:14,663 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 35]), 34)
2023-10-08 06:41:14,663 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,663 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 35]), 34)
2023-10-08 06:41:14,663 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:14,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:14,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:14,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:14,664 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:14,664 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:14,664 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:14,665 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:14,671 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:14,677 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,678 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,678 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,678 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:14,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:14,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:14,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:14,693 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,694 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,694 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:14,696 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:14,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:14,717 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,719 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,719 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,719 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:14,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:14,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:14,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:14,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,734 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:14,737 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:14,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,754 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,754 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,755 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,755 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:14,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:14,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:14,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:14,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,765 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,765 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:14,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:14,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,781 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:14,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:14,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:14,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:14,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,794 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:14,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:14,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:14,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:14,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:14,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:14,821 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,821 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:14,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:14,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,838 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,838 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,838 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,839 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:14,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:14,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:14,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:14,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,850 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:14,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:14,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,865 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,865 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,865 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,866 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:14,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:14,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:14,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:14,875 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,875 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:14,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:14,884 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,891 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,891 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,891 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,891 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:14,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:14,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:14,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:14,900 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,901 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,901 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:14,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:14,909 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,916 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,916 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,916 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,916 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:14,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:14,921 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:14,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:14,926 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,927 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,927 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:14,928 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:14,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,942 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,943 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,943 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,943 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:14,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:14,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:14,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:14,951 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,952 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,952 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:14,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:14,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,966 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,966 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,966 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,966 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:14,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:14,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:14,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:14,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,976 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,976 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:14,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:14,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,985 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,985 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,986 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,986 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:14,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:14,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:14,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:14,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:14,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-08 06:41:14,995 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-08 06:41:14,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:14,996 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:14,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:14,998 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:14,998 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:14,998 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:14,998 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:14,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:15,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:15,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:15,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:15,001 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,002 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,002 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:15,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:15,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,003 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,004 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,004 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,004 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:15,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:15,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:15,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:15,034 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:15,036 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:15,037 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:15,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,044 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,044 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:15,045 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,045 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:15,045 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:15,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:15,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:15,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:15,046 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,046 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,046 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:15,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,054 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 36]), 35)
2023-10-08 06:41:15,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,054 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 36]), 35)
2023-10-08 06:41:15,054 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:15,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:15,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:15,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:15,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,055 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,055 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:15,056 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,068 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,068 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,068 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,069 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:15,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:15,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:15,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:15,078 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,079 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,079 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:15,081 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,087 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,093 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,093 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,093 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,094 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:15,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:15,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:15,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:15,102 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,103 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,103 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:15,105 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,118 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,118 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,118 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,118 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:15,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:15,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:15,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:15,127 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,127 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,127 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:15,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,142 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:15,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:15,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:15,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:15,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,152 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:15,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,167 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,167 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:15,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:15,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:15,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:15,180 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,181 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,181 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:15,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,200 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,200 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,200 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,201 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:15,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:15,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:15,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:15,211 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,212 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,212 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:15,213 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,220 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,226 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,226 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,227 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,227 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:15,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:15,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:15,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:15,235 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,236 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:15,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,252 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:15,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:15,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:15,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:15,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:15,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:15,276 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,276 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,276 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,276 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:15,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:15,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:15,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:15,286 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,286 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,286 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:15,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:15,294 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:15,301 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,301 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,301 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,301 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:15,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:15,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:15,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:15,310 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,311 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:15,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:15,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:15,327 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,327 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,327 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:15,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:15,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:15,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:15,336 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,336 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,336 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:15,338 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:15,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:15,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:15,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:15,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:15,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:15,357 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-08 06:41:15,358 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-08 06:41:15,358 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:15,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:15,360 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:15,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,361 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,361 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:15,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:15,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:15,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:15,362 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,362 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,363 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:15,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:15,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,364 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,364 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,364 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,364 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:15,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:15,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:15,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:15,397 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:15,398 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:15,398 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:15,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,405 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:15,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,405 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:15,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:15,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:15,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:15,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:15,406 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,406 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:15,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,414 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 37]), 36)
2023-10-08 06:41:15,414 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,414 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 37]), 36)
2023-10-08 06:41:15,414 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:15,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:15,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:15,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:15,415 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,415 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,416 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:15,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,429 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,429 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,429 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,430 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:15,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:15,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:15,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:15,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,440 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:15,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,448 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,455 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,455 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,455 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,455 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:15,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:15,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:15,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:15,464 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:15,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,480 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,480 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,480 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,480 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:15,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:15,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:15,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:15,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,492 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:15,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,507 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,507 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,507 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,507 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:15,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:15,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:15,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:15,516 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,516 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,516 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:15,518 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,525 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,533 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,533 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,533 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,533 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:15,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:15,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:15,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:15,543 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,543 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,543 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:15,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,558 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,558 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,559 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,559 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:15,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:15,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:15,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:15,570 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,570 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,571 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:15,572 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,578 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,586 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,586 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,586 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,586 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:15,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:15,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:15,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:15,596 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,596 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:15,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,611 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,612 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,612 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:15,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:15,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:15,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:15,621 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,622 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,622 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:15,623 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,630 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:15,637 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,637 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:15,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:15,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:15,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:15,647 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,648 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:15,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:15,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:15,663 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,663 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,663 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,663 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:15,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:15,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:15,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:15,673 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,674 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,674 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:15,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:15,681 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:15,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,689 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:15,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:15,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:15,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:15,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,697 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:15,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:15,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:15,707 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,707 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,707 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:15,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:15,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:15,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:15,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-08 06:41:15,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-08 06:41:15,716 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:15,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:15,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:15,719 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,719 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,719 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,719 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:15,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:15,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:15,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:15,720 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,720 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,720 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:15,721 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:15,721 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,722 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,722 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,722 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:15,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:15,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:15,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:15,752 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:15,753 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:15,753 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:15,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:15,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,760 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:15,760 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,760 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:15,760 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:15,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:15,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:15,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:15,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,761 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,761 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:15,762 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:15,762 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,769 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 38]), 37)
2023-10-08 06:41:15,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:15,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 38]), 37)
2023-10-08 06:41:15,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:15,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:15,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:15,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:15,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:15,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:15,771 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:15,771 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:15,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:15,778 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,785 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,785 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,785 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,785 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:15,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:15,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:15,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:15,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,795 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:15,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:15,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,809 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,809 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,810 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:15,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:15,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:15,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:15,819 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,819 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,820 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:15,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:15,828 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,835 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,835 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,835 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,835 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:15,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:15,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:15,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:15,843 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,844 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,844 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:15,845 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:15,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,859 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,859 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,859 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,859 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,859 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:15,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:15,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:15,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:15,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,868 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:15,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:15,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,883 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,884 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:15,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:15,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:15,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:15,893 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,894 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,894 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:15,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:15,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,908 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,908 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,908 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,908 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:15,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:15,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:15,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:15,918 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,919 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,919 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:15,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:15,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,934 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,934 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,934 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,934 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:15,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:15,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:15,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:15,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,946 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,946 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:15,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:15,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,961 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,961 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,961 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,961 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:15,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:15,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:15,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:15,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,971 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:15,972 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:15,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:15,986 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:15,986 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,987 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:15,987 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:15,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:15,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:15,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:15,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:15,995 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:15,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:15,996 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:15,998 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:16,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:16,011 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,011 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,011 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,012 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:16,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:16,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:16,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:16,034 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:16,035 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:16,035 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:16,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:16,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:16,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,054 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,054 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:16,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:16,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:16,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:16,065 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:16,066 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:16,066 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:16,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:16,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:16,075 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,076 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,076 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,076 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:16,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:16,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:16,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:16,087 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-08 06:41:16,088 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-08 06:41:16,088 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:16,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:16,090 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:16,091 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,091 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,091 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:16,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:16,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:16,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:16,092 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:16,093 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:16,093 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:16,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:16,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:16,094 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,094 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,095 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,095 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:16,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:16,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:16,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:16,129 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:16,130 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:16,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:16,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:16,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:16,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 06:41:16,137 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,137 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 06:41:16,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-08 06:41:16,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-08 06:41:16,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-08 06:41:16,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-08 06:41:16,138 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:16,138 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:16,138 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-08 06:41:16,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-08 06:41:16,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:16,146 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 39]), 38)
2023-10-08 06:41:16,146 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,147 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 39]), 38)
2023-10-08 06:41:16,147 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-08 06:41:16,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-08 06:41:16,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-08 06:41:16,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-08 06:41:16,148 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:16,148 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:16,148 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-08 06:41:16,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-08 06:41:16,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:16,162 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,162 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,162 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,163 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-08 06:41:16,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-08 06:41:16,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-08 06:41:16,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-08 06:41:16,172 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,172 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,173 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-08 06:41:16,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-08 06:41:16,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:16,193 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,193 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,193 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,193 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-08 06:41:16,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-08 06:41:16,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-08 06:41:16,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-08 06:41:16,203 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,204 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,204 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-08 06:41:16,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-08 06:41:16,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:16,219 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,219 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,219 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,219 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-08 06:41:16,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-08 06:41:16,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-08 06:41:16,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-08 06:41:16,229 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,230 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,230 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-08 06:41:16,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-08 06:41:16,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:16,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,246 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-08 06:41:16,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-08 06:41:16,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-08 06:41:16,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-08 06:41:16,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-08 06:41:16,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-08 06:41:16,264 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:16,271 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,271 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,271 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,271 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-08 06:41:16,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-08 06:41:16,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-08 06:41:16,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-08 06:41:16,280 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,281 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,281 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-08 06:41:16,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-08 06:41:16,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:16,296 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,296 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,296 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-08 06:41:16,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-08 06:41:16,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-08 06:41:16,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-08 06:41:16,305 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,305 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,305 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-08 06:41:16,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-08 06:41:16,314 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:16,321 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,321 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,321 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,321 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-08 06:41:16,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-08 06:41:16,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-08 06:41:16,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-08 06:41:16,332 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,333 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,333 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-08 06:41:16,334 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-08 06:41:16,341 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:16,348 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,348 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,348 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,348 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-08 06:41:16,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-08 06:41:16,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-08 06:41:16,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-08 06:41:16,358 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,359 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,359 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-08 06:41:16,360 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-08 06:41:16,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:16,375 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,375 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,375 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,375 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-08 06:41:16,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-08 06:41:16,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-08 06:41:16,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-08 06:41:16,384 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,384 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-08 06:41:16,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-08 06:41:16,392 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:16,399 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,399 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,399 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-08 06:41:16,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-08 06:41:16,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-08 06:41:16,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-08 06:41:16,409 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,410 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,410 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-08 06:41:16,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-08 06:41:16,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:16,424 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,424 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,425 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-08 06:41:16,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-08 06:41:16,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-08 06:41:16,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-08 06:41:16,435 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,436 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,436 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-08 06:41:16,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-08 06:41:16,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:16,444 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,444 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,445 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,445 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-08 06:41:16,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-08 06:41:16,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-08 06:41:16,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-08 06:41:16,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-08 06:41:16,454 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-08 06:41:16,455 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-08 06:41:16,455 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-08 06:41:16,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-08 06:41:16,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:16,458 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,458 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,458 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,458 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-08 06:41:16,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-08 06:41:16,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-08 06:41:16,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-08 06:41:16,459 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-08 06:41:16,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-08 06:41:16,459 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-08 06:41:16,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 06:41:16,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-08 06:41:16,461 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-08 06:41:16,461 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 06:41:16,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-08 06:41:16,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 06:41:16,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 06:41:16,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 06:41:16,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 06:41:16,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 06:41:16,492 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-08 06:41:16,493 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-08 06:41:16,493 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 06:41:16,502 [flexgen_test.py:39 in test_hf_gen] INFO - for i in range(10):                                
2023-10-08 06:41:16,502 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,502 [flexgen_test.py:39 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm not
2023-10-08 06:41:16,502 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - for i in range(10):                                
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm not
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,503 [flexgen_test.py:39 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that
2023-10-08 06:41:16,503 [flexgen_test.py:40 in test_hf_gen] INFO - ----------
2023-10-08 06:41:16,512 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-08 06:41:16,512 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-08 06:41:16,513 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-08 06:41:16,514 [flexgen_forward.py:22 in to_old_forward] DEBUG - lm_head from flexgen to old.
