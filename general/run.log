2023-10-30 05:34:57,835 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp_iixdcjf
2023-10-30 05:34:57,836 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp_iixdcjf/_remote_module_non_scriptable.py
2023-10-30 05:34:58,310 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-30 05:34:58,373 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:34:59,905 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-30 05:35:00,200 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 05:35:00,200 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 05:35:00,200 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 05:35:00,200 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 05:35:01,244 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:35:01,337 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-30 05:35:01,337 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-30 05:35:01,378 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:35:01,471 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-30 05:35:01,475 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-30 05:35:01,475 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-30 05:35:01,476 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-30 05:35:01,477 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-30 05:35:01,478 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-30 05:35:01,479 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-30 05:35:01,480 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-30 05:35:01,481 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-30 05:35:01,482 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-30 05:35:01,482 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-30 05:35:01,483 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-30 05:35:01,484 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-30 05:35:01,485 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-30 05:35:01,486 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-30 05:35:01,487 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 05:35:01,487 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 05:35:01,488 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-30 05:35:01,490 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-30 05:35:01,529 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 05:35:01,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-30 05:35:01,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-30 05:35:01,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-30 05:35:01,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-30 05:35:01,707 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-30 05:35:01,708 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-30 05:35:01,709 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-30 05:35:01,709 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-30 05:35:01,709 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-30 05:35:01,712 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:01,714 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-30 05:35:01,715 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:01,716 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-30 05:35:01,716 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:01,728 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-30 05:35:01,732 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:01,738 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-30 05:35:01,740 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:01,746 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-30 05:35:01,749 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:01,755 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-30 05:35:01,757 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:01,763 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-30 05:35:01,765 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:01,771 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-30 05:35:01,774 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:01,780 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-30 05:35:01,782 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:01,788 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-30 05:35:01,791 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:01,797 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-30 05:35:01,800 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:01,805 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-30 05:35:01,808 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:01,814 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-30 05:35:01,816 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:01,823 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-30 05:35:01,825 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:01,826 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-30 05:35:01,826 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:01,835 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-30 05:35:01,840 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-30 05:35:01,841 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-30 05:35:01,842 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-30 05:35:01,842 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-30 05:35:01,850 [model.py:408 in init_all_weights] DEBUG - init all weights...
2023-10-30 05:35:01,877 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-30 05:35:01,877 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-30 05:35:01,877 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-30 05:35:01,878 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-30 05:35:01,879 [flexgen.py:148 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-30 05:35:01,918 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 05:35:02,062 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-30 05:35:02,062 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,062 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:02,063 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,064 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,065 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,066 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,066 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,066 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:35:02,067 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:02,067 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-30 05:35:02,068 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,068 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,068 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,072 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,073 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,073 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,074 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,074 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:35:02,074 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:02,078 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,078 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,078 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,082 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,090 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,094 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,100 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,103 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,103 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,103 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:02,105 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,105 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,105 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,109 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,117 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,121 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,125 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,128 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,129 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,129 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:02,130 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,130 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,130 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,134 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,142 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,146 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,149 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,153 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,153 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,153 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:02,155 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,155 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,155 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,159 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:02,167 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,170 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,174 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,177 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,178 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,178 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:02,179 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,180 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:02,183 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:02,192 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,195 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,199 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,203 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,203 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,203 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:02,205 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,205 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,205 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:02,209 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:02,217 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,221 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,225 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,228 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,229 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,229 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:02,230 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,230 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,230 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:02,234 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:02,242 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,246 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,250 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,254 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,254 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,254 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:02,256 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,256 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,256 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:02,260 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:02,268 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,272 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,276 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,280 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,280 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,280 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:02,282 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,282 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,282 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:02,286 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:02,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,298 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,301 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,305 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,306 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,306 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:02,308 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,308 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:02,312 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:02,320 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,323 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,327 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,331 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,331 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,331 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:02,332 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,332 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,333 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:02,336 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:02,366 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,392 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,414 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,417 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,418 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,418 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:02,420 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,420 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,420 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:02,424 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:02,429 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,433 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,437 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,441 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:35:02,441 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:35:02,442 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:02,443 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,443 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,443 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:02,444 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:02,445 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,446 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,447 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,448 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:35:02,448 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:35:02,448 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:02,448 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:35:02,449 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,449 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:02,449 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:02,461 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:35:02,471 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:35:02,480 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:35:02,490 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:35:02,494 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-30 05:35:02,495 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:02,499 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:02,499 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,499 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:02,500 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,500 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,501 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,502 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,502 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,502 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:02,502 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:02,503 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-30 05:35:02,503 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,503 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,504 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,508 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,509 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,510 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,510 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,510 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:02,511 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:02,514 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,514 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,514 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,517 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,525 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,529 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,534 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,538 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,538 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,538 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:02,540 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,540 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,540 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,544 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,553 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,556 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,560 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,563 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,563 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,563 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:02,565 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,565 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,569 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,577 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,580 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,583 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,587 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,587 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,587 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:02,589 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,589 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,589 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,592 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:02,600 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,604 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,608 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,611 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,611 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,612 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:02,616 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,617 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,617 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:02,623 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:02,646 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,656 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,660 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,663 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,663 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,663 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:02,665 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,665 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,666 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:02,669 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:02,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,682 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,685 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,688 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,688 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,689 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:02,690 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,690 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,690 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:02,694 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:02,702 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,705 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,709 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,712 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,713 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,713 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:02,714 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,714 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,715 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:02,718 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:02,726 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,730 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,733 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,736 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,737 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,737 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:02,738 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,738 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,738 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:02,742 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:02,751 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,754 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,758 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,761 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,761 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,761 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:02,763 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,763 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,763 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:02,767 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:02,775 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,779 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,782 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,786 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,786 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,786 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:02,787 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,787 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,787 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:02,791 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:02,809 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,813 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,816 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,819 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,819 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,820 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:02,821 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,821 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,821 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:02,825 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:02,830 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,833 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,837 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,840 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:35:02,840 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:35:02,840 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:02,841 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,842 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:02,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:02,843 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,844 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,845 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,846 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,846 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:02,846 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:02,846 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,846 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,846 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:02,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:02,855 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:02,862 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:02,869 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:02,877 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:02,878 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:02,878 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:02,883 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:02,883 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,883 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:02,884 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,885 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,885 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,886 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,887 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,887 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:02,887 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:02,887 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-30 05:35:02,888 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:02,888 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:02,888 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,893 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,894 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,894 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,895 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:02,895 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:02,895 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:02,898 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,899 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,899 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:02,902 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,910 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,914 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,917 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,920 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,921 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:02,921 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:02,922 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,923 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,923 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:02,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,935 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,939 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,942 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,947 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,947 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:02,947 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:02,949 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,949 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:02,952 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,961 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,965 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,969 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,972 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,972 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:02,972 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:02,974 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,974 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,974 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:02,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:02,986 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,993 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:02,996 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:02,997 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:02,998 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:02,998 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:02,998 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:03,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,010 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,014 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,017 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,021 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,021 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,021 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:03,022 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,023 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,023 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,026 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,034 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,038 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,041 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,045 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,045 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,045 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:03,046 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,046 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,047 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,050 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,086 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,101 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,105 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,108 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,109 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,109 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:03,111 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,111 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,111 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,115 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,125 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,129 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,132 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,135 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,136 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,136 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:03,137 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,137 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,137 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,141 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,149 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,153 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,157 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,161 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,161 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,161 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:03,163 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,163 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,163 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,166 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,174 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,178 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,181 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,185 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,185 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,185 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:03,186 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,186 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,187 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,190 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,199 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,203 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,207 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,211 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,212 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,212 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:03,213 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,214 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,218 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,223 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,227 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,231 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,234 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:35:03,235 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:35:03,235 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:03,236 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,236 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,236 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,237 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,238 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,239 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,240 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,241 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,241 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,241 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:03,241 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,242 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,242 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,242 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:03,251 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,258 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,267 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,275 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,277 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:03,277 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:03,283 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:03,283 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,283 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:03,284 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:03,285 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,285 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,286 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,286 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,287 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,287 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:03,287 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-30 05:35:03,287 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,288 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:03,288 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:03,293 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,295 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,295 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,295 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:03,299 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,299 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:03,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:03,312 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,316 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,330 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,334 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,334 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,334 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:03,336 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,337 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,337 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:03,340 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:03,349 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,353 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,356 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,360 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,360 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,360 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:03,362 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,362 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,362 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:03,365 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:03,373 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,377 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,381 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,384 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,384 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,384 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:03,386 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,386 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,386 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:03,390 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:03,398 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,401 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,405 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,408 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,408 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,408 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:03,410 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,410 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,410 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:03,414 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,423 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,427 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,431 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,434 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,435 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,435 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:03,437 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,437 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,437 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,441 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,449 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,453 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,457 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,460 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,460 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,460 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:03,462 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,462 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,462 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,474 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,478 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,481 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,484 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,485 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,485 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:03,486 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,486 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,490 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,498 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,502 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,505 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,508 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,509 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,509 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:03,510 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,510 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,510 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,515 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,523 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,527 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,530 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,535 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,535 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,536 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:03,537 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,537 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,538 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,541 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,550 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,554 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,557 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,560 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,561 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,561 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:03,562 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,562 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,563 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,566 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,575 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,605 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,609 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,614 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,614 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,614 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:03,616 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,616 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,616 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,620 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,624 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,628 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,631 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,634 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:35:03,634 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:35:03,635 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:03,636 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,636 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,636 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,638 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,638 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,639 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,640 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,640 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,640 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:03,641 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,641 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,641 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,641 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:03,649 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,656 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,662 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,669 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:03,671 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:03,671 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:03,675 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:03,676 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,676 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:03,676 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:03,677 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,679 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,679 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,679 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:03,680 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-30 05:35:03,680 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:03,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:03,685 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,685 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,686 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,687 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,687 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,687 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:03,690 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,690 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,690 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:03,694 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:03,703 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,706 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,709 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,713 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,713 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:03,714 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,715 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,715 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:03,718 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:03,726 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,730 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,733 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,736 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,736 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,736 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:03,738 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,738 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,738 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:03,742 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:03,749 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,758 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,762 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,765 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,765 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,766 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:03,767 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,767 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,768 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:03,772 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:03,780 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,784 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,787 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,791 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,791 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,791 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:03,793 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,793 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,793 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:03,797 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,805 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,808 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,812 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,815 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,816 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,816 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:03,817 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,818 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,818 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:03,821 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,830 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,833 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,837 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,841 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,841 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,841 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:03,843 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,843 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,843 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:03,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,865 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,869 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,872 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,875 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,876 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,876 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:03,877 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,878 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,878 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:03,881 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,893 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,896 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,899 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,899 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,900 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:03,901 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,901 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,901 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:03,904 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,912 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,915 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,918 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,922 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,922 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,922 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:03,923 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,924 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:03,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,934 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,940 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,943 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,944 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,944 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:03,945 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,945 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:03,948 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,956 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,959 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,962 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,966 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,966 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,966 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:03,967 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,968 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:03,968 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:03,971 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,976 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,979 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,983 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,986 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:35:03,986 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:35:03,986 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:03,987 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,987 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:03,988 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,990 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,990 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,991 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:03,991 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:03,991 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:03,992 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:03,992 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:03,992 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:03,992 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,006 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,013 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,019 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,027 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,029 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:04,029 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:04,034 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:04,034 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,034 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,035 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,036 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,036 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,037 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,038 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,038 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,038 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:04,039 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-30 05:35:04,039 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,043 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,044 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,045 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,046 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,046 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,046 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:04,049 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,049 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,050 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,053 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,061 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,064 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,068 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,071 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,071 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,071 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:04,073 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,073 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,076 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,084 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,087 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,090 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,094 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,094 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,094 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:04,095 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,096 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,096 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,099 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,107 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,110 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,139 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,148 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,148 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,148 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:04,150 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,150 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,150 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,153 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,165 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,169 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,172 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,173 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,173 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:04,174 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,175 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,175 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,178 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,187 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,190 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,193 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,197 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,197 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,197 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:04,199 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,199 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,199 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,203 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,211 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,215 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,223 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,223 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,223 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:04,224 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,225 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,225 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,228 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:04,237 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,241 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,244 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,248 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,248 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,248 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:04,249 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,250 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,250 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:04,253 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:04,266 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,269 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,273 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,276 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,276 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,277 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:04,278 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,278 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,278 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:04,282 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:04,290 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,309 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,313 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,319 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,319 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,319 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:04,321 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,321 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,321 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:04,325 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:04,333 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,337 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,340 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,344 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,344 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,344 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:04,346 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,346 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,346 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:04,350 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:04,358 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,365 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,368 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,368 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,369 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:04,370 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,370 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,370 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:04,374 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:04,379 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,382 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,386 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,389 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:35:04,390 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:35:04,390 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:04,391 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,391 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,391 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:04,392 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:04,393 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,394 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,394 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,395 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,395 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,395 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:04,396 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,396 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,396 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:04,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,406 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,412 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,419 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,425 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,428 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:04,428 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:04,435 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:04,435 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,436 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,436 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,437 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,438 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,438 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,438 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,438 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:04,439 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-30 05:35:04,439 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,439 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,444 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,445 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,445 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,446 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,446 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,446 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:04,450 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,450 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,450 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,454 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,461 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,465 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,468 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,471 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,472 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,472 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:04,473 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,474 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,477 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,485 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,488 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,491 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,495 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,495 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,495 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:04,497 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,497 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,497 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,500 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,508 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,511 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,516 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,519 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,519 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,519 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:04,521 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,521 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,521 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,524 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,532 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,535 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,539 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,542 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,543 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,543 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:04,544 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,545 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,545 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,548 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,563 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,591 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,595 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,598 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,599 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,599 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:04,600 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,600 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,601 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,604 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,612 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,616 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,620 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,623 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,623 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,624 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:04,625 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,625 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,625 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,629 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:04,638 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,642 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,645 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,648 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,649 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,649 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:04,650 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,650 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,650 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:04,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:04,662 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,665 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,669 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,672 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,672 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,672 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:04,674 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,674 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:04,678 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:04,686 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,689 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,693 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,696 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,697 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,697 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:04,698 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,698 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,698 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:04,702 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:04,710 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,717 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,720 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,721 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,721 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:04,722 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,722 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,722 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:04,726 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:04,736 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,754 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,757 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,760 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,761 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,761 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:04,762 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,762 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,763 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:04,766 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:04,771 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,775 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,780 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,785 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:35:04,785 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:35:04,785 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:04,787 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,787 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,787 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:04,787 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:04,788 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,789 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,790 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,790 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,791 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,791 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:04,791 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,791 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,791 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:04,792 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,806 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,814 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,821 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,828 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:04,830 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:04,830 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:04,835 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:04,836 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,836 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:04,836 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,837 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,838 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,838 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,839 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,839 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,839 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:04,840 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-30 05:35:04,840 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:04,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:04,841 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,845 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,846 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,847 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,847 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:04,847 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:04,847 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:04,850 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,850 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,851 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:04,854 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,862 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,866 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,869 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,872 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,872 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,873 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:04,874 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,874 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,874 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:04,878 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,886 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,893 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,896 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,896 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,896 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:04,898 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,898 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,898 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:04,902 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,909 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,913 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,916 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,919 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,920 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,920 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:04,921 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,922 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,922 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:04,925 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,933 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,941 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,944 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,944 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,944 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:04,946 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,946 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,946 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:04,950 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,958 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,962 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,965 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,969 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,969 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,969 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:04,971 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,971 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,971 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:04,975 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,982 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,987 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,990 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,993 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:04,993 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:04,994 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:04,995 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:04,995 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:04,995 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:04,998 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,018 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,038 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,042 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,053 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,053 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,054 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:05,055 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,056 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,086 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,090 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,093 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,097 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,097 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,097 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:05,099 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,100 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,100 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:05,113 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,116 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,120 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,124 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,124 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,125 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:05,126 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,126 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,127 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:05,130 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:05,138 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,142 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,145 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,148 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,149 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,149 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:05,150 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,150 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,150 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:05,154 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:05,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,165 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,168 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,172 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,172 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,172 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:05,173 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,173 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,173 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:05,177 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:05,182 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,185 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,188 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,192 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:35:05,192 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:35:05,192 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:05,193 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,193 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,193 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:05,194 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:05,195 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,196 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,196 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,197 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,197 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,197 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:05,198 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,198 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,198 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:05,198 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:05,206 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,212 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,218 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,223 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,225 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:05,225 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:05,230 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:05,230 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,230 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:05,231 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:05,231 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,232 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,233 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,233 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,233 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,234 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:05,234 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-30 05:35:05,234 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,234 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:05,235 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:05,239 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,240 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,240 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,241 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,241 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,241 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:05,245 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,245 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,245 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:05,249 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:05,257 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,261 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,264 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,268 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,268 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,268 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:05,270 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,270 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,270 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:05,274 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:05,282 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,285 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,289 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,292 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,292 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,292 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:05,294 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,294 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,294 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:05,298 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:05,306 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,313 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,317 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,321 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,321 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,321 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:05,323 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,323 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:05,327 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:05,360 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,422 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,459 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,465 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,466 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,466 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:05,468 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,468 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,468 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:05,472 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:05,481 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,485 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,493 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,496 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,496 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,497 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:05,498 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,499 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,499 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:05,503 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:05,511 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,514 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,527 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,531 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,531 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,531 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:05,533 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,533 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,533 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:05,537 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,546 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,550 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,553 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,556 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,557 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,557 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:05,559 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,559 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,559 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,562 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,574 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,578 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,582 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,583 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,583 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:05,584 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,584 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,584 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,588 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:05,621 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,630 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,633 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,637 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,638 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,638 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:05,640 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,640 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,640 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:05,644 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:05,652 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,656 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,660 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,663 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,664 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,664 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:05,665 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,665 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,665 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:05,669 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:05,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,681 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,685 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,689 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,689 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,689 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:05,691 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,691 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:05,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:05,700 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,703 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,707 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,710 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:35:05,711 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:35:05,711 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:05,712 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,712 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,712 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:05,713 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:05,714 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,715 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,715 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,716 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,716 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,716 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:05,717 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,717 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,717 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:05,717 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:05,725 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,731 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,738 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,745 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:05,747 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:05,747 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:05,753 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:05,753 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,753 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:05,754 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:05,755 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,755 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,756 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,757 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,757 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,757 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:05,757 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-30 05:35:05,758 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:05,758 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:05,758 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:05,763 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,764 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,765 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,766 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:05,766 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:05,766 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:05,769 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,769 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,769 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:05,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:05,797 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,803 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,807 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,810 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,810 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,811 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:05,812 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,813 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,813 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:05,816 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:05,825 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,829 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,833 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,836 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,836 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,836 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:05,838 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,838 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,838 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:05,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:05,850 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,854 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,857 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,861 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,861 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,861 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:05,863 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,863 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,863 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:05,866 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:05,874 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,878 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,881 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,885 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,885 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,885 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:05,887 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,887 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,887 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:05,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:05,904 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,907 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,910 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,916 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,916 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,916 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:05,918 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,918 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,918 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:05,922 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:05,930 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,934 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,941 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,941 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,941 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:05,942 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,942 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,943 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:05,946 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,956 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,959 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,963 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,966 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,966 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,966 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:05,968 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,968 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,968 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:05,972 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,980 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,983 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,987 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,991 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:05,991 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:05,991 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:05,992 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:05,992 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:05,993 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:05,996 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,004 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,008 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,011 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,015 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,015 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:06,015 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:06,017 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,017 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,017 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,020 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,028 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,032 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,035 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,039 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,039 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:06,039 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:06,040 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,041 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,041 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,044 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,065 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,070 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,077 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,081 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,081 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:06,081 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:06,083 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,083 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,083 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,091 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,095 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,099 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,102 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:35:06,102 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:35:06,102 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:06,104 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,104 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,105 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,106 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,107 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,107 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,108 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,108 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:06,108 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,108 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,108 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,109 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,116 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,123 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,130 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,137 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,138 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:06,138 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:06,143 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:06,143 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,143 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,143 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,144 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,145 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,146 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,146 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,146 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,146 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:06,147 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-30 05:35:06,147 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,147 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,147 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:06,152 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,153 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,153 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,154 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,154 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,154 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:06,157 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,158 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,158 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:06,161 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:06,169 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,173 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,177 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,181 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,181 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,181 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:06,182 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,183 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,183 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:06,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:06,204 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,210 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,214 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,218 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,218 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:06,220 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,220 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,220 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:06,224 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:06,233 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,251 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,255 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,259 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,259 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,260 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:06,261 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,261 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,262 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:06,265 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:06,273 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,277 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,280 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,285 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,286 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,286 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:06,287 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,287 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,288 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:06,291 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:06,300 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,303 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,307 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,310 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,310 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,310 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:06,312 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,312 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,312 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:06,316 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:06,324 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,327 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,331 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,335 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,335 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,335 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:06,336 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,337 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,337 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:06,340 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:06,348 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,352 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,356 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,359 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,359 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,359 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:06,361 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,361 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,361 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:06,365 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:06,373 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,377 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,380 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,383 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,383 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,383 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:06,385 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,385 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,385 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:06,389 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,401 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,405 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,409 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,413 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,413 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,413 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:06,415 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,415 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,415 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,419 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,427 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,431 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,435 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,439 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,439 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:06,440 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,441 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,441 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,445 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,453 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,457 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,460 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,464 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,464 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,464 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:06,465 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,466 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,469 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,474 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,478 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,481 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,485 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:35:06,485 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:35:06,485 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:06,486 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,487 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,488 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,489 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,490 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,490 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,491 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,491 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:06,491 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,491 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,491 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,492 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,500 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,510 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,520 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,533 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,546 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:06,546 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:06,559 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:06,559 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,561 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,562 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,562 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,563 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,563 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,563 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:06,564 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-30 05:35:06,564 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,564 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,564 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:06,569 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,570 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,570 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,571 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,571 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:06,574 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,574 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,574 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:06,578 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:06,612 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,619 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,623 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,627 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,627 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,627 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:06,629 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,629 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,629 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:06,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:06,646 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,650 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,654 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,657 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,658 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,658 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:06,659 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,660 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,660 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:06,664 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:06,672 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,675 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,679 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,683 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,683 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,683 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:06,685 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,685 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,685 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:06,689 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:06,697 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,701 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,704 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,708 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,708 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,708 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:06,710 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,710 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,710 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:06,714 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:06,723 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,727 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,744 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,747 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,748 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,748 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:06,749 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,750 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,750 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:06,753 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:06,762 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,766 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,769 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,773 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,773 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,773 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:06,774 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,775 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,775 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:06,778 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:06,787 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,791 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,795 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,798 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,799 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,799 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:06,800 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,801 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,801 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:06,805 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:06,813 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,817 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,820 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,824 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,824 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,824 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:06,825 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,826 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,826 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:06,830 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,838 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,842 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,845 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,849 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,849 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,850 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:06,851 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,851 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,851 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:06,855 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,864 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,868 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,872 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,875 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,875 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,876 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:06,877 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,877 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,877 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:06,881 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,903 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,907 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,912 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,915 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,915 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,916 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:06,917 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,917 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:06,917 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:06,921 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,928 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,932 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,942 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:35:06,942 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:35:06,942 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:06,944 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,944 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,944 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:06,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,946 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,946 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,947 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,948 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,948 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,948 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:06,949 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:06,949 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:06,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,958 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,965 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,973 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,980 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:06,981 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:06,981 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:06,986 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:06,986 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:06,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,987 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,988 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,989 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,990 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:06,990 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-30 05:35:06,990 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:06,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:06,991 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:06,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,997 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,998 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:06,998 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:06,998 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:07,001 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,001 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,001 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:07,005 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,019 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,023 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,027 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,031 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,031 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,031 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:07,033 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,033 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,033 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,036 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,044 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,048 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,053 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,056 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,057 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,057 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:07,058 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,059 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,062 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,071 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,075 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,079 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,083 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,083 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,083 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:07,085 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,085 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,085 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,089 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,098 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,101 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,105 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,109 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,109 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,110 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:07,111 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,111 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,111 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,115 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,124 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,128 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,131 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,135 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,135 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,135 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:07,137 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,137 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,137 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,141 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,149 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,152 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,156 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,159 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,160 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,160 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:07,161 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,161 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,162 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,165 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:07,174 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,177 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,181 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,184 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,184 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,184 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:07,186 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,186 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:07,190 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:07,198 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,201 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,205 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,208 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,208 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,209 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:07,210 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,210 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,210 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:07,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:07,222 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,226 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,229 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,233 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,233 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,233 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:07,235 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,235 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,235 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:07,239 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:07,247 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,250 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,254 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,258 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,258 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,258 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:07,259 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,260 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,260 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:07,264 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:07,280 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,285 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,289 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,293 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,293 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,293 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:07,295 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,295 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,295 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:07,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:07,304 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,308 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,312 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,316 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:35:07,316 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:35:07,316 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:07,318 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,318 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,318 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:07,319 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:07,320 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,320 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,321 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,322 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,322 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,322 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:07,322 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,323 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:07,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:07,331 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,338 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,346 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,353 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,354 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:07,354 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:07,359 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:07,359 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,359 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:07,360 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:07,361 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,361 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,363 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,363 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,363 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:07,364 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-30 05:35:07,364 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,364 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:07,364 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:07,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,370 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,371 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,371 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,371 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:07,374 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,374 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,374 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:07,378 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,386 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,391 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,395 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,398 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,398 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,398 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:07,400 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,400 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,400 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,404 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,413 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,416 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,420 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,424 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,424 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,424 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:07,426 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,426 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,426 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,429 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,438 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,441 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,445 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,449 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,449 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,449 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:07,451 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,451 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,451 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,455 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,463 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,467 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,470 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,474 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,474 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,474 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:07,476 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,476 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,476 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,480 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,489 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,493 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,497 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,500 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,501 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,501 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:07,502 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,503 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,503 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,507 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,515 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,525 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,550 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,557 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,558 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,558 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:07,559 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,559 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,559 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,563 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:07,572 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,576 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,580 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,583 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,583 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,584 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:07,585 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,585 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,586 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:07,589 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:07,603 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,609 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,627 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,632 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,632 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:07,634 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,634 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,634 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:07,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:07,647 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,651 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,655 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,659 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,659 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,659 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:07,661 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,661 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,661 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:07,665 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:07,675 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,679 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,683 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,687 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,687 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,687 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:07,688 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,689 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,689 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:07,693 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:07,703 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,707 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,711 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,715 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,715 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,715 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:07,717 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,717 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,717 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:07,721 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:07,727 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,731 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,736 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,740 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:35:07,740 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:35:07,740 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:07,741 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,742 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,742 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:07,742 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:07,743 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,744 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,745 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,746 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,746 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,746 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:07,747 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,747 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,747 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:07,747 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:07,755 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,761 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,769 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,780 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:07,790 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:07,791 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:07,796 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:07,796 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,796 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:07,797 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:07,798 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,799 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,800 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,800 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,801 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,801 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:07,801 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-30 05:35:07,801 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:07,801 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:07,802 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:07,808 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,809 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,810 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,811 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:07,811 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:07,812 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:07,816 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,817 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,817 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:07,821 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,830 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,833 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,838 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,841 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,841 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,842 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:07,843 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,843 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,844 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:07,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,856 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,860 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,863 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,867 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,867 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,867 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:07,869 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,869 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,869 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:07,873 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,881 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,885 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,892 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,893 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,893 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:07,894 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,895 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,895 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:07,899 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,907 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,910 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,914 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,918 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,918 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,918 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:07,920 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,920 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,920 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:07,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,933 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,941 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,945 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,946 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,946 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:07,947 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,948 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,948 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:07,952 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,960 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,964 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,968 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,972 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:07,973 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:07,973 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:07,974 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:07,974 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:07,974 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:07,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:07,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,000 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,004 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,009 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,009 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,009 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:08,011 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,011 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,011 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:08,014 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,023 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,027 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,030 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,034 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,034 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,034 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:08,035 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,036 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,036 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,040 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:08,069 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,074 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,078 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,083 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,083 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,084 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:08,086 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,086 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:08,091 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:08,100 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,104 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,109 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,113 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,113 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,113 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:08,115 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,115 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,116 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:08,120 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:08,128 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,132 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,136 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,139 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,140 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,140 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:08,142 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,142 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,142 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:08,146 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:08,151 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,155 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,159 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:35:08,162 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:35:08,162 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:08,164 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,164 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,164 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:08,165 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:08,166 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,166 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,167 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,168 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,168 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,168 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:08,169 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,169 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,169 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:08,169 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:08,177 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,185 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,192 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,199 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,201 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:08,201 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:08,207 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:08,207 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,208 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:08,208 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:08,209 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,210 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,210 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,211 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,211 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,211 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:08,212 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-30 05:35:08,212 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,212 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:08,212 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:08,217 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,217 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,219 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,219 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,219 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:08,222 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,222 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,222 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:08,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:08,235 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,238 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,242 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,245 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,246 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,246 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:08,247 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,248 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,248 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:08,252 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:08,271 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,275 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,279 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,283 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,283 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,283 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:08,285 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,285 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,285 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:08,288 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:08,297 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,301 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,304 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,308 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,309 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,309 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:08,310 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,310 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,310 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:08,314 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:08,324 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,328 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,332 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,336 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,336 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,336 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:08,338 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,338 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,338 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:08,341 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:08,350 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,354 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,358 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,362 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,362 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:08,364 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,364 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,364 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:08,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:08,377 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,381 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,384 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,388 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,388 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,389 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:08,390 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,390 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,390 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:08,394 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:08,403 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,406 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,410 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,414 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,414 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,414 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:08,416 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,416 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,416 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:08,420 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,428 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,432 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,436 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,439 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,439 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:08,441 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,441 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,441 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,445 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:08,453 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,457 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,461 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,631 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,631 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,631 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:08,633 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,633 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:08,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:08,645 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,649 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,653 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,657 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,657 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,657 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:08,658 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,658 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,658 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:08,662 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:08,674 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,682 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,686 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,686 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,686 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:08,687 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,688 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,688 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:08,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:08,698 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,702 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,705 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,709 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:35:08,709 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:35:08,709 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:08,711 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,711 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,711 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:08,711 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:08,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,714 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,715 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,715 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,715 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:08,716 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,716 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,716 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:08,716 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:08,726 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,732 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,739 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,745 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:08,746 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:08,747 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:08,752 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:08,752 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,752 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:08,753 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:08,754 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,754 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,755 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,756 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,756 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,756 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:08,756 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-30 05:35:08,757 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:08,757 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:08,757 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:08,762 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,762 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,763 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,764 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:08,764 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:08,764 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:08,767 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,767 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,767 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:08,771 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:08,779 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,783 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,787 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,791 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,791 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,791 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:08,793 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,793 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,794 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:08,797 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:08,806 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,810 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,813 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,817 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,817 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,817 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:08,819 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,819 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,819 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:08,823 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:08,832 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,836 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,839 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,843 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,843 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,843 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:08,845 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,845 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,845 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:08,849 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:08,858 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,862 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,866 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,869 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,870 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,870 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:08,871 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,872 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,872 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:08,876 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:08,884 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,888 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,892 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,895 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,896 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,896 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:08,897 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,898 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,898 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:08,901 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:08,910 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,914 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,918 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,921 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,922 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,922 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:08,923 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,923 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,923 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:08,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:08,936 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,940 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,944 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,948 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,948 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,948 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:08,950 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,950 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,950 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:08,954 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,962 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,966 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,970 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,974 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,974 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:08,974 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:08,975 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:08,976 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:08,976 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:08,980 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:08,988 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,992 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:08,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,000 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,000 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:09,000 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:09,002 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,002 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:09,006 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:09,014 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,018 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,021 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,025 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,025 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:09,025 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:09,026 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,027 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,027 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:09,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:09,039 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,043 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,046 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,050 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,050 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:09,050 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:09,052 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,052 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,052 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:09,055 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:09,061 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,064 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,068 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,071 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:35:09,072 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:35:09,072 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:09,073 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,073 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:09,074 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:09,075 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,076 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,077 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,077 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,078 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,078 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:09,078 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,078 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,078 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:09,079 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:09,087 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,095 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,101 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,107 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,108 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:09,109 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:09,113 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:09,113 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,113 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:09,114 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:09,115 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,116 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,116 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,117 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,117 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,117 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:09,118 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-30 05:35:09,118 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,118 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:09,118 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:09,123 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,123 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,124 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,125 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,125 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,125 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:09,128 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,128 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:09,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:09,140 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,144 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,147 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,151 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,151 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,151 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:09,153 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,153 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,153 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:09,156 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:09,165 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,168 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,172 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,175 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,176 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,176 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:09,177 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,178 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,178 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:09,181 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:09,189 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,193 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,197 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,200 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,200 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,201 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:09,202 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,202 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,202 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:09,206 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:09,214 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,221 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,225 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,225 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,225 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:09,227 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,227 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,227 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:09,231 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:09,239 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,243 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,246 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,250 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,250 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,250 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:09,251 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,252 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,252 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:09,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:09,263 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,267 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,322 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,323 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,323 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:09,324 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,324 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,324 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:09,328 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:09,355 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,359 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,366 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,366 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,367 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:09,368 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,368 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:09,372 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:09,380 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,383 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,387 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,390 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,390 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,391 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:09,392 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,392 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,392 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:09,395 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:09,404 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,407 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,411 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,414 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,414 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,415 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:09,416 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,417 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,417 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:09,420 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:09,428 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,432 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,435 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,439 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,439 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:09,440 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,440 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:09,444 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:09,452 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,456 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,459 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,462 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,463 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,463 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:09,464 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,465 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,465 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:09,468 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:09,473 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,836 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,840 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,843 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:35:09,844 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:35:09,844 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:09,845 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,845 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,846 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:09,846 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:09,847 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,848 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,849 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,849 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,850 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,850 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:09,850 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,850 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,850 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:09,851 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:09,859 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,865 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,872 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,878 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:09,880 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:09,881 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:09,886 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:09,886 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,886 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:09,886 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:09,887 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,888 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,890 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,890 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:09,890 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-30 05:35:09,890 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:09,890 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:09,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:09,895 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,896 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,897 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,898 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:09,898 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:09,898 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:09,901 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,901 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,901 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:09,905 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:09,913 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,916 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,920 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,923 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,924 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:09,924 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:09,925 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,925 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,925 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:09,929 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:09,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,940 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,944 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,947 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,948 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:09,948 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:09,949 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,949 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:09,953 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:09,961 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,964 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,968 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,971 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,972 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:09,972 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:09,973 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,974 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,974 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:09,977 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:09,985 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,992 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:09,996 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:09,996 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:09,998 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:09,998 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:09,998 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:10,001 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,031 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,036 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,040 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,044 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,044 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,045 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:10,047 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,047 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,047 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,051 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,060 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,064 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,067 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,071 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,071 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,072 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:10,073 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,073 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,077 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,085 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,089 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,092 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,096 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,096 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,096 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:10,098 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,098 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,098 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,102 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,111 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,114 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,118 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,122 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,122 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,122 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:10,123 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,123 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,124 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,127 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:10,135 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,139 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,143 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,146 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,147 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,147 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:10,149 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,149 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,149 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:10,152 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:10,165 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,193 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,210 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,214 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,214 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,214 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:10,216 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,216 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,216 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:10,219 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:10,228 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,231 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,235 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,239 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,239 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,239 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:10,241 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,241 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,241 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:10,245 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:10,250 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,254 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,257 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,261 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:35:10,261 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:35:10,261 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:10,263 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,263 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,263 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:10,264 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:10,265 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,265 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,266 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,267 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,267 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,267 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:10,268 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,268 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,268 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:10,268 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:10,276 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,283 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,289 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,295 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,297 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:10,297 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:10,302 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:10,302 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,302 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:10,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:10,304 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,305 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,306 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,306 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,307 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,307 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:10,307 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-30 05:35:10,307 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:10,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:10,313 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,313 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,314 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,315 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,315 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,315 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:10,318 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,318 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,318 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:10,322 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:10,330 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,334 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,338 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,341 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,342 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,342 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:10,343 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,344 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,344 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:10,347 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:10,361 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,365 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,373 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,373 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,373 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:10,375 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,375 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:10,378 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:10,387 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,391 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,394 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,398 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,398 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,398 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:10,400 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,400 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,400 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:10,404 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:10,412 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,416 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,420 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,423 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,424 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,424 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:10,425 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,425 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,426 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:10,429 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,438 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,441 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,445 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,449 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,449 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,449 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:10,451 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,451 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,451 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,454 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,463 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,466 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,470 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,474 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,474 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,474 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:10,475 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,476 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,476 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,480 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,488 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,493 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,496 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,500 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,501 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,501 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:10,502 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,502 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,502 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,506 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,514 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,518 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,522 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,526 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,526 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,527 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:10,528 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,528 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,528 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,532 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:10,541 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,545 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,548 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,552 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,552 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,553 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:10,555 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,555 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,555 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:10,559 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:10,567 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,575 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,578 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,579 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,579 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:10,580 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,580 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,580 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:10,584 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:10,593 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,597 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,601 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,605 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,605 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,605 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:10,607 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,607 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,607 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:10,611 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:10,621 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,626 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,632 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,636 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:35:10,636 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:35:10,637 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:10,638 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,638 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:10,639 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:10,640 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,641 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,642 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,642 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,643 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,643 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:10,643 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,643 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,643 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:10,644 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:10,653 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,659 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,665 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,672 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:10,674 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:10,674 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:10,679 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:10,679 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,679 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:10,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:10,681 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,682 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,682 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,683 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,683 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,683 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:10,684 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-30 05:35:10,684 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:10,684 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:10,684 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:10,689 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,690 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,691 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,692 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:10,692 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:10,692 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:10,695 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,695 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:10,699 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:10,708 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,744 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,767 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,771 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,772 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,772 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:10,773 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,774 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,774 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:10,777 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:10,786 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,790 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,793 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,797 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,798 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,798 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:10,799 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,799 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,799 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:10,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:10,811 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,815 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,819 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,829 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,829 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,829 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:10,831 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,831 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,831 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:10,835 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:10,843 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,847 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,850 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,854 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,854 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,854 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:10,856 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,856 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,856 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:10,860 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,869 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,886 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,890 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,894 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,894 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,895 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:10,896 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,897 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,897 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:10,900 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,909 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,913 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,917 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,921 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,921 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,921 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:10,923 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,923 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,923 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:10,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,935 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,939 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,943 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,947 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,947 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,947 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:10,949 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,949 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,950 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:10,953 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,962 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,966 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,970 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,974 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,974 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,974 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:10,975 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:10,976 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:10,976 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:10,979 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:10,988 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,992 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,995 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,999 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:10,999 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:10,999 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:11,001 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,001 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,001 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:11,005 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,013 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,017 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,020 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,024 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,024 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:11,024 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:11,026 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,026 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,026 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,038 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,042 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,046 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,057 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,057 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:11,057 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:11,059 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,059 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,063 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:11,067 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,071 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,075 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,079 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:35:11,079 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:35:11,079 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:11,080 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,081 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,081 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:11,081 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:11,082 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,083 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,084 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,084 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,085 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,085 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:11,085 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,085 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,085 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:11,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:11,093 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,101 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,108 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,116 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,119 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:11,119 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:11,123 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:11,124 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,124 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:11,124 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:11,125 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,126 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,126 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,127 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,127 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,127 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:11,128 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-30 05:35:11,128 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:11,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:11,133 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,133 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,134 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,135 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,135 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,135 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:11,138 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,138 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,138 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:11,141 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:11,159 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,166 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,169 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,170 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,170 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:11,171 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,172 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,172 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:11,175 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:11,183 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,187 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,191 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,194 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,194 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,195 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:11,196 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,196 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,197 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:11,200 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:11,208 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,212 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,215 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,219 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,219 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,219 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:11,221 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,221 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,221 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:11,225 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:11,232 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,236 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,240 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,244 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,244 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,244 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:11,246 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,246 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,246 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:11,250 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:11,258 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,261 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,265 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,269 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,269 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,269 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:11,271 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,271 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:11,274 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:11,283 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,286 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,290 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,293 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,293 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,294 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:11,295 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,295 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,295 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:11,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:11,307 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,311 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,314 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,318 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,319 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,319 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:11,320 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,321 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,321 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:11,325 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:11,333 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,336 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,340 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,343 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,344 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,344 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:11,345 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,345 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,345 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:11,349 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:11,357 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,360 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,364 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,367 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,367 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,367 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:11,369 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,369 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,369 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:11,373 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,381 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,384 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,388 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,391 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,391 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,392 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:11,393 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,393 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,393 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,396 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,404 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,408 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,411 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,415 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,415 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,415 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:11,417 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,417 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,417 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,420 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:11,425 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,429 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,432 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,436 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:35:11,436 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:35:11,436 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:11,437 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,437 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,437 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:11,438 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:11,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,440 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,441 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,441 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,441 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,441 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:11,442 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,442 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,442 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:11,442 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:11,449 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,456 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,463 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,469 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:11,471 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:11,471 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:11,476 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:11,476 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,476 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:11,477 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:11,478 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,478 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,479 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,479 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,480 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,480 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:11,480 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-30 05:35:11,480 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:11,481 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:11,481 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:11,485 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,486 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,487 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,487 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:11,488 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:11,488 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:11,491 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,491 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,491 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:11,494 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:11,502 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,505 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,509 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,512 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,513 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,513 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:11,514 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,514 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,514 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:11,518 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:11,525 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,529 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,532 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,536 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,536 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,536 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:11,538 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,538 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,538 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:11,541 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:11,549 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,553 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,556 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,559 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,560 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,560 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:11,561 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,562 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,562 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:11,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:11,573 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,577 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,580 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,583 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,584 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,584 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:11,585 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,585 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,586 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:11,589 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:11,597 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,601 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,604 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,607 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,608 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,819 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:11,821 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,821 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,821 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:11,825 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:11,833 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,837 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,841 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,845 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,845 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,845 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:11,846 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,847 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:11,850 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:11,859 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,863 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,867 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,870 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,871 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,871 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:11,872 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,873 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,873 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:11,877 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:11,885 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,889 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,905 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,909 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,909 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,909 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:11,911 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,911 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,911 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:11,914 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:11,923 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,926 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,930 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,933 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,934 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,934 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:11,935 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,936 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,936 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:11,940 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,948 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,952 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,955 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,959 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,959 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,959 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:11,960 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,961 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,961 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:11,964 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,973 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,977 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,980 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,984 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:11,984 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:11,984 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:11,986 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:11,986 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:11,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:11,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,011 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:12,015 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:12,019 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:12,022 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:35:12,023 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:35:12,023 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:12,024 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,024 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,024 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,025 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,026 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,027 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,028 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,028 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,028 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,029 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:12,029 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,029 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,029 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,039 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,046 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,054 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,060 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,062 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:12,062 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:12,067 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:12,067 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,067 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,068 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,069 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,069 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,070 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,070 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,071 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,071 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:12,071 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-30 05:35:12,071 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,071 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,072 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,077 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,077 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,078 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,079 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,079 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,079 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:12,082 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,082 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,082 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,094 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,098 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,101 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,105 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,105 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,105 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:12,106 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,107 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,107 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,110 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,118 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,122 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,126 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,129 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,130 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,130 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:12,131 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,132 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,136 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,144 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,148 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,151 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,155 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,155 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,155 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:12,157 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,157 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,157 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,161 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,169 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,173 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,176 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,180 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,180 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,180 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:12,182 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,182 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,182 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:12,195 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,198 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,202 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,205 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,206 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,206 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:12,207 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,207 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,208 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:12,211 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:12,220 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,224 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,228 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,232 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,232 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,232 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:12,233 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,233 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,234 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:12,237 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:12,270 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,286 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,290 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,294 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,294 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:12,295 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,296 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,296 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:12,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:12,308 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,316 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,320 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,325 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,326 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,326 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:12,327 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,327 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,327 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:12,331 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:12,340 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,344 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,347 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,351 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,351 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,351 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:12,353 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,353 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,353 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:12,357 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:12,365 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,373 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,377 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,377 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,377 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:12,379 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,379 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,379 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:12,383 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:12,392 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,395 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,399 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,402 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,403 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,403 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:12,404 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,404 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,405 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:12,408 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,413 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,417 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,422 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,426 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:35:12,426 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:35:12,426 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:12,427 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,427 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,427 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,428 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,429 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,430 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,430 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,431 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,431 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,431 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:12,432 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,432 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,432 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,432 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,440 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,446 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,453 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,459 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,461 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:12,461 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:12,465 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:12,466 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,467 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,468 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,468 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,469 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,469 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,469 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:12,470 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-30 05:35:12,470 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,470 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,470 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,475 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,476 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,476 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,477 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,477 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,477 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:12,480 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,481 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,481 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,484 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,493 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,496 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,500 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,504 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,504 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,504 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:12,506 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,506 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,506 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,510 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,518 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,522 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,525 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,529 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,529 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,529 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:12,531 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,531 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,531 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,535 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,543 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,547 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,551 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,554 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,554 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,555 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:12,556 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,556 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,557 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,569 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,572 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,576 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,579 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,580 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,580 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:12,581 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,582 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,582 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,585 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:12,595 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,599 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,602 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,606 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,606 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,606 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:12,607 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,608 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,608 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:12,611 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:12,620 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,624 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,638 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,643 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,643 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,643 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:12,645 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,645 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,645 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:12,649 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:12,658 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,662 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,666 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,669 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,670 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,670 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:12,671 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,672 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,672 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:12,675 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:12,684 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,687 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,691 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,696 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,696 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,696 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:12,697 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,697 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,697 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:12,701 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:12,710 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,717 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,720 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,721 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,721 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:12,722 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,722 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,723 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:12,726 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:12,734 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,738 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,742 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,746 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,746 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,746 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:12,747 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,748 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:12,751 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:12,760 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,764 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,767 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,771 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,771 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,771 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:12,773 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,773 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:12,777 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,782 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,786 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,789 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,793 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:35:12,793 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:35:12,793 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:12,794 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,795 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,795 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:12,795 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,796 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,797 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,798 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,799 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,799 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,799 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:12,799 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,799 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,800 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:12,800 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,808 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,815 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,821 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,828 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:12,833 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:12,833 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:12,838 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:12,838 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:12,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,840 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,841 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,842 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,842 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,842 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,842 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:12,843 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-30 05:35:12,843 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:12,843 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:12,843 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,848 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,849 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,850 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,850 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:12,850 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:12,850 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:12,853 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,854 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,854 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:12,857 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,865 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,869 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,873 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,876 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,876 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:12,877 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:12,878 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,878 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,878 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:12,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,890 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,911 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,914 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,918 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,918 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:12,918 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:12,921 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,921 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,921 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:12,925 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,933 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,941 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,944 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,944 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:12,945 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:12,946 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,946 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,946 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:12,950 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,958 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,962 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,966 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,970 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,970 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:12,970 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:12,972 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:12,972 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:12,972 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:12,975 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:12,993 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:12,996 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,000 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,003 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,004 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,004 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:13,005 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,005 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,006 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:13,009 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,017 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,021 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,024 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,027 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,028 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,028 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:13,029 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,029 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,029 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,033 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,042 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,045 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,049 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,053 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,053 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,053 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:13,055 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,055 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,055 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,067 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,070 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,074 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,078 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,078 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,078 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:13,080 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,080 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,083 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,092 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,095 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,099 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,102 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,103 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,103 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:13,104 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,104 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,108 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,116 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,120 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,124 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,127 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,127 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,127 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:13,129 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,129 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,129 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,141 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,144 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,148 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,151 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,151 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,152 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:13,153 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,154 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,154 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,157 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,166 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,169 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,173 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:35:13,173 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:35:13,173 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:13,174 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,174 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,174 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,175 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,176 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,177 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,177 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,178 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,178 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,178 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:13,179 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,179 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,179 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,179 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:13,186 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,193 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,200 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,207 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,209 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:13,209 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:13,214 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:13,214 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:13,215 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:13,216 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,216 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,217 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,218 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,218 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:13,218 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-30 05:35:13,219 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,219 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:13,219 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:13,224 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,224 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,225 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,225 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,226 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,226 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:13,228 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,229 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,229 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:13,232 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:13,258 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,262 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,266 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,269 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,269 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,269 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:13,271 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,271 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:13,275 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:13,283 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,287 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,290 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,294 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,294 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,294 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:13,296 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,296 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,296 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:13,300 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:13,307 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,311 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,315 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,318 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,319 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,319 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:13,320 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,320 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,321 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:13,324 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:13,333 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,337 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,341 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,344 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,344 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,344 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:13,346 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,346 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,346 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:13,350 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:13,358 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,365 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,369 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,369 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:13,371 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,371 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,371 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:13,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,383 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,387 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,390 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,394 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,394 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,394 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:13,395 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,396 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,396 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,399 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,407 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,411 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,415 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,418 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,419 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,419 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:13,420 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,421 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,421 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,424 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,432 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,436 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,442 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,447 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,447 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,447 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:13,449 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,449 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,450 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,454 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,462 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,467 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,472 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,476 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,476 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,476 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:13,477 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,478 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,478 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,481 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,489 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,493 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,497 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,500 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,500 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,500 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:13,501 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,502 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,502 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,505 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,525 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,529 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,532 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,536 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,536 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,536 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:13,538 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,538 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,538 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,541 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,546 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,550 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,563 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,567 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:35:13,567 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:35:13,567 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:13,569 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,569 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,569 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,570 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,572 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,573 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,573 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,573 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:13,574 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,574 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,574 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,574 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:13,585 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,595 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,605 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,612 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:13,615 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:13,616 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:13,621 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:13,621 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,622 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:13,622 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:13,623 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,624 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,624 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,625 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,625 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,625 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:13,626 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-30 05:35:13,626 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,626 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:13,627 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:13,631 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,632 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,633 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,633 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,633 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,634 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:13,636 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,637 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:13,640 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:13,648 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,652 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,656 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,660 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,660 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,660 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:13,662 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,662 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,662 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:13,665 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:13,674 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,678 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,681 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,685 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,685 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,685 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:13,687 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,687 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,687 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:13,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:13,699 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,703 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,711 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,729 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,729 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,729 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:13,732 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,733 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,733 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:13,738 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:13,750 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,754 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,758 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,762 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,762 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,762 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:13,764 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,764 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,764 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:13,768 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:13,795 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,799 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,804 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,808 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,808 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,808 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:13,810 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,810 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,810 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:13,814 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,822 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,826 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,830 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,834 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,834 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,834 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:13,836 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,836 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,836 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:13,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,848 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,852 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,856 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,859 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,860 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,860 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:13,861 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,861 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,862 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:13,865 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,874 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,878 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,882 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,886 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,886 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,886 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:13,887 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,888 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,888 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:13,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,901 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,904 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,908 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,912 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,912 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,912 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:13,913 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,914 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,914 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:13,917 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,926 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,930 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,933 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,937 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,937 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,937 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:13,939 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,939 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,939 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:13,943 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,951 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,955 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,959 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,963 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,963 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,963 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:13,965 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,965 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:13,965 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:13,968 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,974 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,978 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,982 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,985 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:35:13,986 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:35:13,986 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:13,987 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,987 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:13,988 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,989 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,990 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,990 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,991 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:13,991 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:13,992 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:13,992 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:13,992 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:13,992 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:13,993 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:14,005 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,013 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,020 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,027 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,029 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:14,029 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:14,034 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:14,034 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,034 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:14,035 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:14,036 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,036 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,037 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,038 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,038 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,038 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:14,039 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-30 05:35:14,039 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:14,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:14,044 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,045 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,046 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,046 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,046 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,047 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:14,049 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,050 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,050 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:14,053 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:14,063 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,066 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,070 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,074 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,074 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,075 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:14,076 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,076 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,076 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:14,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:14,088 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,092 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,096 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,099 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,100 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,100 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:14,101 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,102 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,102 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:14,105 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:14,114 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,118 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,122 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,126 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,126 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,126 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:14,128 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,128 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:14,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:14,154 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,158 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,162 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,166 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,166 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,166 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:14,168 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,168 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,168 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:14,172 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:14,181 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,185 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,188 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,192 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,192 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,192 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:14,194 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,194 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,194 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:14,198 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:14,207 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,210 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,214 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,218 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,218 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,218 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:14,220 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,220 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,220 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:14,223 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:14,232 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,236 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,241 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,270 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,270 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,270 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:14,272 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,273 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,273 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:14,277 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:14,286 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,290 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,293 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,297 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,298 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,298 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:14,299 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,299 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:14,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:14,311 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,315 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,319 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,323 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,323 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,323 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:14,324 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,325 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,325 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:14,328 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:14,337 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,340 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,344 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,348 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,348 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,348 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:14,350 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,350 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,350 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:14,353 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:14,362 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,366 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,369 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,373 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,373 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,373 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:14,375 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,375 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:14,379 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:14,384 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,387 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,391 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,395 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:35:14,395 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:35:14,395 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:14,396 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,397 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:14,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:14,398 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,399 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,400 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,400 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,400 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,401 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:14,401 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,401 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,401 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:14,401 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:14,409 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,416 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,424 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,430 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,432 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:14,432 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:14,436 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:35:14,437 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,437 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:14,437 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:14,438 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,439 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,440 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,440 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,440 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:35:14,441 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-30 05:35:14,441 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,441 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:35:14,441 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:14,446 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,447 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,447 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,448 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,448 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,448 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:35:14,451 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,452 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,452 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:35:14,456 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:14,465 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,469 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,474 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,477 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,478 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,478 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:35:14,479 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,480 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,480 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:35:14,483 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:14,491 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,495 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,499 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,503 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,503 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,503 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:35:14,505 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,505 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,506 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:35:14,509 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:14,517 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,537 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,541 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,545 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,545 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,545 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:35:14,547 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,547 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,547 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:35:14,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:14,560 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,563 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,567 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,571 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,571 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,571 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:35:14,573 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,573 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,573 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:35:14,576 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:14,585 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,589 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,592 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,596 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,596 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,597 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:35:14,598 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,598 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,598 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:35:14,602 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:14,610 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,614 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,618 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,622 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,622 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,622 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:35:14,623 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,623 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,623 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:35:14,627 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:14,635 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,641 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,644 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,648 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,649 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,649 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:35:14,650 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,650 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:35:14,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:14,663 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,668 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,672 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,676 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,677 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,677 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:35:14,678 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,678 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,678 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:35:14,682 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:14,691 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,695 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,713 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,717 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,717 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,718 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:35:14,719 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,719 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,720 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:35:14,723 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:14,731 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,736 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,743 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,747 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,747 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,747 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:35:14,748 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,748 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:35:14,752 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:14,760 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,764 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,768 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,772 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,772 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,772 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:35:14,774 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,774 [flexgen.py:110 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:35:14,774 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:35:14,778 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:14,784 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,804 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,808 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,812 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:35:14,812 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:35:14,813 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:35:14,814 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,814 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,814 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:35:14,815 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:14,816 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,817 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,817 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,818 [flexgen.py:127 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:35:14,818 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:35:14,819 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:35:14,819 [flexgen.py:109 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:35:14,819 [flexgen.py:110 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:35:14,819 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:35:14,820 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:35:14,828 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,834 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,840 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,846 [flexgen.py:127 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:35:14,848 [flexgen.py:140 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:35:14,848 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:35:14,853 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-30 05:35:14,853 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:35:14,853 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-30 05:35:14,853 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:35:14,853 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-30 05:35:14,854 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:35:14,854 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-30 05:35:14,854 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:35:14,862 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-30 05:35:14,862 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-30 05:35:14,862 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-30 05:35:14,862 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-30 05:35:14,863 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-30 05:35:14,864 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-30 05:35:14,864 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-30 05:35:14,864 [flexgen.py:66 in layer_reset] DEBUG - lm_head from flexgen to old.
