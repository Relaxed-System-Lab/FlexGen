{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fsuser/FlexGen/general',\n",
       " '/home/fsuser/miniconda3/lib/python310.zip',\n",
       " '/home/fsuser/miniconda3/lib/python3.10',\n",
       " '/home/fsuser/miniconda3/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/fsuser/miniconda3/lib/python3.10/site-packages',\n",
       " '/home/fsuser/FlexGen']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:45:01,678 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpmbco7n3i\n",
      "2023-11-21 09:45:01,679 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpmbco7n3i/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from loaders import MetaModel\n",
    "from utils import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:45:01,915 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-11-21 09:45:02,057 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /NousResearch/Llama-2-7b-chat-hf/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-11-21 09:45:02.387276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 09:45:02,894 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "2023-11-21 09:45:02,979 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-11-21 09:45:02,980 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-11-21 09:45:02,981 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5\n",
      "2023-11-21 09:45:02,981 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7\n",
      "2023-11-21 09:45:03,675 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /NousResearch/Llama-2-7b-chat-hf/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-11-21 09:45:03,785 [model.py:134 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/NousResearch.Llama-2-7b-chat-hf'\n",
      "2023-11-21 09:45:03,786 [model.py:72 in __init__] INFO - weights offload folder: weights_offload_dir/NousResearch.Llama-2-7b-chat-hf\n",
      "2023-11-21 09:45:03,843 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /NousResearch/Llama-2-7b-chat-hf/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-11-21 09:45:03,958 [model.py:83 in __init__] INFO - tied_params: []\n",
      "2023-11-21 09:45:03,962 [model.py:265 in get_policy_weight_map] DEBUG - model.embed_tokens, [0. 0. 1.], size_todo: 6607343616\n",
      "2023-11-21 09:45:03,963 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.0, [0.         0.20125292 0.79874708], size_todo: 6404960256\n",
      "2023-11-21 09:45:03,964 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.1, [0.         0.25048158 0.74951842], size_todo: 6202576896\n",
      "2023-11-21 09:45:03,965 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.2, [0.         0.27271819 0.72728181], size_todo: 6000193536\n",
      "2023-11-21 09:45:03,966 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.3, [0.         0.28538582 0.71461418], size_todo: 5797810176\n",
      "2023-11-21 09:45:03,967 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.4, [0.         0.29356746 0.70643254], size_todo: 5595426816\n",
      "2023-11-21 09:45:03,967 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.5, [0.         0.29928758 0.70071242], size_todo: 5393043456\n",
      "2023-11-21 09:45:03,968 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.6, [0.         0.30351179 0.69648821], size_todo: 5190660096\n",
      "2023-11-21 09:45:03,969 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.7, [0.         0.30675903 0.69324097], size_todo: 4988276736\n",
      "2023-11-21 09:45:03,970 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.8, [0.         0.30933311 0.69066689], size_todo: 4785893376\n",
      "2023-11-21 09:45:03,971 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.9, [0.         0.31142368 0.68857632], size_todo: 4583510016\n",
      "2023-11-21 09:45:03,972 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.10, [0.         0.31315529 0.68684471], size_todo: 4381126656\n",
      "2023-11-21 09:45:03,973 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.11, [0.         0.31461307 0.68538693], size_todo: 4178743296\n",
      "2023-11-21 09:45:03,974 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.12, [0.         0.31585722 0.68414278], size_todo: 3976359936\n",
      "2023-11-21 09:45:03,974 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.13, [0.       0.311272 0.688728], size_todo: 3773976576\n",
      "2023-11-21 09:45:03,975 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.14, [0.         0.31257065 0.68742935], size_todo: 3571593216\n",
      "2023-11-21 09:45:03,977 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.15, [0.         0.30873371 0.69126629], size_todo: 3369209856\n",
      "2023-11-21 09:45:03,977 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.16, [0.         0.31002901 0.68997099], size_todo: 3166826496\n",
      "2023-11-21 09:45:03,979 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.17, [0.         0.30673989 0.69326011], size_todo: 2964443136\n",
      "2023-11-21 09:45:03,980 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.18, [0.         0.30800482 0.69199518], size_todo: 2762059776\n",
      "2023-11-21 09:45:03,981 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.19, [0.         0.30513233 0.69486767], size_todo: 2559676416\n",
      "2023-11-21 09:45:03,982 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.20, [0.         0.30635465 0.69364535], size_todo: 2357293056\n",
      "2023-11-21 09:45:03,983 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.21, [0.         0.30380869 0.69619131], size_todo: 2154909696\n",
      "2023-11-21 09:45:03,984 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.22, [0.         0.30498361 0.69501639], size_todo: 1952526336\n",
      "2023-11-21 09:45:03,985 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.23, [0.         0.30269986 0.69730014], size_todo: 1750142976\n",
      "2023-11-21 09:45:03,986 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.24, [0.        0.3038264 0.6961736], size_todo: 1547759616\n",
      "2023-11-21 09:45:03,987 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.25, [0.         0.30175748 0.69824252], size_todo: 1345376256\n",
      "2023-11-21 09:45:03,988 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.26, [0.        0.3028366 0.6971634], size_todo: 1142992896\n",
      "2023-11-21 09:45:03,989 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.27, [0.         0.30094668 0.69905332], size_todo: 940609536\n",
      "2023-11-21 09:45:03,990 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.28, [0.         0.30198035 0.69801965], size_todo: 738226176\n",
      "2023-11-21 09:45:03,991 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.29, [0.        0.3002417 0.6997583], size_todo: 535842816\n",
      "2023-11-21 09:45:03,992 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.30, [0.         0.30123233 0.69876767], size_todo: 333459456\n",
      "2023-11-21 09:45:03,993 [model.py:265 in get_policy_weight_map] DEBUG - model.layers.31, [0.         0.29962309 0.70037691], size_todo: 131076096\n",
      "2023-11-21 09:45:03,993 [model.py:265 in get_policy_weight_map] DEBUG - model.norm, [0.        0.2996229 0.7003771], size_todo: 131072000\n",
      "2023-11-21 09:45:03,994 [model.py:265 in get_policy_weight_map] DEBUG - lm_head, [0.        0.2937948 0.7062052], size_todo: 0\n",
      "2023-11-21 09:45:03,995 [model.py:271 in get_policy_weight_map] INFO - device_map is prepared!\n",
      "2023-11-21 09:45:03,997 [model.py:307 in get_policy_weight_map] INFO - CausalLM NousResearch/Llama-2-7b-chat-hf is to be loaded on: \n",
      "GPU Mem 0.00 GiB (0.00%), CPU Mem 3.69 GiB (29.38%), Disk Mem 8.86 Gib (70.62%)\n",
      "2023-11-21 09:45:04,053 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /NousResearch/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2023-11-21 09:45:04,110 [model.py:433 in to_layer_offloading_forward] DEBUG - model.embed_tokens to test forward\n",
      "2023-11-21 09:45:04,111 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.0 to test forward\n",
      "2023-11-21 09:45:04,111 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.1 to test forward\n",
      "2023-11-21 09:45:04,112 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.2 to test forward\n",
      "2023-11-21 09:45:04,112 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.3 to test forward\n",
      "2023-11-21 09:45:04,113 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.4 to test forward\n",
      "2023-11-21 09:45:04,113 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.5 to test forward\n",
      "2023-11-21 09:45:04,113 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.6 to test forward\n",
      "2023-11-21 09:45:04,114 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.7 to test forward\n",
      "2023-11-21 09:45:04,115 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.8 to test forward\n",
      "2023-11-21 09:45:04,116 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.9 to test forward\n",
      "2023-11-21 09:45:04,116 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.10 to test forward\n",
      "2023-11-21 09:45:04,117 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.11 to test forward\n",
      "2023-11-21 09:45:04,118 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.12 to test forward\n",
      "2023-11-21 09:45:04,118 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.13 to test forward\n",
      "2023-11-21 09:45:04,119 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.14 to test forward\n",
      "2023-11-21 09:45:04,119 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.15 to test forward\n",
      "2023-11-21 09:45:04,120 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.16 to test forward\n",
      "2023-11-21 09:45:04,120 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.17 to test forward\n",
      "2023-11-21 09:45:04,123 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.18 to test forward\n",
      "2023-11-21 09:45:04,124 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.19 to test forward\n",
      "2023-11-21 09:45:04,124 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.20 to test forward\n",
      "2023-11-21 09:45:04,125 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.21 to test forward\n",
      "2023-11-21 09:45:04,126 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.22 to test forward\n",
      "2023-11-21 09:45:04,126 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.23 to test forward\n",
      "2023-11-21 09:45:04,127 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.24 to test forward\n",
      "2023-11-21 09:45:04,127 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.25 to test forward\n",
      "2023-11-21 09:45:04,128 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.26 to test forward\n",
      "2023-11-21 09:45:04,129 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.27 to test forward\n",
      "2023-11-21 09:45:04,129 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.28 to test forward\n",
      "2023-11-21 09:45:04,130 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.29 to test forward\n",
      "2023-11-21 09:45:04,130 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.30 to test forward\n",
      "2023-11-21 09:45:04,131 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.31 to test forward\n",
      "2023-11-21 09:45:04,131 [model.py:433 in to_layer_offloading_forward] DEBUG - model.norm to test forward\n",
      "2023-11-21 09:45:04,131 [model.py:433 in to_layer_offloading_forward] DEBUG - lm_head to test forward\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-11-21 09:45:04,134 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.embed_tokens to cpu\n",
      "/home/fsuser/FlexGen/general/loaders/model.py:345: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  value = torch.from_numpy(np_memmap)\n",
      "2023-11-21 09:45:04,136 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.embed_tokens to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,137 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.0 to cpu\n",
      "2023-11-21 09:45:04,186 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.0 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,200 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.1 to cpu\n",
      "2023-11-21 09:45:04,250 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.1 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,265 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.2 to cpu\n",
      "2023-11-21 09:45:04,317 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.2 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,331 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.3 to cpu\n",
      "2023-11-21 09:45:04,383 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.3 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,395 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.4 to cpu\n",
      "2023-11-21 09:45:04,442 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.4 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,454 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.5 to cpu\n",
      "2023-11-21 09:45:04,504 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.5 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,522 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.6 to cpu\n",
      "2023-11-21 09:45:04,574 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.6 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,591 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.7 to cpu\n",
      "2023-11-21 09:45:04,645 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.7 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,657 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.8 to cpu\n",
      "2023-11-21 09:45:04,705 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.8 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,716 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.9 to cpu\n",
      "2023-11-21 09:45:04,761 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.9 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,772 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.10 to cpu\n",
      "2023-11-21 09:45:04,819 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.10 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,830 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.11 to cpu\n",
      "2023-11-21 09:45:04,877 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.11 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,891 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.12 to cpu\n",
      "2023-11-21 09:45:04,946 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.12 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:04,957 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.13 to cpu\n",
      "2023-11-21 09:45:05,007 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.13 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,021 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.14 to cpu\n",
      "2023-11-21 09:45:05,088 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.14 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,103 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.15 to cpu\n",
      "2023-11-21 09:45:05,154 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.15 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,165 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.16 to cpu\n",
      "2023-11-21 09:45:05,215 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.16 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,226 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.17 to cpu\n",
      "2023-11-21 09:45:05,278 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.17 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,289 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.18 to cpu\n",
      "2023-11-21 09:45:05,339 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.18 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,350 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.19 to cpu\n",
      "2023-11-21 09:45:05,408 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.19 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,418 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.20 to cpu\n",
      "2023-11-21 09:45:05,489 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.20 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,500 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.21 to cpu\n",
      "2023-11-21 09:45:05,555 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.21 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,566 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.22 to cpu\n",
      "2023-11-21 09:45:05,644 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.22 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,654 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.23 to cpu\n",
      "2023-11-21 09:45:05,772 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.23 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,782 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.24 to cpu\n",
      "2023-11-21 09:45:05,826 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.24 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,837 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.25 to cpu\n",
      "2023-11-21 09:45:05,884 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.25 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,897 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.26 to cpu\n",
      "2023-11-21 09:45:05,942 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.26 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:05,953 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.27 to cpu\n",
      "2023-11-21 09:45:05,998 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.27 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,013 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.28 to cpu\n",
      "2023-11-21 09:45:06,060 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.28 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,076 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.29 to cpu\n",
      "2023-11-21 09:45:06,123 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.29 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,138 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.30 to cpu\n",
      "2023-11-21 09:45:06,185 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.30 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,199 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.31 to cpu\n",
      "2023-11-21 09:45:06,246 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.31 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,260 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.norm to cpu\n",
      "2023-11-21 09:45:06,261 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.norm to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,261 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.lm_head to cpu\n",
      "2023-11-21 09:45:06,293 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.lm_head to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,301 [model.py:441 in reset_forward] DEBUG - model.embed_tokens from layer-offloading to old.\n",
      "2023-11-21 09:45:06,302 [model.py:441 in reset_forward] DEBUG - model.layers.0 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,302 [model.py:441 in reset_forward] DEBUG - model.layers.1 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,303 [model.py:441 in reset_forward] DEBUG - model.layers.2 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,303 [model.py:441 in reset_forward] DEBUG - model.layers.3 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,303 [model.py:441 in reset_forward] DEBUG - model.layers.4 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,304 [model.py:441 in reset_forward] DEBUG - model.layers.5 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,304 [model.py:441 in reset_forward] DEBUG - model.layers.6 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,305 [model.py:441 in reset_forward] DEBUG - model.layers.7 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,305 [model.py:441 in reset_forward] DEBUG - model.layers.8 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,305 [model.py:441 in reset_forward] DEBUG - model.layers.9 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,306 [model.py:441 in reset_forward] DEBUG - model.layers.10 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,306 [model.py:441 in reset_forward] DEBUG - model.layers.11 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,306 [model.py:441 in reset_forward] DEBUG - model.layers.12 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,307 [model.py:441 in reset_forward] DEBUG - model.layers.13 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,307 [model.py:441 in reset_forward] DEBUG - model.layers.14 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,308 [model.py:441 in reset_forward] DEBUG - model.layers.15 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,308 [model.py:441 in reset_forward] DEBUG - model.layers.16 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,309 [model.py:441 in reset_forward] DEBUG - model.layers.17 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,309 [model.py:441 in reset_forward] DEBUG - model.layers.18 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,309 [model.py:441 in reset_forward] DEBUG - model.layers.19 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,310 [model.py:441 in reset_forward] DEBUG - model.layers.20 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,310 [model.py:441 in reset_forward] DEBUG - model.layers.21 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,310 [model.py:441 in reset_forward] DEBUG - model.layers.22 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,311 [model.py:441 in reset_forward] DEBUG - model.layers.23 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,311 [model.py:441 in reset_forward] DEBUG - model.layers.24 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,311 [model.py:441 in reset_forward] DEBUG - model.layers.25 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,312 [model.py:441 in reset_forward] DEBUG - model.layers.26 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,312 [model.py:441 in reset_forward] DEBUG - model.layers.27 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,313 [model.py:441 in reset_forward] DEBUG - model.layers.28 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,313 [model.py:441 in reset_forward] DEBUG - model.layers.29 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,313 [model.py:441 in reset_forward] DEBUG - model.layers.30 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,314 [model.py:441 in reset_forward] DEBUG - model.layers.31 from layer-offloading to old.\n",
      "2023-11-21 09:45:06,314 [model.py:441 in reset_forward] DEBUG - model.norm from layer-offloading to old.\n",
      "2023-11-21 09:45:06,315 [model.py:441 in reset_forward] DEBUG - lm_head from layer-offloading to old.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.embed_tokens', 'model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.norm', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = \"facebook/opt-125m\"  # 125m 6.7b 13b 30b\n",
    "# checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "# checkpoint = 'bigscience/bloom-560m' #\n",
    "checkpoint = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "policy = Policy(\n",
    "    gpu_batch_size=1,\n",
    "    num_gpu_batches=4,\n",
    "    weights_gpu_percent=0.0,\n",
    "    weights_cpu_percent=0.3,\n",
    "    cache_gpu_percent=0.0,\n",
    "    cache_cpu_percent=0.2,\n",
    "    act_gpu_percent=0.0,\n",
    "    act_cpu_percent=0.5,\n",
    "    overlap=True,\n",
    "    pin_weight=True,\n",
    ")\n",
    "\n",
    "m = MetaModel(checkpoint, policy)\n",
    "print(m.layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:45:06,451 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 \"HEAD /NousResearch/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2023-11-21 09:45:06,578 [model.py:433 in to_layer_offloading_forward] DEBUG - model.embed_tokens to test forward\n",
      "2023-11-21 09:45:06,579 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.0 to test forward\n",
      "2023-11-21 09:45:06,579 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.1 to test forward\n",
      "2023-11-21 09:45:06,580 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.2 to test forward\n",
      "2023-11-21 09:45:06,581 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.3 to test forward\n",
      "2023-11-21 09:45:06,581 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.4 to test forward\n",
      "2023-11-21 09:45:06,582 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.5 to test forward\n",
      "2023-11-21 09:45:06,582 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.6 to test forward\n",
      "2023-11-21 09:45:06,582 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.7 to test forward\n",
      "2023-11-21 09:45:06,583 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.8 to test forward\n",
      "2023-11-21 09:45:06,583 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.9 to test forward\n",
      "2023-11-21 09:45:06,584 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.10 to test forward\n",
      "2023-11-21 09:45:06,584 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.11 to test forward\n",
      "2023-11-21 09:45:06,585 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.12 to test forward\n",
      "2023-11-21 09:45:06,585 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.13 to test forward\n",
      "2023-11-21 09:45:06,586 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.14 to test forward\n",
      "2023-11-21 09:45:06,586 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.15 to test forward\n",
      "2023-11-21 09:45:06,587 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.16 to test forward\n",
      "2023-11-21 09:45:06,587 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.17 to test forward\n",
      "2023-11-21 09:45:06,587 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.18 to test forward\n",
      "2023-11-21 09:45:06,588 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.19 to test forward\n",
      "2023-11-21 09:45:06,589 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.20 to test forward\n",
      "2023-11-21 09:45:06,589 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.21 to test forward\n",
      "2023-11-21 09:45:06,590 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.22 to test forward\n",
      "2023-11-21 09:45:06,590 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.23 to test forward\n",
      "2023-11-21 09:45:06,591 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.24 to test forward\n",
      "2023-11-21 09:45:06,591 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.25 to test forward\n",
      "2023-11-21 09:45:06,592 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.26 to test forward\n",
      "2023-11-21 09:45:06,592 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.27 to test forward\n",
      "2023-11-21 09:45:06,593 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.28 to test forward\n",
      "2023-11-21 09:45:06,593 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.29 to test forward\n",
      "2023-11-21 09:45:06,594 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.30 to test forward\n",
      "2023-11-21 09:45:06,594 [model.py:433 in to_layer_offloading_forward] DEBUG - model.layers.31 to test forward\n",
      "2023-11-21 09:45:06,595 [model.py:433 in to_layer_offloading_forward] DEBUG - model.norm to test forward\n",
      "2023-11-21 09:45:06,595 [model.py:433 in to_layer_offloading_forward] DEBUG - lm_head to test forward\n",
      "/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-11-21 09:45:06,676 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.embed_tokens to cuda:0\n",
      "2023-11-21 09:45:06,761 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.embed_tokens to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:06,763 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.0 to cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:45:07,038 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.0 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,042 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.1 to cuda:0\n",
      "2023-11-21 09:45:07,163 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.1 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,168 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.2 to cuda:0\n",
      "2023-11-21 09:45:07,304 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.2 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,310 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.3 to cuda:0\n",
      "2023-11-21 09:45:07,442 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.3 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,448 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.4 to cuda:0\n",
      "2023-11-21 09:45:07,574 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.4 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,579 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.5 to cuda:0\n",
      "2023-11-21 09:45:07,707 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.5 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,712 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.6 to cuda:0\n",
      "2023-11-21 09:45:07,848 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.6 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,853 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.7 to cuda:0\n",
      "2023-11-21 09:45:07,983 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.7 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:07,987 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.8 to cuda:0\n",
      "2023-11-21 09:45:08,116 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.8 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,121 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.9 to cuda:0\n",
      "2023-11-21 09:45:08,260 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.9 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,264 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.10 to cuda:0\n",
      "2023-11-21 09:45:08,383 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.10 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,388 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.11 to cuda:0\n",
      "2023-11-21 09:45:08,522 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.11 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,527 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.12 to cuda:0\n",
      "2023-11-21 09:45:08,671 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.12 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,676 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.13 to cuda:0\n",
      "2023-11-21 09:45:08,800 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.13 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,805 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.14 to cuda:0\n",
      "2023-11-21 09:45:08,928 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.14 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:08,933 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.15 to cuda:0\n",
      "2023-11-21 09:45:09,057 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.15 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,062 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.16 to cuda:0\n",
      "2023-11-21 09:45:09,189 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.16 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,194 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.17 to cuda:0\n",
      "2023-11-21 09:45:09,335 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.17 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,340 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.18 to cuda:0\n",
      "2023-11-21 09:45:09,469 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.18 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,473 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.19 to cuda:0\n",
      "2023-11-21 09:45:09,600 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.19 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,605 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.20 to cuda:0\n",
      "2023-11-21 09:45:09,749 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.20 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,754 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.21 to cuda:0\n",
      "2023-11-21 09:45:09,891 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.21 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:09,896 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.22 to cuda:0\n",
      "2023-11-21 09:45:10,025 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.22 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,030 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.23 to cuda:0\n",
      "2023-11-21 09:45:10,210 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.23 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,215 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.24 to cuda:0\n",
      "2023-11-21 09:45:10,331 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.24 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,336 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.25 to cuda:0\n",
      "2023-11-21 09:45:10,454 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.25 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,459 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.26 to cuda:0\n",
      "2023-11-21 09:45:10,576 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.26 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,581 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.27 to cuda:0\n",
      "2023-11-21 09:45:10,714 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.27 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,719 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.28 to cuda:0\n",
      "2023-11-21 09:45:10,854 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.28 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,858 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.29 to cuda:0\n",
      "2023-11-21 09:45:10,990 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.29 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:10,994 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.30 to cuda:0\n",
      "2023-11-21 09:45:11,129 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.30 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:11,133 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.layers.31 to cuda:0\n",
      "2023-11-21 09:45:11,253 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.layers.31 to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:11,257 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.model.norm to cuda:0\n",
      "2023-11-21 09:45:11,258 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.model.norm to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:11,258 [model.py:352 in layer_device_load] DEBUG - load_layer_weights: LlamaForCausalLM.lm_head to cuda:0\n",
      "2023-11-21 09:45:11,337 [model.py:387 in layer_offload] DEBUG - offload_layer_weights: LlamaForCausalLM.lm_head to meta\n",
      "\n",
      "\n",
      "2023-11-21 09:45:12,294 [model.py:441 in reset_forward] DEBUG - model.embed_tokens from layer-offloading to old.\n",
      "2023-11-21 09:45:12,295 [model.py:441 in reset_forward] DEBUG - model.layers.0 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,295 [model.py:441 in reset_forward] DEBUG - model.layers.1 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,296 [model.py:441 in reset_forward] DEBUG - model.layers.2 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,296 [model.py:441 in reset_forward] DEBUG - model.layers.3 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,297 [model.py:441 in reset_forward] DEBUG - model.layers.4 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,298 [model.py:441 in reset_forward] DEBUG - model.layers.5 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,298 [model.py:441 in reset_forward] DEBUG - model.layers.6 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,299 [model.py:441 in reset_forward] DEBUG - model.layers.7 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,299 [model.py:441 in reset_forward] DEBUG - model.layers.8 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,300 [model.py:441 in reset_forward] DEBUG - model.layers.9 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,300 [model.py:441 in reset_forward] DEBUG - model.layers.10 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,301 [model.py:441 in reset_forward] DEBUG - model.layers.11 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,301 [model.py:441 in reset_forward] DEBUG - model.layers.12 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,302 [model.py:441 in reset_forward] DEBUG - model.layers.13 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,302 [model.py:441 in reset_forward] DEBUG - model.layers.14 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,303 [model.py:441 in reset_forward] DEBUG - model.layers.15 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,303 [model.py:441 in reset_forward] DEBUG - model.layers.16 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,303 [model.py:441 in reset_forward] DEBUG - model.layers.17 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,304 [model.py:441 in reset_forward] DEBUG - model.layers.18 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,304 [model.py:441 in reset_forward] DEBUG - model.layers.19 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,305 [model.py:441 in reset_forward] DEBUG - model.layers.20 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,305 [model.py:441 in reset_forward] DEBUG - model.layers.21 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,306 [model.py:441 in reset_forward] DEBUG - model.layers.22 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,307 [model.py:441 in reset_forward] DEBUG - model.layers.23 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,307 [model.py:441 in reset_forward] DEBUG - model.layers.24 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,308 [model.py:441 in reset_forward] DEBUG - model.layers.25 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,308 [model.py:441 in reset_forward] DEBUG - model.layers.26 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,308 [model.py:441 in reset_forward] DEBUG - model.layers.27 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,309 [model.py:441 in reset_forward] DEBUG - model.layers.28 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,309 [model.py:441 in reset_forward] DEBUG - model.layers.29 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,310 [model.py:441 in reset_forward] DEBUG - model.layers.30 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,310 [model.py:441 in reset_forward] DEBUG - model.layers.31 from layer-offloading to old.\n",
      "2023-11-21 09:45:12,311 [model.py:441 in reset_forward] DEBUG - model.norm from layer-offloading to old.\n",
      "2023-11-21 09:45:12,311 [model.py:441 in reset_forward] DEBUG - lm_head from layer-offloading to old.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens',\n",
       " 'model.layers.0',\n",
       " 'model.layers.1',\n",
       " 'model.layers.2',\n",
       " 'model.layers.3',\n",
       " 'model.layers.4',\n",
       " 'model.layers.5',\n",
       " 'model.layers.6',\n",
       " 'model.layers.7',\n",
       " 'model.layers.8',\n",
       " 'model.layers.9',\n",
       " 'model.layers.10',\n",
       " 'model.layers.11',\n",
       " 'model.layers.12',\n",
       " 'model.layers.13',\n",
       " 'model.layers.14',\n",
       " 'model.layers.15',\n",
       " 'model.layers.16',\n",
       " 'model.layers.17',\n",
       " 'model.layers.18',\n",
       " 'model.layers.19',\n",
       " 'model.layers.20',\n",
       " 'model.layers.21',\n",
       " 'model.layers.22',\n",
       " 'model.layers.23',\n",
       " 'model.layers.24',\n",
       " 'model.layers.25',\n",
       " 'model.layers.26',\n",
       " 'model.layers.27',\n",
       " 'model.layers.28',\n",
       " 'model.layers.29',\n",
       " 'model.layers.30',\n",
       " 'model.layers.31',\n",
       " 'model.norm',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.test_run('cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
